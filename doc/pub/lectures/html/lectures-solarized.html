<!--
HTML file automatically generated from DocOnce source
(https://github.com/doconce/doconce/)
doconce format html lectures.do.txt --pygments_html_style=perldoc --html_style=solarized3 --html_links_in_new_window --html_output=lectures-solarized --no_mako
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/doconce/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title></title>
<link href="https://cdn.rawgit.com/doconce/doconce/master/bundled/html_styles/style_solarized_box/css/solarized_light_code.css" rel="stylesheet" type="text/css" title="light"/>
<script src="https://cdn.rawgit.com/doconce/doconce/master/bundled/html_styles/style_solarized_box/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<link href="https://thomasf.github.io/solarized-css/solarized-light.min.css" rel="stylesheet">
<style type="text/css">
h1 {color: #b58900;}  /* yellow */
/* h1 {color: #cb4b16;}  orange */
/* h1 {color: #d33682;}  magenta, the original choice of thomasf */
code { padding: 0px; background-color: inherit; }
pre {
  border: 0pt solid #93a1a1;
  box-shadow: none;
}
div { text-align: justify; text-justify: inter-word; }
.tab {
  padding-left: 1.5em;
}
div.toc p,a {
  line-height: 1.3;
  margin-top: 1.1;
  margin-bottom: 1.1;
}
</style>
</head>

<!-- tocinfo
{'highest level': 1,
 'sections': [('Harmonic oscillator problem with and without interactions',
               2,
               None,
               'harmonic-oscillator-problem-with-and-without-interactions'),
              ('Simple program for one particle in a harmonic oscillator trap',
               2,
               None,
               'simple-program-for-one-particle-in-a-harmonic-oscillator-trap'),
              ('Unit Testing', 2, None, 'unit-testing'),
              ('Simple example of unit test',
               2,
               None,
               'simple-example-of-unit-test'),
              ('Examples', 2, None, 'examples'),
              ('Factorial Example', 2, None, 'factorial-example'),
              ("Discussion of Householder's method for eigenvalues",
               2,
               None,
               'discussion-of-householder-s-method-for-eigenvalues'),
              ('Eigenvalues with the *QR* and Lanczos methods',
               2,
               None,
               'eigenvalues-with-the-qr-and-lanczos-methods'),
              ("Lanczos' method", 2, None, 'lanczos-method'),
              ('Variational Monte Carlo methods',
               1,
               None,
               'variational-monte-carlo-methods'),
              ('Quantum Monte Carlo Motivation',
               2,
               None,
               'quantum-monte-carlo-motivation'),
              ('Quantum Monte Carlo: hydrogen atom',
               3,
               None,
               'quantum-monte-carlo-hydrogen-atom'),
              ('A simple Python code that solves the two-boson or two-fermion '
               'case in two-dimensions',
               3,
               None,
               'a-simple-python-code-that-solves-the-two-boson-or-two-fermion-case-in-two-dimensions'),
              ('Quantum Monte Carlo: the helium atom',
               2,
               None,
               'quantum-monte-carlo-the-helium-atom'),
              ('The Metropolis algorithm', 2, None, 'the-metropolis-algorithm'),
              ('Importance sampling', 2, None, 'importance-sampling'),
              ('Importance sampling, program elements',
               2,
               None,
               'importance-sampling-program-elements'),
              ('Importance sampling, Fokker-Planck and Langevin equations',
               2,
               None,
               'importance-sampling-fokker-planck-and-langevin-equations'),
              ('Code example for two electrons in a quantum dots',
               2,
               None,
               'code-example-for-two-electrons-in-a-quantum-dots'),
              ('Bringing the gradient optmization',
               3,
               None,
               'bringing-the-gradient-optmization'),
              ('VMC for fermions: Efficient calculation of Slater determinants',
               2,
               None,
               'vmc-for-fermions-efficient-calculation-of-slater-determinants'),
              ('The gradient and the Laplacian',
               3,
               None,
               'the-gradient-and-the-laplacian'),
              ('Expectation value of the kinetic energy',
               3,
               None,
               'expectation-value-of-the-kinetic-energy'),
              ('Gradient Methods', 1, None, 'gradient-methods'),
              ('Top-down start', 2, None, 'top-down-start'),
              ('Motivation', 2, None, 'motivation'),
              ('Simple example and demonstration',
               2,
               None,
               'simple-example-and-demonstration'),
              ('Simple example and demonstration',
               2,
               None,
               'simple-example-and-demonstration'),
              ('Exercise 1: Find the local energy for the harmonic oscillator',
               2,
               None,
               'exercise-1-find-the-local-energy-for-the-harmonic-oscillator'),
              ('Variance in the simple model',
               2,
               None,
               'variance-in-the-simple-model'),
              ('Computing the derivatives',
               2,
               None,
               'computing-the-derivatives'),
              ('Expressions for finding the derivatives of the local energy',
               2,
               None,
               'expressions-for-finding-the-derivatives-of-the-local-energy'),
              ('Derivatives of the local energy',
               2,
               None,
               'derivatives-of-the-local-energy'),
              ('Exercise 2: General expression for the derivative of the '
               'energy',
               2,
               None,
               'exercise-2-general-expression-for-the-derivative-of-the-energy'),
              ('Python program for 2-electrons in 2 dimensions',
               2,
               None,
               'python-program-for-2-electrons-in-2-dimensions'),
              ("Using Broyden's algorithm in scipy",
               2,
               None,
               'using-broyden-s-algorithm-in-scipy'),
              ("Brief reminder on Newton-Raphson's method",
               2,
               None,
               'brief-reminder-on-newton-raphson-s-method'),
              ('The equations', 2, None, 'the-equations'),
              ('Simple geometric interpretation',
               2,
               None,
               'simple-geometric-interpretation'),
              ('Extending to more than one variable',
               2,
               None,
               'extending-to-more-than-one-variable'),
              ('Steepest descent', 2, None, 'steepest-descent'),
              ('More on Steepest descent', 2, None, 'more-on-steepest-descent'),
              ('The ideal', 2, None, 'the-ideal'),
              ('The sensitiveness of the gradient descent',
               2,
               None,
               'the-sensitiveness-of-the-gradient-descent'),
              ('Convex functions', 2, None, 'convex-functions'),
              ('Convex function', 2, None, 'convex-function'),
              ('Conditions on convex functions',
               2,
               None,
               'conditions-on-convex-functions'),
              ('More on convex functions', 2, None, 'more-on-convex-functions'),
              ('Some simple problems', 2, None, 'some-simple-problems'),
              ('Standard steepest descent',
               2,
               None,
               'standard-steepest-descent'),
              ('Gradient method', 2, None, 'gradient-method'),
              ('Steepest descent  method', 2, None, 'steepest-descent-method'),
              ('Steepest descent  method', 2, None, 'steepest-descent-method'),
              ('Final expressions', 2, None, 'final-expressions'),
              ('Conjugate gradient method',
               2,
               None,
               'conjugate-gradient-method'),
              ('Conjugate gradient method',
               2,
               None,
               'conjugate-gradient-method'),
              ('Conjugate gradient method',
               2,
               None,
               'conjugate-gradient-method'),
              ('Conjugate gradient method',
               2,
               None,
               'conjugate-gradient-method'),
              ('Conjugate gradient method and iterations',
               2,
               None,
               'conjugate-gradient-method-and-iterations'),
              ('Conjugate gradient method',
               2,
               None,
               'conjugate-gradient-method'),
              ('Conjugate gradient method',
               2,
               None,
               'conjugate-gradient-method'),
              ('Conjugate gradient method',
               2,
               None,
               'conjugate-gradient-method'),
              ('Broyden–Fletcher–Goldfarb–Shanno algorithm',
               2,
               None,
               'broyden-fletcher-goldfarb-shanno-algorithm'),
              ('Stochastic Gradient Descent',
               2,
               None,
               'stochastic-gradient-descent'),
              ('Computation of gradients', 2, None, 'computation-of-gradients'),
              ('SGD example', 2, None, 'sgd-example'),
              ('The gradient step', 2, None, 'the-gradient-step'),
              ('Simple example code', 2, None, 'simple-example-code'),
              ('When do we stop?', 2, None, 'when-do-we-stop'),
              ('Slightly different approach',
               2,
               None,
               'slightly-different-approach'),
              ('Program for stochastic gradient',
               2,
               None,
               'program-for-stochastic-gradient'),
              ('Using gradient descent methods, limitations',
               2,
               None,
               'using-gradient-descent-methods-limitations'),
              ('Codes from numerical recipes',
               2,
               None,
               'codes-from-numerical-recipes'),
              ('Finding the minimum of the harmonic oscillator model in one '
               'dimension',
               2,
               None,
               'finding-the-minimum-of-the-harmonic-oscillator-model-in-one-dimension'),
              ('Functions to observe', 2, None, 'functions-to-observe'),
              ('Resampling Techniques, Bootstrap and Blocking',
               1,
               None,
               'resampling-techniques-bootstrap-and-blocking'),
              ('Why resampling methods ?', 2, None, 'why-resampling-methods'),
              ('Statistical analysis', 2, None, 'statistical-analysis'),
              ('Statistics, wrapping up from last week',
               2,
               None,
               'statistics-wrapping-up-from-last-week'),
              ('Statistics, final expression',
               2,
               None,
               'statistics-final-expression'),
              ('Statistics, effective number of correlations',
               2,
               None,
               'statistics-effective-number-of-correlations'),
              ('Can we understand this? Time Auto-correlation Function',
               2,
               None,
               'can-we-understand-this-time-auto-correlation-function'),
              ('Time Auto-correlation Function',
               2,
               None,
               'time-auto-correlation-function'),
              ('Time Auto-correlation Function',
               2,
               None,
               'time-auto-correlation-function'),
              ('Time Auto-correlation Function',
               2,
               None,
               'time-auto-correlation-function'),
              ('Time Auto-correlation Function',
               2,
               None,
               'time-auto-correlation-function'),
              ('Time Auto-correlation Function',
               2,
               None,
               'time-auto-correlation-function'),
              ('Correlation Time', 2, None, 'correlation-time'),
              ('Resampling methods: Jackknife and Bootstrap',
               2,
               None,
               'resampling-methods-jackknife-and-bootstrap'),
              ('Resampling methods: Jackknife',
               2,
               None,
               'resampling-methods-jackknife'),
              ('Resampling methods: Jackknife estimator',
               2,
               None,
               'resampling-methods-jackknife-estimator'),
              ('Jackknife code example', 2, None, 'jackknife-code-example'),
              ('Resampling methods: Bootstrap',
               2,
               None,
               'resampling-methods-bootstrap'),
              ('Resampling methods: Bootstrap background',
               2,
               None,
               'resampling-methods-bootstrap-background'),
              ('Resampling methods: More Bootstrap background',
               2,
               None,
               'resampling-methods-more-bootstrap-background'),
              ('Resampling methods: Bootstrap approach',
               2,
               None,
               'resampling-methods-bootstrap-approach'),
              ('Resampling methods: Bootstrap steps',
               2,
               None,
               'resampling-methods-bootstrap-steps'),
              ('Code example for the Bootstrap method',
               2,
               None,
               'code-example-for-the-bootstrap-method'),
              ('Resampling methods: Blocking',
               2,
               None,
               'resampling-methods-blocking'),
              ('Blocking Transformations', 2, None, 'blocking-transformations'),
              ('Blocking Transformations', 2, None, 'blocking-transformations'),
              ('Blocking Transformations, getting there',
               2,
               None,
               'blocking-transformations-getting-there'),
              ('Blocking Transformations, final expressions',
               2,
               None,
               'blocking-transformations-final-expressions'),
              ('Boltzmann Machines', 1, None, 'boltzmann-machines'),
              ('The network', 2, None, 'the-network'),
              ('Joint distribution', 3, None, 'joint-distribution'),
              ('Network Elements, the energy function',
               3,
               None,
               'network-elements-the-energy-function'),
              ('Defining different types of RBMs',
               3,
               None,
               'defining-different-types-of-rbms'),
              ('Cost function', 3, None, 'cost-function'),
              ('Optimization / Training', 3, None, 'optimization-training'),
              ('Kullback-Leibler relative entropy',
               3,
               None,
               'kullback-leibler-relative-entropy'),
              ('Setting up for gradient descent calculations',
               2,
               None,
               'setting-up-for-gradient-descent-calculations'),
              ('RBMs for the quantum many body problem',
               2,
               None,
               'rbms-for-the-quantum-many-body-problem'),
              ('Representing the wave function',
               2,
               None,
               'representing-the-wave-function'),
              ('Choose the cost function', 2, None, 'choose-the-cost-function'),
              ('Mathematical details', 3, None, 'mathematical-details'),
              ('Marginal Probability Density Functions',
               3,
               None,
               'marginal-probability-density-functions'),
              ('Conditional Probability Density Functions',
               3,
               None,
               'conditional-probability-density-functions'),
              ('Gaussian-Binary Restricted Boltzmann Machines',
               3,
               None,
               'gaussian-binary-restricted-boltzmann-machines'),
              ('Joint Probability Density Function',
               3,
               None,
               'joint-probability-density-function'),
              ('Marginal Probability Density Functions',
               3,
               None,
               'marginal-probability-density-functions'),
              ('Conditional Probability Density Functions',
               3,
               None,
               'conditional-probability-density-functions'),
              ('Neural Quantum States', 2, None, 'neural-quantum-states'),
              ('Cost function', 3, None, 'cost-function'),
              ('Python version for the two non-interacting particles',
               2,
               None,
               'python-version-for-the-two-non-interacting-particles')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- ------------------- main content ---------------------- -->
\( \mathbf{A}\mathbf{x}=\lambda\mathbf{x} \) 
<p>   \( \mathbf{A}=\mathbf{A}^* \)  Conjugate gradient and  Lanczos
  \( \mathbf{A}\ne \mathbf{A}^* \)  GMRES etc           Arnoldi 
</p>

<p>The Numerical Recipes codes have been rewritten in Fortran 90/95 and C/C++ by us.
The original source codes are taken from the widely used software
package LAPACK, which follows two other popular packages developed in the 1970s, 
namely EISPACK and LINPACK.
</p>
<ul>
 <li> LINPACK: package for linear equations  and least square problems.</li>
 <li> LAPACK:package for solving symmetric, unsymmetric and generalized eigenvalue problems. From LAPACK's website <a href="http://www.netlib.org" target="_blank"><tt>http://www.netlib.org</tt></a>  it is  possible to download for free all source codes from this library. Both C/C++ and Fortran versions are available.</li>
 <li> BLAS (I, II and III): (Basic Linear Algebra Subprograms)  are routines that provide standard building blocks for performing basic vector and matrix operations.   Blas I is vector operations, II vector-matrix operations and III matrix-matrix operations.</li> 
</ul>
<p>Consider an  example of an (\( n\times n \)) orthogonal transformation matrix </p>
$$
\mathbf{S}=
 \left( 
   \begin{array}{cccccccc}
   1  &    0  & \dots &   0        &    0  & \dots & 0 &   0       \\
   0  &    1  & \dots &   0        &    0  & \dots & 0 &   0       \\
\dots & \dots & \dots & \dots      & \dots & \dots & 0 & \dots     \\ 
   0  &    0  & \dots & \cos\theta  &    0  & \dots & 0 & \sin\theta \\
   0  &    0  & \dots &   0        &    1  & \dots & 0 &   0       \\
\dots & \dots & \dots & \dots      & \dots & \dots & 1 & \dots     \\
   0  &    0  & \dots &  -\sin\theta        &    0  & \dots & 0 &   \cos\theta   
   \end{array}
 \right)
$$

<p>with property \( \mathbf{S^{T}} = \mathbf{S^{-1}} \).
It performs a plane rotation around an angle \( \theta \) in the Euclidean 
$n-$dimensional space. 
</p>

<p>It means that its matrix elements that differ
from zero are given by
</p>
$$
    s_{kk}= s_{ll}=\cos\theta, 
    s_{kl}=-s_{lk}= -\sin\theta, 
    s_{ii}=1\hspace{0.5cm} i\ne k \hspace{0.5cm} i \ne l,
$$

<p>A similarity transformation </p>
$$
     \mathbf{B}= \mathbf{S}^T \mathbf{A}\mathbf{S},
$$

<p>results in </p>
$$
\begin{align*}
b_{ik} =& a_{ik}\cos\theta - a_{il}\sin\theta , i \ne k, i \ne l \\
b_{il} =& a_{il}\cos\theta + a_{ik}\sin\theta , i \ne k, i \ne l \nonumber\\
b_{kk} =& a_{kk}\cos^2\theta - 2a_{kl}\cos\theta \sin\theta +a_{ll}\sin^2\theta\nonumber\\
b_{ll} =& a_{ll}\cos^2\theta +2a_{kl}\cos\theta sin\theta +a_{kk}\sin^2\theta\nonumber\\
b_{kl} =& (a_{kk}-a_{ll})\cos\theta \sin\theta +a_{kl}(\cos^2\theta-\sin^2\theta)\nonumber 
\end{align*}
$$

<p>The angle \( \theta \) is  arbitrary. The recipe is to choose  \( \theta \) so that all
non-diagonal matrix elements \( b_{kl} \) become zero.  
</p>

<p>The main idea is thus to reduce systematically the 
norm of the 
off-diagonal matrix elements  of a matrix  \( \mathbf{A} \) 
</p>
$$
\mathrm{off}(\mathbf{A}) = \sqrt{\sum_{i=1}^n\sum_{j=1,j\ne i}^n a_{ij}^2}.
$$

<p> To demonstrate the algorithm, we consider the  simple \( 2\times 2 \)  similarity transformation
of the full matrix. The matrix is symmetric, we single out $ 1 \le k < l \le n$  and 
use the abbreviations \( c=\cos\theta \) and \( s=\sin\theta \) to obtain
</p>
$$
 \left( \begin{array}{cc} b_{kk} & 0 \\
                          0 & b_{ll} \\\end{array} \right)  =  \left( \begin{array}{cc} c & -s \\
                          s &c \\\end{array} \right)  \left( \begin{array}{cc} a_{kk} & a_{kl} \\
                          a_{lk} &a_{ll} \\\end{array} \right) \left( \begin{array}{cc} c & s \\
                          -s & c \\\end{array} \right).
$$

<p>We require that the non-diagonal matrix elements \( b_{kl}=b_{lk}=0 \), implying that </p>
$$
a_{kl}(c^2-s^2)+(a_{kk}-a_{ll})cs = b_{kl} = 0.
$$

<p>If \( a_{kl}=0 \) one sees immediately that \( \cos\theta = 1 \) and \( \sin\theta=0 \).</p>

<p>The Frobenius norm of an orthogonal transformation is always preserved. The Frobenius norm is defined
as 
</p>
$$
 \mathrm{norm}(\mathbf{A})_F =  \sqrt{\sum_{i=1}^n\sum_{j=1}^n |a_{ij}|^2}.
$$

<p>This means that for our \( 2\times 2 \) case  we have</p>
$$
2a_{kl}^2+a_{kk}^2+a_{ll}^2 = b_{kk}^2+b_{ll}^2,
$$

<p>which leads to</p>
$$
\mathrm{off}(\mathbf{B})^2 = \mathrm{norm}(\mathbf{B})_F^2-\sum_{i=1}^nb_{ii}^2=\mathrm{off}(\mathbf{A})^2-2a_{kl}^2,
$$

<p>since</p>
$$
  \mathrm{norm}(\mathbf{B})_F^2-\sum_{i=1}^nb_{ii}^2=\mathrm{norm}(\mathbf{A})_F^2-\sum_{i=1}^na_{ii}^2+(a_{kk}^2+a_{ll}^2 -b_{kk}^2-b_{ll}^2).
$$

<p>This results means that  the matrix \( \mathbf{A} \) moves closer to diagonal form  for each transformation.</p>

<p>Defining the quantities \( \tan\theta = t= s/c \) and</p>
$$\cot 2\theta=\tau = \frac{a_{ll}-a_{kk}}{2a_{kl}},
$$

<p>we obtain the quadratic equation (using \( \cot 2\theta=1/2(\cot \theta-\tan\theta) \)</p>
$$
t^2+2\tau t-1= 0,
$$

<p>resulting in </p>
$$
  t = -\tau \pm \sqrt{1+\tau^2},
$$

<p>and \( c \) and \( s \) are easily obtained via</p>
$$
   c = \frac{1}{\sqrt{1+t^2}},
$$

<p>and \( s=tc \).  Convince yourself that we have \( |\theta| \le \pi/4 \). This has the effect  
of minimizing the difference between the matrices \( \mathbf{B} \) and \( \mathbf{A} \) since
</p>
$$
\mathrm{norm}(\mathbf{B}-\mathbf{A})_F^2=4(1-c)\sum_{i=1,i\ne k,l}^n(a_{ik}^2+a_{il}^2) +\frac{2a_{kl}^2}{c^2}.
$$


<ul>
 <li> Choose a tolerance \( \epsilon \), making it a small number, typically \( 10^{-8} \) or smaller.</li>
 <li> Setup a <em>while</em> test  where one compares the norm of the newly computed off-diagonal matrix elements  \[ \mathrm{off}(\mathbf{A}) = \sqrt{\sum_{i=1}^n\sum_{j=1,j\ne i}^n a_{ij}^2}   >  \epsilon. \]</li>
 <li> Now choose the matrix elements \( a_{kl} \) so that we have those with largest value, that is \( |a_{kl}|=\mathrm{max}_{i\ne j} |a_{ij}| \).</li>
 <li> Compute thereafter \( \tau = (a_{ll}-a_{kk})/2a_{kl} \), \( \tan\theta \), \( \cos\theta \) and \( \sin\theta \).</li>
 <li> Compute thereafter the similarity transformation for this set of values \( (k,l) \), obtaining the new matrix \( \mathbf{B}= \mathbf{S}(k,l,\theta)^T \mathbf{A}\mathbf{S}(k,l,\theta) \).</li>
 <li> Compute the new norm of the off-diagonal matrix elements and continue till you have satisfied \( \mathrm{off}(\mathbf{B})  \le  \epsilon \)</li>
</ul>
<p>The convergence rate of the Jacobi method is however poor, one needs typically \( 3n^2-5n^2 \) rotations and each rotation 
requires \( 4n \) operations, resulting in a total of \( 12n^3-20n^3 \) operations in order to zero out non-diagonal matrix elements.
</p>

<p>We specialize to a symmetric \( 3\times 3  \) matrix \( \mathbf{A} \).
We start the process as follows (assuming that \( a_{23}=a_{32} \) is the largest non-diagonal)
with \( c=\cos{\theta} \) and \( s=\sin{\theta} \)
</p>
$$
 \mathbf{B} =
      \left( \begin{array}{ccc} 
                1 & 0 & 0    \\
                0 & c & -s     \\
                0 & s & c
             \end{array} \right)\left( \begin{array}{ccc} 
                a_{11} & a_{12} & a_{13}    \\
                a_{21} & a_{22} & a_{23}     \\
                a_{31} & a_{32} & a_{33}
             \end{array} \right)
              \left( \begin{array}{ccc} 
                1 & 0 & 0    \\
                0 & c & s     \\
                0 & -s & c
             \end{array} \right).
$$

<p>We will choose the angle \( \theta \) in order to have \( a_{23}=a_{32}=0 \).
We get (symmetric matrix)
</p>
$$
 \mathbf{B} =\left( \begin{array}{ccc} 
                a_{11} & a_{12}c -a_{13}s& a_{12}s+a_{13}c    \\
                a_{12}c -a_{13}s & a_{22}c^2+a_{33}s^2 -2a_{23}sc& (a_{22}-a_{33})sc +a_{23}(c^2-s^2)     \\
                a_{12}s+a_{13}c & (a_{22}-a_{33})sc +a_{23}(c^2-s^2) & a_{22}s^2+a_{33}c^2 +2a_{23}sc
             \end{array} \right).
$$

<p>Note that \( a_{11} \) is unchanged! As it should.</p>

<p>We have</p>
$$
 \mathbf{B} =\left( \begin{array}{ccc} 
                a_{11} & a_{12}c -a_{13}s& a_{12}s+a_{13}c    \\
                a_{12}c -a_{13}s & a_{22}c^2+a_{33}s^2 -2a_{23}sc& (a_{22}-a_{33})sc +a_{23}(c^2-s^2)     \\
                a_{12}s+a_{13}c & (a_{22}-a_{33})sc +a_{23}(c^2-s^2) & a_{22}s^2+a_{33}c^2 +2a_{23}sc
             \end{array} \right).
$$

<p>or</p>
$$
\begin{align*}
b_{11} =& a_{11} \\
b_{12} =& a_{12}\cos\theta - a_{13}\sin\theta , 1 \ne 2, 1 \ne 3 \\
b_{13} =& a_{13}\cos\theta + a_{12}\sin\theta , 1 \ne 2, 1 \ne 3 \nonumber\\
b_{22} =& a_{22}\cos^2\theta - 2a_{23}\cos\theta \sin\theta +a_{33}\sin^2\theta\nonumber\\
b_{33} =& a_{33}\cos^2\theta +2a_{23}\cos\theta \sin\theta +a_{22}\sin^2\theta\nonumber\\
b_{23} =& (a_{22}-a_{33})\cos\theta \sin\theta +a_{23}(\cos^2\theta-\sin^2\theta)\nonumber 
\end{align*}
$$

<p>We will fix the angle \( \theta \) so that \( b_{23}=0 \).</p>

<p>We get then a new matrix</p>
$$
 \mathbf{B} =\left( \begin{array}{ccc} 
                b_{11} & b_{12}& b_{13}    \\
                b_{12}& b_{22}& 0    \\
                b_{13}& 0& a_{33}
             \end{array} \right).
$$

<p>We repeat then assuming that \( b_{12} \) is the largest non-diagonal matrix element and get a
new matrix 
</p>
$$
 \mathbf{C} =
      \left( \begin{array}{ccc} 
                c & -s & 0    \\
                s & c & 0     \\
                0 & 0 & 1
             \end{array} \right)\left( \begin{array}{ccc} 
                b_{11} & b_{12} & b_{13}    \\
                b_{12} & b_{22} & 0     \\
                b_{13} & 0 & b_{33}
             \end{array} \right)
              \left( \begin{array}{ccc} 
                c & s & 0    \\
                -s & c & 0     \\
                0 & 0 & 1
             \end{array} \right).
$$

<p>We continue this process till all non-diagonal matrix elements are zero (ideally).
You will notice that performing the above operations that the matrix element 
\( b_{23} \) which was previous zero becomes different from zero.  This is one of the problems which slows
down the jacobi procedure.
</p>

<p>The more general expression for the new matrix elements are</p>
$$
\begin{align*}
b_{ii} =& a_{ii}, i \ne k, i \ne l \\
b_{ik} =& a_{ik}\cos\theta - a_{il}\sin\theta , i \ne k, i \ne l \\
b_{il} =& a_{il}\cos\theta + a_{ik}\sin\theta , i \ne k, i \ne l \nonumber\\
b_{kk} =& a_{kk}\cos^2\theta - 2a_{kl}\cos\theta \sin\theta +a_{ll}\sin^2\theta\nonumber\\
b_{ll} =& a_{ll}\cos^2\theta +2a_{kl}\cos\theta \sin\theta +a_{kk}\sin^2\theta\nonumber\\
b_{kl} =& (a_{kk}-a_{ll})\cos\theta \sin\theta +a_{kl}(\cos^2\theta-\sin^2\theta)\nonumber 
\end{align*}
$$

<p>This is what we will need to code.</p>

<p> Code example</p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #228B22">//  we have defined a matrix A and a matrix R for the eigenvector, both of dim n x n</span>
<span style="color: #228B22">//  The final matrix R has the eigenvectors in its row elements, it is set to one</span>
<span style="color: #228B22">//  for the diagonal elements in the beginning, zero else.</span>
....<span style="color: #bbbbbb"></span>
<span style="color: #00688B; font-weight: bold">double</span><span style="color: #bbbbbb"> </span>tolerance<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span><span style="color: #B452CD">1.0E-10</span>;<span style="color: #bbbbbb"> </span>
<span style="color: #00688B; font-weight: bold">int</span><span style="color: #bbbbbb"> </span>iterations<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span><span style="color: #B452CD">0</span>;<span style="color: #bbbbbb"></span>
<span style="color: #8B008B; font-weight: bold">while</span><span style="color: #bbbbbb"> </span>(<span style="color: #bbbbbb"> </span>maxnondiag<span style="color: #bbbbbb"> </span>&gt;<span style="color: #bbbbbb"> </span>tolerance<span style="color: #bbbbbb"> </span>&amp;&amp;<span style="color: #bbbbbb"> </span>iterations<span style="color: #bbbbbb"> </span>&lt;=<span style="color: #bbbbbb"> </span>maxiter)<span style="color: #bbbbbb"></span>
{<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">   </span><span style="color: #00688B; font-weight: bold">int</span><span style="color: #bbbbbb"> </span>p,<span style="color: #bbbbbb"> </span>q;<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">   </span>offdiag(A,<span style="color: #bbbbbb"> </span>&amp;p,<span style="color: #bbbbbb"> </span>&amp;q,<span style="color: #bbbbbb"> </span>n);<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">   </span>Jacobi_rotate(A,<span style="color: #bbbbbb"> </span>R,<span style="color: #bbbbbb"> </span>p,<span style="color: #bbbbbb"> </span>q,<span style="color: #bbbbbb"> </span>n);<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">   </span>iterations++;<span style="color: #bbbbbb"></span>
}<span style="color: #bbbbbb"></span>
...<span style="color: #bbbbbb"></span>
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Finding the max nondiagonal element</p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #228B22">//  the offdiag function, using Armadillo</span>
<span style="color: #00688B; font-weight: bold">void</span><span style="color: #bbbbbb"> </span><span style="color: #008b45">offdiag</span>(mat<span style="color: #bbbbbb"> </span>A,<span style="color: #bbbbbb"> </span><span style="color: #00688B; font-weight: bold">int</span><span style="color: #bbbbbb"> </span>*p,<span style="color: #bbbbbb"> </span><span style="color: #00688B; font-weight: bold">int</span><span style="color: #bbbbbb"> </span>*q,<span style="color: #bbbbbb"> </span><span style="color: #00688B; font-weight: bold">int</span><span style="color: #bbbbbb"> </span>n);<span style="color: #bbbbbb"></span>
{<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">   </span><span style="color: #00688B; font-weight: bold">double</span><span style="color: #bbbbbb"> </span>max;<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">   </span><span style="color: #8B008B; font-weight: bold">for</span><span style="color: #bbbbbb"> </span>(<span style="color: #00688B; font-weight: bold">int</span><span style="color: #bbbbbb"> </span>i<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span><span style="color: #B452CD">0</span>;<span style="color: #bbbbbb"> </span>i<span style="color: #bbbbbb"> </span>&lt;<span style="color: #bbbbbb"> </span>n;<span style="color: #bbbbbb"> </span>++i)<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">   </span>{<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">       </span><span style="color: #8B008B; font-weight: bold">for</span><span style="color: #bbbbbb"> </span>(<span style="color: #bbbbbb"> </span><span style="color: #00688B; font-weight: bold">int</span><span style="color: #bbbbbb"> </span>j<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span>i+<span style="color: #B452CD">1</span>;<span style="color: #bbbbbb"> </span>j<span style="color: #bbbbbb"> </span>&lt;<span style="color: #bbbbbb"> </span>n;<span style="color: #bbbbbb"> </span>++j)<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">       </span>{<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">           </span><span style="color: #00688B; font-weight: bold">double</span><span style="color: #bbbbbb"> </span>aij<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span>fabs(A(i,j));<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">           </span><span style="color: #8B008B; font-weight: bold">if</span><span style="color: #bbbbbb"> </span>(<span style="color: #bbbbbb"> </span>aij<span style="color: #bbbbbb"> </span>&gt;<span style="color: #bbbbbb"> </span>max)<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">           </span>{<span style="color: #bbbbbb"> </span>
<span style="color: #bbbbbb">              </span>max<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span>aij;<span style="color: #bbbbbb">  </span>p<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span>i;<span style="color: #bbbbbb"> </span>q<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span>j;<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">           </span>}<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">       </span>}<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">   </span>}<span style="color: #bbbbbb"></span>
}<span style="color: #bbbbbb"></span>
<span style="color: #228B22">// more statements</span>
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Finding the new matrix elements</p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #00688B; font-weight: bold">void</span><span style="color: #bbbbbb"> </span><span style="color: #008b45">Jacobi_rotate</span><span style="color: #bbbbbb"> </span>(<span style="color: #bbbbbb"> </span>mat<span style="color: #bbbbbb"> </span>A,<span style="color: #bbbbbb"> </span>mat<span style="color: #bbbbbb"> </span>R,<span style="color: #bbbbbb"> </span><span style="color: #00688B; font-weight: bold">int</span><span style="color: #bbbbbb"> </span>k,<span style="color: #bbbbbb"> </span><span style="color: #00688B; font-weight: bold">int</span><span style="color: #bbbbbb"> </span>l,<span style="color: #bbbbbb"> </span><span style="color: #00688B; font-weight: bold">int</span><span style="color: #bbbbbb"> </span>n<span style="color: #bbbbbb"> </span>)<span style="color: #bbbbbb"></span>
{<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">  </span><span style="color: #00688B; font-weight: bold">double</span><span style="color: #bbbbbb"> </span>s,<span style="color: #bbbbbb"> </span>c;<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">  </span><span style="color: #8B008B; font-weight: bold">if</span><span style="color: #bbbbbb"> </span>(<span style="color: #bbbbbb"> </span>A(k,l)<span style="color: #bbbbbb"> </span>!=<span style="color: #bbbbbb"> </span><span style="color: #B452CD">0.0</span><span style="color: #bbbbbb"> </span>)<span style="color: #bbbbbb"> </span>{<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span><span style="color: #00688B; font-weight: bold">double</span><span style="color: #bbbbbb"> </span>t,<span style="color: #bbbbbb"> </span>tau;<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span>tau<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span>(A(l,l)<span style="color: #bbbbbb"> </span>-<span style="color: #bbbbbb"> </span>A(k,k))/(<span style="color: #B452CD">2</span>*A(k,l));<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span>
<span style="color: #bbbbbb">    </span><span style="color: #8B008B; font-weight: bold">if</span><span style="color: #bbbbbb"> </span>(<span style="color: #bbbbbb"> </span>tau<span style="color: #bbbbbb"> </span>&gt;=<span style="color: #bbbbbb"> </span><span style="color: #B452CD">0</span><span style="color: #bbbbbb"> </span>)<span style="color: #bbbbbb"> </span>{<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">      </span>t<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span><span style="color: #B452CD">1.0</span>/(tau<span style="color: #bbbbbb"> </span>+<span style="color: #bbbbbb"> </span>sqrt(<span style="color: #B452CD">1.0</span><span style="color: #bbbbbb"> </span>+<span style="color: #bbbbbb"> </span>tau*tau));<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span>}<span style="color: #bbbbbb"> </span><span style="color: #8B008B; font-weight: bold">else</span><span style="color: #bbbbbb"> </span>{<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">      </span>t<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span><span style="color: #B452CD">-1.0</span>/(-tau<span style="color: #bbbbbb"> </span>+sqrt(<span style="color: #B452CD">1.0</span><span style="color: #bbbbbb"> </span>+<span style="color: #bbbbbb"> </span>tau*tau));<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span>}<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span>
<span style="color: #bbbbbb">    </span>c<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span><span style="color: #B452CD">1</span>/sqrt(<span style="color: #B452CD">1</span>+t*t);<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span>s<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span>c*t;<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">  </span>}<span style="color: #bbbbbb"> </span><span style="color: #8B008B; font-weight: bold">else</span><span style="color: #bbbbbb"> </span>{<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span>c<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span><span style="color: #B452CD">1.0</span>;<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span>s<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span><span style="color: #B452CD">0.0</span>;<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">  </span>}<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">  </span><span style="color: #00688B; font-weight: bold">double</span><span style="color: #bbbbbb"> </span>a_kk,<span style="color: #bbbbbb"> </span>a_ll,<span style="color: #bbbbbb"> </span>a_ik,<span style="color: #bbbbbb"> </span>a_il,<span style="color: #bbbbbb"> </span>r_ik,<span style="color: #bbbbbb"> </span>r_il;<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">  </span>a_kk<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span>A(k,k);<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">  </span>a_ll<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span>A(l,l);<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">  </span>A(k,k)<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span>c*c*a_kk<span style="color: #bbbbbb"> </span>-<span style="color: #bbbbbb"> </span><span style="color: #B452CD">2.0</span>*c*s*A(k,l)<span style="color: #bbbbbb"> </span>+<span style="color: #bbbbbb"> </span>s*s*a_ll;<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">  </span>A(l,l)<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span>s*s*a_kk<span style="color: #bbbbbb"> </span>+<span style="color: #bbbbbb"> </span><span style="color: #B452CD">2.0</span>*c*s*A(k,l)<span style="color: #bbbbbb"> </span>+<span style="color: #bbbbbb"> </span>c*c*a_ll;<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">  </span>A(k,l)<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span><span style="color: #B452CD">0.0</span>;<span style="color: #bbbbbb">  </span><span style="color: #228B22">// hard-coding non-diagonal elements by hand</span>
<span style="color: #bbbbbb">  </span>A(l,k)<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span><span style="color: #B452CD">0.0</span>;<span style="color: #bbbbbb">  </span><span style="color: #228B22">// same here</span>
<span style="color: #bbbbbb">  </span><span style="color: #8B008B; font-weight: bold">for</span><span style="color: #bbbbbb"> </span>(<span style="color: #bbbbbb"> </span><span style="color: #00688B; font-weight: bold">int</span><span style="color: #bbbbbb"> </span>i<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span><span style="color: #B452CD">0</span>;<span style="color: #bbbbbb"> </span>i<span style="color: #bbbbbb"> </span>&lt;<span style="color: #bbbbbb"> </span>n;<span style="color: #bbbbbb"> </span>i++<span style="color: #bbbbbb"> </span>)<span style="color: #bbbbbb"> </span>{<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span><span style="color: #8B008B; font-weight: bold">if</span><span style="color: #bbbbbb"> </span>(<span style="color: #bbbbbb"> </span>i<span style="color: #bbbbbb"> </span>!=<span style="color: #bbbbbb"> </span>k<span style="color: #bbbbbb"> </span>&amp;&amp;<span style="color: #bbbbbb"> </span>i<span style="color: #bbbbbb"> </span>!=<span style="color: #bbbbbb"> </span>l<span style="color: #bbbbbb"> </span>)<span style="color: #bbbbbb"> </span>{<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">      </span>a_ik<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span>A(i,k);<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">      </span>a_il<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span>A(i,l);<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">      </span>A(i,k)<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span>c*a_ik<span style="color: #bbbbbb"> </span>-<span style="color: #bbbbbb"> </span>s*a_il;<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">      </span>A(k,i)<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span>A(i,k);<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">      </span>A(i,l)<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span>c*a_il<span style="color: #bbbbbb"> </span>+<span style="color: #bbbbbb"> </span>s*a_ik;<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">      </span>A(l,i)<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span>A(i,l);<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span>}<span style="color: #bbbbbb"></span>
<span style="color: #228B22">//  And finally the new eigenvectors</span>
<span style="color: #bbbbbb">    </span>r_ik<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span>R(i,k);<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span>r_il<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span>R(i,l);<span style="color: #bbbbbb"></span>

<span style="color: #bbbbbb">    </span>R(i,k)<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span>c*r_ik<span style="color: #bbbbbb"> </span>-<span style="color: #bbbbbb"> </span>s*r_il;<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span>R(i,l)<span style="color: #bbbbbb"> </span>=<span style="color: #bbbbbb"> </span>c*r_il<span style="color: #bbbbbb"> </span>+<span style="color: #bbbbbb"> </span>s*r_ik;<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">  </span>}<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">  </span><span style="color: #8B008B; font-weight: bold">return</span>;<span style="color: #bbbbbb"></span>
}<span style="color: #bbbbbb"> </span><span style="color: #228B22">// end of function jacobi_rotate</span>
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
<h2 id="harmonic-oscillator-problem-with-and-without-interactions">Harmonic oscillator problem with and without interactions </h2>

<p>In project 1 we rewrote our original differential equation in terms of a discretized equation with approximations to the 
derivatives as
</p>
$$
    -\frac{u_{i+1} -2u_i +u_{i-i}}{h^2}=f(x_i,u(x_i)),
$$

<p>with \( i=1,2,\dots, n \). We need to add to this system the two boundary conditions \( u(a) =u_0 \) and \( u(b) = u_{n+1} \).
If we define a matrix
</p>
$$
    \mathbf{A} = \frac{1}{h^2}\left(\begin{array}{cccccc}
                          2 & -1 &  &   &  & \\
                          -1 & 2 & -1 & & & \\
                           & -1 & 2 & -1 & &  \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           &   &  &-1  &2& -1 \\
                           &    &  &   &-1 & 2 \\
                      \end{array} \right)
$$

<p>and the corresponding vectors \( \mathbf{u} = (u_1, u_2, \dots,u_n)^T \) and 
\( \mathbf{f}(\mathbf{u}) = f(x_1,x_2,\dots, x_n,u_1, u_2, \dots,u_n)^T \)  we can rewrite the differential equation
including the boundary conditions as a system of linear equations with  a large number of unknowns 
</p>
$$
   \mathbf{A}\mathbf{u} = \mathbf{f}(\mathbf{u}).
 $$

<p>We are first interested in the solution of the radial part of Schroedinger's equation for one electron. This equation reads</p>
$$
  -\frac{\hbar^2}{2 m} \left ( \frac{1}{r^2} \frac{d}{dr} r^2
  \frac{d}{dr} - \frac{l (l + 1)}{r^2} \right )R(r) 
     + V(r) R(r) = E R(r).
$$

<p>In our case \( V(r) \) is the harmonic oscillator potential \( (1/2)kr^2 \) with
\( k=m\omega^2 \) and \( E \) is
the energy of the harmonic oscillator in three dimensions.
The oscillator frequency is \( \omega \) and the energies are
</p>
$$
E_{nl}=  \hbar \omega \left(2n+l+\frac{3}{2}\right),
$$

<p>with \( n=0,1,2,\dots \) and \( l=0,1,2,\dots \).</p>

<p>Since we have made a transformation to spherical coordinates it means that 
\( r\in [0,\infty) \).  
The quantum number
\( l \) is the orbital momentum of the electron.   Then we substitute \( R(r) = (1/r) u(r) \) and obtain
</p>
$$
  -\frac{\hbar^2}{2 m} \frac{d^2}{dr^2} u(r) 
       + \left ( V(r) + \frac{l (l + 1)}{r^2}\frac{\hbar^2}{2 m}
                                    \right ) u(r)  = E u(r) .
$$

<p>The boundary conditions are \( u(0)=0 \) and \( u(\infty)=0 \).</p>

<p>We introduce a dimensionless variable \( \rho = (1/\alpha) r \)
where \( \alpha \) is a constant with dimension length and get
</p>
$$
  -\frac{\hbar^2}{2 m \alpha^2} \frac{d^2}{d\rho^2} u(\rho) 
       + \left ( V(\rho) + \frac{l (l + 1)}{\rho^2}
         \frac{\hbar^2}{2 m\alpha^2} \right ) u(\rho)  = E u(\rho) .
$$

<p>In project 2 we choose \( l=0 \).
Inserting \( V(\rho) = (1/2) k \alpha^2\rho^2 \) we end up with
</p>
$$
  -\frac{\hbar^2}{2 m \alpha^2} \frac{d^2}{d\rho^2} u(\rho) 
       + \frac{k}{2} \alpha^2\rho^2u(\rho)  = E u(\rho) .
$$

<p>We multiply thereafter with \( 2m\alpha^2/\hbar^2 \) on both sides and obtain</p>
$$
  -\frac{d^2}{d\rho^2} u(\rho) 
       + \frac{mk}{\hbar^2} \alpha^4\rho^2u(\rho)  = \frac{2m\alpha^2}{\hbar^2}E u(\rho) .
$$

<p>We have thus</p>
$$
  -\frac{d^2}{d\rho^2} u(\rho) 
       + \frac{mk}{\hbar^2} \alpha^4\rho^2u(\rho)  = \frac{2m\alpha^2}{\hbar^2}E u(\rho) .
$$

<p>The constant \( \alpha \) can now be fixed
so that
</p>
$$
\frac{mk}{\hbar^2} \alpha^4 = 1,
$$

<p>or </p>
$$
\alpha = \left(\frac{\hbar^2}{mk}\right)^{1/4}.
$$

<p>Defining</p>
$$
\lambda = \frac{2m\alpha^2}{\hbar^2}E,
$$

<p>we can rewrite Schroedinger's equation as</p>
$$
  -\frac{d^2}{d\rho^2} u(\rho) + \rho^2u(\rho)  = \lambda u(\rho) .
$$

<p>This is the first equation to solve numerically. In three dimensions 
the eigenvalues for \( l=0 \) are 
\( \lambda_0=3,\lambda_1=7,\lambda_2=11,\dots . \)
</p>

<p>We use the by now standard
expression for the second derivative of a function \( u \)
</p>
$$
\begin{equation}
    u''=\frac{u(\rho+h) -2u(\rho) +u(\rho-h)}{h^2} +O(h^2),
\label{eq:diffoperation}
\end{equation} 
$$

<p>where \( h \) is our step.
Next we define minimum and maximum values for the variable \( \rho \),
\( \rho_{\mathrm{min}}=0 \)  and \( \rho_{\mathrm{max}} \), respectively.
You need to check your results for the energies against different values
\( \rho_{\mathrm{max}} \), since we cannot set
\( \rho_{\mathrm{max}}=\infty \). 
</p>

<p>With a given number of steps, \( n_{\mathrm{step}} \), we then 
define the step \( h \) as
</p>
$$
  h=\frac{\rho_{\mathrm{max}}-\rho_{\mathrm{min}} }{n_{\mathrm{step}}}.
$$

<p>Define an arbitrary value of \( \rho \) as </p>
$$
    \rho_i= \rho_{\mathrm{min}} + ih \hspace{1cm} i=0,1,2,\dots , n_{\mathrm{step}}
$$

<p>we can rewrite the Schr\"odinger equation for \( \rho_i \) as</p>
$$
-\frac{u(\rho_i+h) -2u(\rho_i) +u(\rho_i-h)}{h^2}+\rho_i^2u(\rho_i)  = \lambda u(\rho_i),
$$

<p>or in  a more compact way</p>
$$
-\frac{u_{i+1} -2u_i +u_{i-1}}{h^2}+\rho_i^2u_i=-\frac{u_{i+1} -2u_i +u_{i-1} }{h^2}+V_iu_i  = \lambda u_i,
$$

<p>where \( V_i=\rho_i^2 \) is the harmonic oscillator potential.</p>

<p>Define first the diagonal matrix element</p>
$$
   d_i=\frac{2}{h^2}+V_i,
$$

<p>and the non-diagonal matrix element </p>
$$
   e_i=-\frac{1}{h^2}.
$$

<p>In this case the non-diagonal matrix elements are given by a mere constant. <em>All non-diagonal matrix elements are equal</em>.</p>

<p>With these definitions the Schroedinger equation takes the following form</p>
$$
d_iu_i+e_{i-1}u_{i-1}+e_{i+1}u_{i+1}  = \lambda u_i,
$$

<p>where \( u_i \) is unknown. We can write the 
latter equation as a matrix eigenvalue problem 
</p>
$$
\begin{equation}
    \left( \begin{array}{ccccccc} d_1 & e_1 & 0   & 0    & \dots  &0     & 0 \\
                                e_1 & d_2 & e_2 & 0    & \dots  &0     &0 \\
                                0   & e_2 & d_3 & e_3  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &\dots       &d_{n_{\mathrm{step}}-2} & e_{n_{\mathrm{step}}-1}\\
                                0   & \dots & \dots & \dots  &\dots       &e_{n_{\mathrm{step}}-1} & d_{n_{\mathrm{step}}-1}
             \end{array} \right)      \left( \begin{array}{c} u_{1} \\
                                                              u_{2} \\
                                                              \dots\\ \dots\\ \dots\\
                                                              u_{n_{\mathrm{step}}-1}
             \end{array} \right)=\lambda \left( \begin{array}{c} u_{1} \\
                                                              u_{2} \\
                                                              \dots\\ \dots\\ \dots\\
                                                              u_{n_{\mathrm{step}}-1}
             \end{array} \right) 
\label{eq:sematrix}
\end{equation} 
$$

<p>or if we wish to be more detailed, we can write the tridiagonal matrix as</p>
$$
\begin{equation}
    \left( \begin{array}{ccccccc} \frac{2}{h^2}+V_1 & -\frac{1}{h^2} & 0   & 0    & \dots  &0     & 0 \\
                                -\frac{1}{h^2} & \frac{2}{h^2}+V_2 & -\frac{1}{h^2} & 0    & \dots  &0     &0 \\
                                0   & -\frac{1}{h^2} & \frac{2}{h^2}+V_3 & -\frac{1}{h^2}  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &\dots       &\frac{2}{h^2}+V_{n_{\mathrm{step}}-2} & -\frac{1}{h^2}\\
                                0   & \dots & \dots & \dots  &\dots       &-\frac{1}{h^2} & \frac{2}{h^2}+V_{n_{\mathrm{step}}-1}
             \end{array} \right)  
\label{eq:matrixse} 
\end{equation} 
$$

<p>Recall that the solutions are known via the boundary conditions at
\( i=n_{\mathrm{step}} \) and at the other end point, that is for  \( \rho_0 \).
The solution is zero in both cases.
</p>

<p>We are going to study two electrons in a harmonic oscillator well which
also interact via a repulsive Coulomb interaction.
Let us start with the single-electron equation written as
</p>
$$
  -\frac{\hbar^2}{2 m} \frac{d^2}{dr^2} u(r) 
       + \frac{1}{2}k r^2u(r)  = E^{(1)} u(r),
$$

<p>where \( E^{(1)} \) stands for the energy with one electron only.
For two electrons with no repulsive Coulomb interaction, we have the following 
Schroedinger equation
</p>
$$
\left(  -\frac{\hbar^2}{2 m} \frac{d^2}{dr_1^2} -\frac{\hbar^2}{2 m} \frac{d^2}{dr_2^2}+ \frac{1}{2}k r_1^2+ \frac{1}{2}k r_2^2\right)u(r_1,r_2)  = E^{(2)} u(r_1,r_2) .
$$

<p>Note that we deal with a two-electron wave function \( u(r_1,r_2) \) and 
two-electron energy \( E^{(2)} \).
</p>

<p>With no interaction this can be written out as the product of two
single-electron wave functions, that is we have a solution on closed form.
</p>

<p>We introduce the relative coordinate \( \mathbf{r} = \mathbf{r}_1-\mathbf{r}_2 \)
and the center-of-mass coordinate \( \mathbf{R} = 1/2(\mathbf{r}_1+\mathbf{r}_2) \).
With these new coordinates, the radial Schroedinger equation reads
</p>
$$
\left(  -\frac{\hbar^2}{m} \frac{d^2}{dr^2} -\frac{\hbar^2}{4 m} \frac{d^2}{dR^2}+ \frac{1}{4} k r^2+  kR^2\right)u(r,R)  = E^{(2)} u(r,R).
$$

<p>The equations for \( r \) and \( R \) can be separated via the ansatz for the 
wave function \( u(r,R) = \psi(r)\phi(R) \) and the energy is given by the sum
of the relative energy \( E_r \) and the center-of-mass energy \( E_R \), that
is
</p>
$$
E^{(2)}=E_r+E_R.
$$

<p>We add then the repulsive Coulomb interaction between two electrons,
namely a term 
</p>
$$
V(r_1,r_2) = \frac{\beta e^2}{|\mathbf{r}_1-\mathbf{r}_2|}=\frac{\beta e^2}{r},
$$

<p>with \( \beta e^2=1.44 \) eVnm.</p>

<p>Adding this term, the \( r \)-dependent Schroedinger equation becomes</p>
$$
\left(  -\frac{\hbar^2}{m} \frac{d^2}{dr^2}+ \frac{1}{4}k r^2+\frac{\beta e^2}{r}\right)\psi(r)  = E_r \psi(r).
$$

<p>This equation is similar to the one we had previously in parts (a) and (b) 
and we introduce
again a dimensionless variable \( \rho = r/\alpha \). Repeating the same
steps, we arrive at 
</p>
$$
  -\frac{d^2}{d\rho^2} \psi(\rho) 
       + \frac{mk}{4\hbar^2} \alpha^4\rho^2\psi(\rho)+\frac{m\alpha \beta e^2}{\rho\hbar^2}\psi(\rho)  = 
\frac{m\alpha^2}{\hbar^2}E_r \psi(\rho) .
$$

<p>We want to manipulate this equation further to make it as similar to that in (a)
as possible. We define a 'frequency' 
</p>
$$
\omega_r^2=\frac{1}{4}\frac{mk}{\hbar^2} \alpha^4,
$$

<p>and fix the constant \( \alpha \) by requiring </p>
$$
\frac{m\alpha \beta e^2}{\hbar^2}=1
$$

<p>or </p>
$$
\alpha = \frac{\hbar^2}{m\beta e^2}.
$$

<p>Defining </p>
$$
\lambda = \frac{m\alpha^2}{\hbar^2}E,
$$

<p>we can rewrite Schroedinger's equation as</p>
$$
  -\frac{d^2}{d\rho^2} \psi(\rho) + \omega_r^2\rho^2\psi(\rho) +\frac{1}{\rho}\psi(\rho) = \lambda \psi(\rho).
$$

<p>We treat \( \omega_r \) as a parameter which reflects the strength of the oscillator potential.</p>

<p>Here we will study the cases \( \omega_r = 0.01 \), \( \omega_r = 0.5 \), \( \omega_r =1 \),
and \( \omega_r = 5 \)   
for the ground state only, that is the lowest-lying state.
</p>

<p>With no repulsive Coulomb interaction 
you should get a result which corresponds to 
the relative energy of a non-interacting system.   
Make sure your results are 
stable as functions of \( \rho_{\mathrm{max}} \) and the number of steps.
</p>

<p>We are only interested in the ground state with \( l=0 \). We omit the 
center-of-mass energy.
</p>

<p>For specific oscillator frequencies, the above equation has analytic answers,
see the article by M.&nbsp;Taut, Phys. Rev. A 48, 3561 - 3566 (1993).
The article can be retrieved from the following web address <a href="http://prola.aps.org/abstract/PRA/v48/i5/p3561_1" target="_blank"><tt>http://prola.aps.org/abstract/PRA/v48/i5/p3561_1</tt></a>.
</p>
<h2 id="simple-program-for-one-particle-in-a-harmonic-oscillator-trap">Simple program for one particle in a harmonic oscillator trap </h2>

<p>The following program uses the eigenvalue solver provided by Armadillo and returns the eigenvalues for the lowest states. You can run this code interactively if you use ipython notebook.</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;">%install_ext https://raw.github.com/dragly/cppmagic/master/cppmagic.py
%load_ext cppmagic
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;">%%cpp -I/usr/local/include -L/usr/local/lib -llapack -lblas -larmadillo
/*
  Solves the one-particle Schrodinger equation
  <span style="color: #8B008B; font-weight: bold">for</span> a potential specified <span style="color: #8B008B">in</span> function
  potential(). This example <span style="color: #8B008B">is</span> <span style="color: #8B008B; font-weight: bold">for</span> the harmonic oscillator <span style="color: #8B008B">in</span> <span style="color: #B452CD">3</span>d
*/
<span style="color: #228B22">#include &lt;cmath&gt;</span>
<span style="color: #228B22">#include &lt;iostream&gt;</span>
<span style="color: #228B22">#include &lt;fstream&gt;</span>
<span style="color: #228B22">#include &lt;iomanip&gt;</span>
<span style="color: #228B22">#include &lt;armadillo&gt;</span>

using namespace  std;
using namespace  arma;

double potential(double);
void output(double, double, <span style="color: #658b00">int</span>, vec&amp; );


// Begin of main program   

<span style="color: #658b00">int</span> main(<span style="color: #658b00">int</span> argc, char* argv[])
{
  <span style="color: #658b00">int</span>       i, j, Dim, lOrbital;
  double    RMin, RMax, Step, DiagConst, NondiagConst, OrbitalFactor; 
  // With spherical coordinates RMin = <span style="color: #B452CD">0</span> always
  RMin = <span style="color: #B452CD">0.0</span>;

  RMax = <span style="color: #B452CD">8.0</span>;  lOrbital = <span style="color: #B452CD">0</span>;  Dim =<span style="color: #B452CD">2000</span>;  
  mat Hamiltonian = zeros&lt;mat&gt;(Dim,Dim);
  // Integration step length
  Step    = RMax/ Dim;
  DiagConst = <span style="color: #B452CD">2.0</span> / (Step*Step);
  NondiagConst =  -<span style="color: #B452CD">1.0</span> / (Step*Step);
  OrbitalFactor = lOrbital * (lOrbital + <span style="color: #B452CD">1.0</span>);
  
  // local memory <span style="color: #8B008B; font-weight: bold">for</span> r <span style="color: #8B008B">and</span> the potential w[r] 
  vec r(Dim); vec w(Dim);
  <span style="color: #8B008B; font-weight: bold">for</span>(i = <span style="color: #B452CD">0</span>; i &lt; Dim; i++) {
    r(i) = RMin + (i+<span style="color: #B452CD">1</span>) * Step;
    w(i) = potential(r(i)) + OrbitalFactor/(r(i) * r(i));
  }


  // Setting up tridiagonal matrix <span style="color: #8B008B">and</span> brute diagonalization using Armadillo
  Hamiltonian(<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>) = DiagConst + w(<span style="color: #B452CD">0</span>);
  Hamiltonian(<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>) = NondiagConst;
  <span style="color: #8B008B; font-weight: bold">for</span>(i = <span style="color: #B452CD">1</span>; i &lt; Dim-<span style="color: #B452CD">1</span>; i++) {
    Hamiltonian(i,i-<span style="color: #B452CD">1</span>)    = NondiagConst;
    Hamiltonian(i,i)    = DiagConst + w(i);
    Hamiltonian(i,i+<span style="color: #B452CD">1</span>)    = NondiagConst;
  }
  Hamiltonian(Dim-<span style="color: #B452CD">1</span>,Dim-<span style="color: #B452CD">2</span>) = NondiagConst;
  Hamiltonian(Dim-<span style="color: #B452CD">1</span>,Dim-<span style="color: #B452CD">1</span>) = DiagConst + w(Dim-<span style="color: #B452CD">1</span>);
  // diagonalize <span style="color: #8B008B">and</span> obtain eigenvalues
  vec Eigval(Dim);
  eig_sym(Eigval, Hamiltonian);
  output(RMin , RMax, Dim, Eigval);

  <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">0</span>;
}  //  end of main function


/*
  The function potential()
  calculates <span style="color: #8B008B">and</span> <span style="color: #8B008B; font-weight: bold">return</span> the value of the 
  potential <span style="color: #8B008B; font-weight: bold">for</span> a given argument x.
  The potential here <span style="color: #8B008B">is</span> <span style="color: #8B008B; font-weight: bold">for</span> the hydrogen atom
*/        

double potential(double x)
{
  <span style="color: #8B008B; font-weight: bold">return</span> x*x;

} // End: function potential()  


void output(double RMin , double RMax, <span style="color: #658b00">int</span> Dim, vec&amp; d)
{
  <span style="color: #658b00">int</span> i;
  cout &lt;&lt; <span style="color: #CD5555">&quot;RESULTS:&quot;</span> &lt;&lt; endl;
  cout &lt;&lt; setiosflags(ios::showpoint | ios::uppercase);
  cout &lt;&lt;<span style="color: #CD5555">&quot;Rmin = &quot;</span> &lt;&lt; setw(<span style="color: #B452CD">15</span>) &lt;&lt; setprecision(<span style="color: #B452CD">8</span>) &lt;&lt; RMin &lt;&lt; endl;  
  cout &lt;&lt;<span style="color: #CD5555">&quot;Rmax = &quot;</span> &lt;&lt; setw(<span style="color: #B452CD">15</span>) &lt;&lt; setprecision(<span style="color: #B452CD">8</span>) &lt;&lt; RMax &lt;&lt; endl;  
  cout &lt;&lt;<span style="color: #CD5555">&quot;Number of steps = &quot;</span> &lt;&lt; setw(<span style="color: #B452CD">15</span>) &lt;&lt; Dim &lt;&lt; endl;  
  cout &lt;&lt; <span style="color: #CD5555">&quot;Five lowest eigenvalues:&quot;</span> &lt;&lt; endl;
  <span style="color: #8B008B; font-weight: bold">for</span>(i = <span style="color: #B452CD">0</span>; i &lt; <span style="color: #B452CD">5</span>; i++) {
    cout &lt;&lt; setw(<span style="color: #B452CD">15</span>) &lt;&lt; setprecision(<span style="color: #B452CD">8</span>) &lt;&lt; d[i] &lt;&lt; endl;
  }
}  // end of function output         
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
<h2 id="unit-testing">Unit Testing </h2>

<p>Unit Testing is the practice of testing the smallest testable parts,
called units, of an application individually and independently to
determine if they behave exactly as expected. 
</p>

<p>Unit tests (short code
fragments) are usually written such that they can be preformed at any
time during the development to continually verify the behavior of the
code. 
</p>

<p>In this way, possible bugs will be identified early in the
development cycle, making the debugging at later stages much
easier. 
</p>

<p>There are many benefits associated with Unit Testing, such as</p>
<ul>
  <li> It increases confidence in changing and maintaining code. Big changes can be made to the code quickly, since the tests will ensure that everything still is working properly.</li>
  <li> Since the code needs to be modular to make Unit Testing possible, the code will be easier to reuse. This improves the code design.</li>
  <li> Debugging is easier, since when a test fails, only the latest changes need to be debugged.</li>
<ul>
   <li> Different parts of a project can be tested without the need to wait for the other parts to be available.</li>
</ul>
  <li> A unit test can serve as a documentation on the functionality of a unit of the code.</li>
</ul>
<h2 id="simple-example-of-unit-test">Simple example of unit test </h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #1e889b">#include</span><span style="color: #bbbbbb"> </span><span style="color: #228B22">&lt;unittest++/UnitTest++.h&gt;</span><span style="color: #1e889b"></span>

<span style="color: #8B008B; font-weight: bold">class</span> <span style="color: #008b45; font-weight: bold">MyMultiplyClass</span>{<span style="color: #bbbbbb"></span>
<span style="color: #8B008B; font-weight: bold">public</span>:<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span><span style="color: #00688B; font-weight: bold">double</span><span style="color: #bbbbbb"> </span>multiply(<span style="color: #00688B; font-weight: bold">double</span><span style="color: #bbbbbb"> </span>x,<span style="color: #bbbbbb"> </span><span style="color: #00688B; font-weight: bold">double</span><span style="color: #bbbbbb"> </span>y)<span style="color: #bbbbbb"> </span>{<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">        </span><span style="color: #8B008B; font-weight: bold">return</span><span style="color: #bbbbbb"> </span>x<span style="color: #bbbbbb"> </span>*<span style="color: #bbbbbb"> </span>y;<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span>}<span style="color: #bbbbbb"></span>
};<span style="color: #bbbbbb"></span>

TEST(MyMath)<span style="color: #bbbbbb"> </span>{<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span>MyMultiplyClass<span style="color: #bbbbbb"> </span>my;<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span>CHECK_EQUAL(<span style="color: #B452CD">56</span>,<span style="color: #bbbbbb"> </span>my.multiply(<span style="color: #B452CD">7</span>,<span style="color: #B452CD">8</span>));<span style="color: #bbbbbb"></span>
}<span style="color: #bbbbbb"></span>

<span style="color: #00688B; font-weight: bold">int</span><span style="color: #bbbbbb"> </span>main()<span style="color: #bbbbbb"></span>
{<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span><span style="color: #8B008B; font-weight: bold">return</span><span style="color: #bbbbbb"> </span>UnitTest::RunAllTests();<span style="color: #bbbbbb"></span>
}<span style="color: #bbbbbb"></span>
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>And without classes</p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #1e889b">#include</span><span style="color: #bbbbbb"> </span><span style="color: #228B22">&lt;unittest++/UnitTest++.h&gt;</span><span style="color: #1e889b"></span>


<span style="color: #00688B; font-weight: bold">double</span><span style="color: #bbbbbb"> </span><span style="color: #008b45">multiply</span>(<span style="color: #00688B; font-weight: bold">double</span><span style="color: #bbbbbb"> </span>x,<span style="color: #bbbbbb"> </span><span style="color: #00688B; font-weight: bold">double</span><span style="color: #bbbbbb"> </span>y)<span style="color: #bbbbbb"> </span>{<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span><span style="color: #8B008B; font-weight: bold">return</span><span style="color: #bbbbbb"> </span>x<span style="color: #bbbbbb"> </span>*<span style="color: #bbbbbb"> </span>y;<span style="color: #bbbbbb"></span>
}<span style="color: #bbbbbb"></span>

TEST(MyMath)<span style="color: #bbbbbb"> </span>{<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span>CHECK_EQUAL(<span style="color: #B452CD">56</span>,<span style="color: #bbbbbb"> </span>multiply(<span style="color: #B452CD">7</span>,<span style="color: #B452CD">8</span>));<span style="color: #bbbbbb"></span>
}<span style="color: #bbbbbb"></span>

<span style="color: #00688B; font-weight: bold">int</span><span style="color: #bbbbbb"> </span>main()<span style="color: #bbbbbb"></span>
{<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span><span style="color: #8B008B; font-weight: bold">return</span><span style="color: #bbbbbb"> </span>UnitTest::RunAllTests();<span style="color: #bbbbbb"></span>
}<span style="color: #bbbbbb"> </span>
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>For Fortran users, the link at <a href="http://sourceforge.net/projects/fortranxunit/" target="_blank"><tt>http://sourceforge.net/projects/fortranxunit/</tt></a> contains a similar
software for unit testing. 
</p>

<p>There are many types of <b>unit test</b> libraries. One which is very popular with C++ programmers is <a href="https://github.com/philsquared/Catch/blob/master/docs/tutorial.md" target="_blank">Catch</a></p>

<p>Catch is header only. All you need to do is drop the file(s) somewhere reachable from your project - either in some central location you can set your header search path to find, or directly into your project tree itself! </p>

<p>This is a particularly good option for other Open-Source projects that want to use Catch for their test suite.</p>
<h2 id="examples">Examples </h2>

<p>Computing factorials</p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #8B008B; font-weight: bold">inline</span><span style="color: #bbbbbb"> </span><span style="color: #00688B; font-weight: bold">unsigned</span><span style="color: #bbbbbb"> </span><span style="color: #00688B; font-weight: bold">int</span><span style="color: #bbbbbb"> </span>Factorial(<span style="color: #bbbbbb"> </span><span style="color: #00688B; font-weight: bold">unsigned</span><span style="color: #bbbbbb"> </span><span style="color: #00688B; font-weight: bold">int</span><span style="color: #bbbbbb"> </span>number<span style="color: #bbbbbb"> </span>)<span style="color: #bbbbbb"> </span>{<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">  </span><span style="color: #8B008B; font-weight: bold">return</span><span style="color: #bbbbbb"> </span>number<span style="color: #bbbbbb"> </span>&gt;<span style="color: #bbbbbb"> </span><span style="color: #B452CD">1</span><span style="color: #bbbbbb"> </span>?<span style="color: #bbbbbb"> </span>Factorial(number<span style="color: #B452CD">-1</span>)*number :<span style="color: #bbbbbb"> </span><span style="color: #B452CD">1</span>;<span style="color: #bbbbbb"></span>
}<span style="color: #bbbbbb"></span>
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
<h2 id="factorial-example">Factorial Example </h2>

<p>Simple test where we put everything in a single file </p>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #1e889b">#define CATCH_CONFIG_MAIN  </span><span style="color: #228B22">// This tells Catch to provide a main()</span>
<span style="color: #1e889b">#include</span><span style="color: #bbbbbb"> </span><span style="color: #228B22">&quot;catch.hpp&quot;</span><span style="color: #1e889b"></span>
<span style="color: #8B008B; font-weight: bold">inline</span><span style="color: #bbbbbb"> </span><span style="color: #00688B; font-weight: bold">unsigned</span><span style="color: #bbbbbb"> </span><span style="color: #00688B; font-weight: bold">int</span><span style="color: #bbbbbb"> </span>Factorial(<span style="color: #bbbbbb"> </span><span style="color: #00688B; font-weight: bold">unsigned</span><span style="color: #bbbbbb"> </span><span style="color: #00688B; font-weight: bold">int</span><span style="color: #bbbbbb"> </span>number<span style="color: #bbbbbb"> </span>)<span style="color: #bbbbbb"> </span>{<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">  </span><span style="color: #8B008B; font-weight: bold">return</span><span style="color: #bbbbbb"> </span>number<span style="color: #bbbbbb"> </span>&gt;<span style="color: #bbbbbb"> </span><span style="color: #B452CD">1</span><span style="color: #bbbbbb"> </span>?<span style="color: #bbbbbb"> </span>Factorial(number<span style="color: #B452CD">-1</span>)*number :<span style="color: #bbbbbb"> </span><span style="color: #B452CD">1</span>;<span style="color: #bbbbbb"></span>
}<span style="color: #bbbbbb"></span>

TEST_CASE(<span style="color: #bbbbbb"> </span><span style="color: #CD5555">&quot;Factorials are computed&quot;</span>,<span style="color: #bbbbbb"> </span><span style="color: #CD5555">&quot;[factorial]&quot;</span><span style="color: #bbbbbb"> </span>)<span style="color: #bbbbbb"> </span>{<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span>REQUIRE(<span style="color: #bbbbbb"> </span>Factorial(<span style="color: #B452CD">0</span>)<span style="color: #bbbbbb"> </span>==<span style="color: #bbbbbb"> </span><span style="color: #B452CD">1</span><span style="color: #bbbbbb"> </span>);<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span>REQUIRE(<span style="color: #bbbbbb"> </span>Factorial(<span style="color: #B452CD">1</span>)<span style="color: #bbbbbb"> </span>==<span style="color: #bbbbbb"> </span><span style="color: #B452CD">1</span><span style="color: #bbbbbb"> </span>);<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span>REQUIRE(<span style="color: #bbbbbb"> </span>Factorial(<span style="color: #B452CD">2</span>)<span style="color: #bbbbbb"> </span>==<span style="color: #bbbbbb"> </span><span style="color: #B452CD">2</span><span style="color: #bbbbbb"> </span>);<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span>REQUIRE(<span style="color: #bbbbbb"> </span>Factorial(<span style="color: #B452CD">3</span>)<span style="color: #bbbbbb"> </span>==<span style="color: #bbbbbb"> </span><span style="color: #B452CD">6</span><span style="color: #bbbbbb"> </span>);<span style="color: #bbbbbb"></span>
<span style="color: #bbbbbb">    </span>REQUIRE(<span style="color: #bbbbbb"> </span>Factorial(<span style="color: #B452CD">10</span>)<span style="color: #bbbbbb"> </span>==<span style="color: #bbbbbb"> </span><span style="color: #B452CD">3628800</span><span style="color: #bbbbbb"> </span>);<span style="color: #bbbbbb"></span>
}<span style="color: #bbbbbb"></span>
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>This will compile to a complete executable which responds to command line arguments. If you just run it with no arguments it will execute all test cases (in this case there is just one), report any failures, report a summary of how many tests passed and failed and return the number of failed tests.</p>

<p>All we did was </p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #1e889b">#define </span>
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>one identifier and </p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #1e889b">#include </span>
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>one header and we got everything - even an implementation of main() that will respond to command line arguments. 
Once you have more than one file with unit tests in you'll just need to 
</p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #1e889b">#include</span><span style="color: #bbbbbb"> </span><span style="color: #228B22">&quot;catch.hpp&quot; </span><span style="color: #1e889b"></span>
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>and go. Usually it's a good idea to have a dedicated implementation file that just has </p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #1e889b">#define CATCH_CONFIG_MAIN </span>
<span style="color: #1e889b">#include</span><span style="color: #bbbbbb"> </span><span style="color: #228B22">&quot;catch.hpp&quot;. </span><span style="color: #1e889b"></span>
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>You can also provide your own implementation of main and drive Catch yourself.</p>

<p>We introduce test cases with the </p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;">TEST_CASE<span style="color: #bbbbbb"> </span>
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>macro.</p>

<p>The test name must be unique. You can run sets of tests by specifying a wildcarded test name or a tag expression. 
All we did was <b>define</b> one identifier and <b>include</b> one header and we got everything.
</p>

<p>We write our individual test assertions using the </p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;">REQUIRE<span style="color: #bbbbbb"> </span>
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>macro.</p>

<p>Three levels of tests</p>
<ol>
<li> Microscopic level: testing small parts of code, use often unit test libraries</li>
<li> Mesoscopic level: testing the integration of various parts  of your code</li>
<li> Macroscopic level: testing that the final result is ok</li>
</ol>
<h2 id="discussion-of-householder-s-method-for-eigenvalues">Discussion of Householder's method for eigenvalues </h2>

<p>The drawbacks with Jacobi's method are rather obvious, with perhaps the most negative feature being the fact that we cannot tell * a priori* how many transformations are needed. Can we do better?  
The answer to this is yes and is given by a clever algorithm outlined by Householder. It was ranked among the top ten algorithms in the previous century.  We will discuss this algorithm in more detail below.
</p>

<p>The first step  consists in finding
an orthogonal  matrix \( \mathbf{S} \) which is the product of \( (n-2) \) orthogonal matrices 
</p>
$$ 
   \mathbf{S}=\mathbf{S}_1\mathbf{S}_2\dots\mathbf{S}_{n-2},
$$

<p>each of which successively transforms one row and one column of \( \mathbf{A} \) into the 
required tridiagonal form. Only \( n-2 \) transformations are required, since the last two
elements are already in tridiagonal form. 
</p>

<p>In order to determine each \( \mathbf{S}_i \) let us
see what happens after the first multiplication, namely,
</p>
$$
    \mathbf{S}_1^T\mathbf{A}\mathbf{S}_1=    \left( \begin{array}{ccccccc} a_{11} & e_1 & 0   & 0    & \dots  &0     & 0 \\
                                e_1 & a'_{22} &a'_{23}  & \dots    & \dots  &\dots &a'_{2n} \\
                                0   & a'_{32} &a'_{33}  & \dots    & \dots  &\dots &a'_{3n} \\
                                0   & \dots &\dots & \dots    & \dots  &\dots & \\
                                0   & a'_{n2} &a'_{n3}  & \dots    & \dots  &\dots &a'_{nn} \\
             \end{array} \right) 
$$

<p>where the primed quantities represent a matrix \( \mathbf{A}' \) of dimension
\( n-1 \) which will subsequentely be transformed by \( \mathbf{S}_2 \).
</p>

<p>The factor  \( e_1 \) is a possibly non-vanishing element. The next
transformation produced by \( \mathbf{S}_2 \) has the same effect as  \( \mathbf{S}s \) but now on the submatirx \( \mathbf{A^{'}} \) only
</p>
$$
   \left (\mathbf{S}_{1}\mathbf{S}_{2} \right )^{T} \mathbf{A}\mathbf{S}_{1} \mathbf{S}_{2}
 = \left( \begin{array}{ccccccc} a_{11} & e_1 & 0   & 0    & \dots  &0     & 0 \\
                                e_1 & a'_{22} &e_2  & 0   & \dots  &\dots &0 \\
                                0   & e_2 &a''_{33}  & \dots    & \dots  &\dots &a''_{3n} \\
                                0   & \dots &\dots & \dots    & \dots  &\dots & \\
                                0   & 0 &a''_{n3}  & \dots    & \dots  &\dots &a''_{nn} \\
             \end{array} \right) 
$$

<em>Note that the effective size of the matrix on which we apply the transformation reduces for every new step. In the previous Jacobi method each similarity transformation is in principle performed on the full size of the original matrix</em>.

<p>After a series of such transformations, we end with a set of diagonal
matrix elements
</p>
$$
  a_{11}, a'_{22}, a''_{33}\dots a^{n-1}_{nn},
$$

<p>and off-diagonal matrix elements </p>
$$
   e_1, e_2,e_3,  \dots, e_{n-1}.
$$

<p>The resulting matrix reads</p>
$$
\mathbf{S}^{T} \mathbf{A} \mathbf{S} = 
    \left( \begin{array}{ccccccc} a_{11} & e_1 & 0   & 0    & \dots  &0     & 0 \\
                                e_1 & a'_{22} & e_2 & 0    & \dots  &0     &0 \\
                                0   & e_2 & a''_{33} & e_3  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &\dots       &a^{(n-1)}_{n-2} & e_{n-1}\\
                                0   & \dots & \dots & \dots  &\dots       &e_{n-1} & a^{(n-1)}_{nn}
             \end{array} \right) .
$$

<p>It remains to find a recipe for determining the transformation \( \mathbf{S}_n \).
We illustrate the method for \( \mathbf{S}_1 \) which we assume takes the form
</p>
$$
    \mathbf{S_{1}} = \left( \begin{array}{cc} 1 & \mathbf{0^{T}} \\
                              \mathbf{0}& \mathbf{P} \end{array} \right),
$$

<p>with \( \mathbf{0^{T}} \) being a zero row vector, \( \mathbf{0^{T}} = \{0,0,\cdots\} \)
of dimension \( (n-1) \). The matrix \( \mathbf{P} \)  is symmetric 
with dimension (\( (n-1) \times (n-1) \)) satisfying
\( \mathbf{P}^2=\mathbf{I} \)  and \( \mathbf{P}^T=\mathbf{P} \). 
A possible choice which fullfils the latter two requirements is 
</p>
$$
    \mathbf{P}=\mathbf{I}-2\mathbf{u}\mathbf{u}^T,
$$

<p>where \( \mathbf{I} \) is the \( (n-1) \) unity matrix and \( \mathbf{u} \) is an \( n-1 \)
column vector with norm \( \mathbf{u}^T\mathbf{u} \) (inner product).
</p>

<p> Note that \( \mathbf{u}\mathbf{u}^T \) is an outer product giving a
matrix of dimension (\( (n-1) \times (n-1) \)). 
Each matrix element of \( \mathbf{P} \) then reads
</p>
$$
   P_{ij}=\delta_{ij}-2u_iu_j,
$$

<p>where \( i \) and \( j \) range from \( 1 \) to \( n-1 \). Applying the transformation  
\( \mathbf{S}_1 \) results in 
</p>
$$
   \mathbf{S}_1^T\mathbf{A}\mathbf{S}_1 =  \left( \begin{array}{cc} a_{11} & (\mathbf{Pv})^T \\
                              \mathbf{Pv}& \mathbf{A}' \end{array} \right) ,
$$

<p>where \( \mathbf{v^{T}} = \{a_{21}, a_{31},\cdots, a_{n1}\} \) and $\mathbf{P}$s
must satisfy (\( \mathbf{Pv})^{T} = \{k, 0, 0,\cdots \} \). Then
</p>
$$
\begin{equation}
    \mathbf{Pv} = \mathbf{v} -2\mathbf{u}( \mathbf{u}^T\mathbf{v})= k \mathbf{e},
\label{eq:palpha}
\end{equation}
$$

<p>with \( \mathbf{e^{T}} = \{1,0,0,\dots 0\} \).</p>

<p>Solving the latter equation gives us \( \mathbf{u} \) and thus the needed transformation
\( \mathbf{P} \). We do first however need to compute the scalar \( k \) by taking the scalar
product of the last equation with its transpose and using the fact that \( \mathbf{P}^2=\mathbf{I} \).
We get then
</p>
$$
   (\mathbf{Pv})^T\mathbf{Pv} = k^{2} = \mathbf{v}^T\mathbf{v}=
   |v|^2 = \sum_{i=2}^{n}a_{i1}^2,
$$

<p>which determines the constant $ k = \pm v$.</p>

<p> Now we can rewrite Eq.&nbsp;\eqref{eq:palpha}
as 
</p>
$$
    \mathbf{v} - k\mathbf{e} = 2\mathbf{u}( \mathbf{u}^T\mathbf{v}),
$$

<p>and taking the scalar product of this equation with itself and obtain</p>
$$
\begin{equation}
    2( \mathbf{u}^T\mathbf{v})^2=(v^2\pm a_{21}v),
\label{eq:pmalpha}
\end{equation}
$$

<p>which finally determines </p>
$$
    \mathbf{u}=\frac{\mathbf{v}-k\mathbf{e}}{2( \mathbf{u}^T\mathbf{v})}.
$$

<p>In solving Eq.&nbsp;\eqref{eq:pmalpha} great care has to be exercised so as to choose
those values which make the right-hand largest in order to avoid loss of numerical
precision. 
The above steps are then repeated for every transformations till we have a 
tridiagonal matrix suitable for obtaining the eigenvalues.  
</p>

<p>Our Householder transformation has given us a tridiagonal matrix. We discuss here how one can use
Householder's iterative procedure to obtain the eigenvalues. 
Let us specialize to a \( 4\times 4  \) matrix.
The tridiagonal matrix takes the form
</p>
$$
 \mathbf{A} =
      \left( \begin{array}{cccc} 
                d_{1} & e_{1} & 0     &  0    \\
                e_{1} & d_{2} & e_{2} &  0    \\
                 0    & e_{2} & d_{3} & e_{3} \\
                 0    &   0   & e_{3} & d_{4} 
             \end{array} \right).
$$

<p>As a first observation, if any of the elements \( e_{i} \) are zero the
matrix can be separated into smaller pieces before
diagonalization. Specifically, if \( e_{1} = 0 \) then \( d_{1} \) is an
eigenvalue.
</p>

<p> Thus, let us introduce  a transformation \( \mathbf{S_{1}} \) which operates like</p>
$$
 \mathbf{S_{1}} =
      \left( \begin{array}{cccc} 
                \cos \theta & 0 & 0 & \sin \theta\\
                 0       & 0 & 0 &      0      \\
                   0        & 0 & 0 &      0      \\
               \cos \theta & 0 & 0 & \cos \theta 
             \end{array} \right)
$$

<p>Then the similarity transformation </p>
$$
\mathbf{S_{1}^{T} A  S_{1}} = \mathbf{A'} = 
      \left( \begin{array}{cccc}
              d'_{1} & e'_{1} &   0    &   0   \\
              e'_{1}  & d_{2}  & e_{2}  &   0   \\
                0    & e_{2}  & d_{3}  & e'{3} \\
                0    &   0    & e'_{3} & d'_{4}
             \end{array} \right)
$$

<p>produces a matrix where the primed elements in \( \mathbf{A'} \) have been
changed by the transformation whereas the unprimed elements are unchanged.
If we now choose \( \theta \) to
give the element \( a_{21}^{'} = e^{'}= 0 \) then we have the first
eigenvalue  \( = a_{11}^{'} = d_{1}^{'} \).
(This is actually what you are doing in project 2!!)
</p>

<p>This procedure can be continued on the remaining three-dimensional
submatrix for the next eigenvalue. Thus after few transformations    
we have the wanted diagonal form.
</p>

<p>What we see here is just a special case of the more general procedure 
developed by Francis in two articles in 1961 and 1962.
</p>

<p>The algorithm is based on the so-called <em>QR</em> method (or just <em>QR</em>-algorithm). It follows from a theorem by Schur which states that any square matrix can be written out in terms of an orthogonal matrix \( \mathbf{Q} \) and an upper triangular matrix \( \mathbf{U} \). Historically <em>R</em> was used instead of 
<em>U</em> since the wording right triangular matrix was first used.
The method is based on an iterative procedure similar to Jacobi's method, by a succession of
planar rotations. For a tridiagonal matrix it is simple to carry out in principle, but complicated in detail! We will discuss this in more detail during week 38.
</p>
<h2 id="eigenvalues-with-the-qr-and-lanczos-methods">Eigenvalues with the <em>QR</em> and Lanczos methods </h2>

<p>Our Householder transformation has given us a tridiagonal matrix. We discuss here how one can use
Jacobi's iterative procedure to obtain the eigenvalues, although it may not be the best approach. 
Let us specialize to a \( 4\times 4  \) matrix.
The tridiagonal matrix takes the form
</p>
$$
 \mathbf{A} =
      \left( \begin{array}{cccc} 
                d_{1} & e_{1} & 0     &  0    \\
                e_{1} & d_{2} & e_{2} &  0    \\
                 0    & e_{2} & d_{3} & e_{3} \\
                 0    &   0   & e_{3} & d_{4} 
             \end{array} \right).
$$

<p>As a first observation, if any of the elements \( e_{i} \) are zero the
matrix can be separated into smaller pieces before
diagonalization. Specifically, if \( e_{1} = 0 \) then \( d_{1} \) is an
eigenvalue.
</p>

<p>Thus, let us introduce  a transformation \( \mathbf{S_{1}} \) which operates like</p>
$$
 \mathbf{S_{1}} =
      \left( \begin{array}{cccc} 
                \cos \theta & 0 & 0 & \sin \theta\\
                 0       & 0 & 0 &      0      \\
                   0        & 0 & 0 &      0      \\
               \cos \theta & 0 & 0 & \cos \theta 
             \end{array} \right)
$$

<p>Then the similarity transformation </p>
$$
\mathbf{S_{1}^{T} A  S_{1}} = \mathbf{A'} = 
      \left( \begin{array}{cccc}
              d'_{1} & e'_{1} &   0    &   0   \\
              e'_{1}  & d_{2}  & e_{2}  &   0   \\
                0    & e_{2}  & d_{3}  & e'{3} \\
                0    &   0    & e'_{3} & d'_{4}
             \end{array} \right)
$$

<p>produces a matrix where the primed elements in \( \mathbf{A'} \) have been
changed by the transformation whereas the unprimed elements are unchanged.
</p>

<p>If we now choose \( \theta \) to
give the element \( a_{21}^{'} = e^{'}= 0 \) then we have the first
eigenvalue  \( = a_{11}^{'} = d_{1}^{'} \).
</p>

<p>This procedure can be continued on the remaining three-dimensional
submatrix for the next eigenvalue. Thus after few transformations    
we have the wanted diagonal form.
</p>

<p>What we see here is just a special case of the more general procedure 
developed by Francis in two articles in 1961 and 1962. Using Jacobi's method is not very efficient ether. 
</p>

<p>The algorithm is based on the so-called <b>QR</b> method (or just <b>QR</b>-algorithm). It follows from a theorem by Schur which states that any square matrix can be written out in terms of an orthogonal matrix \( \hat{Q} \) and an upper triangular matrix \( \hat{U} \). Historically \( R \) was used instead of 
\( U \) since the wording right triangular matrix was first used.
</p>

<p>The method is based on an iterative procedure similar to Jacobi's method, by a succession of
planar rotations. For a tridiagonal matrix it is simple to carry out in principle, but complicated in detail!
</p>

<p>Schur's theorem</p>
$$
\hat{A} = \hat{Q}\hat{U},
$$

<p>is used to rewrite any square matrix into a unitary matrix times an upper triangular matrix.
We say that a square matrix is similar to a triangular matrix. 
</p>

<p>Householder's algorithm which we have derived is just a special case of the general Householder algorithm. For a symmetric square matrix we obtain a tridiagonal matrix. </p>

<p>There is a corollary to Schur's theorem which states that every Hermitian matrix is unitarily similar to a diagonal matrix.</p>

<p>It follows that we can define a new matrix</p>
$$
\hat{A}\hat{Q} = \hat{Q}\hat{U}\hat{Q},
$$

<p>and multiply from the left with \( \hat{Q}^{-1} \) we get</p>
$$
\hat{Q}^{-1}\hat{A}\hat{Q} = \hat{B}=\hat{U}\hat{Q},
$$

<p>where the matrix \( \hat{B} \) is a similarity transformation of \( \hat{A} \) and has the same eigenvalues
as \( \hat{B} \). 
</p>

<p>Suppose \( \hat{A} \) is the triangular matrix we obtained after the Householder  transformation,</p>
$$
\hat{A} = \hat{Q}\hat{U},
$$

<p>and multiply from the left with \( \hat{Q}^{-1} \) resulting in</p>
$$
\hat{Q}^{-1}\hat{A} = \hat{U}.
$$

<p>Suppose that \( \hat{Q} \) consists of a series of planar Jacobi like rotations acting on sub blocks
of \( \hat{A} \) so that all elements below the diagonal are zeroed out
</p>
$$
\hat{Q}=\hat{R}_{12}\hat{R}_{23}\dots\hat{R}_{n-1,n}.
$$

<p>A transformation of the type \( \hat{R}_{12} \) looks like</p>
$$
 \hat{R}_{12} =
      \left( \begin{array}{ccccccccc} 
                 c&s &0 &0 &0 &  \dots &0 & 0 & 0\\
                 -s&c &0 &0 &0 &   \dots &0 & 0 & 0\\
                 0&0 &1 &0 &0 &   \dots &0 & 0 & 0\\
                 \dots&\dots &\dots &\dots &\dots &\dots      \\
                 0&0 &0 & 0 & 0 & \dots &1 &0 &0      \\
                 0&0 &0 & 0 & 0 & \dots &0 &1 &0      \\
                 0&0 &0 & 0 & 0 & \dots &0 &0 & 1  
             \end{array} \right)
$$

<p>The matrix \( \hat{U} \) takes then the form</p>
$$
 \hat{U} =
      \left( \begin{array}{ccccccccc} 
                 x&x &x &0 &0 &  \dots &0 & 0 & 0\\
                 0&x &x &x &0 &   \dots &0 & 0 & 0\\
                 0&0 &x &x &x &   \dots &0 & 0 & 0\\
                 \dots&\dots &\dots &\dots &\dots &\dots      \\
                 0&0 &0 & 0 & 0 & \dots &x &x &x      \\
                 0&0 &0 & 0 & 0 & \dots &0 &x &x      \\
                 0&0 &0 & 0 & 0 & \dots &0 &0 & x  
             \end{array} \right)
$$

<p>which has a second superdiagonal.</p>

<p>We have now found \( \hat{Q} \) and \( \hat{U} \) and this allows us to find the matrix \( \hat{B} \)
which is, due to Schur's theorem,  unitarily similar to a triangular matrix (upper in our case) 
since we have that 
</p>
$$
\hat{Q}^{-1}\hat{A}\hat{Q} = \hat{B}, 
$$

<p>from Schur's theorem the  matrix \( \hat{B} \) is triangular and the eigenvalues the same as those of 
\( \hat{A} \) and are given by the diagonal matrix elements of 
\( \hat{B} \). Why?  
</p>

<p>Our matrix \( \hat{B}=\hat{U}\hat{Q} \). </p>

<p>The matrix \( \hat{A} \) is transformed into a tridiagonal form and the last
step is to transform it into a diagonal matrix giving the eigenvalues
on the diagonal. 
</p>

<p>The eigenvalues of a  matrix can be obtained using the characteristic polynomial </p>
$$
P(\lambda) = det(\lambda\mathbf{I}-\mathbf{A})= \prod_{i=1}^{n}\left(\lambda_i-\lambda\right),
$$

<p>which rewritten in matrix form reads </p>
$$
P(\lambda)= \left( \begin{array}{ccccccc} d_1-\lambda & e_1 & 0   & 0    & \dots  &0     & 0 \\
                                e_1 & d_2-\lambda & e_2 & 0    & \dots  &0     &0 \\
                                0   & e_2 & d_3-\lambda & e_3  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &\dots       &d_{N_{\mathrm{step}}-2}-\lambda & e_{N_{\mathrm{step}}-1}\\
                                0   & \dots & \dots & \dots  &\dots       &e_{N_{\mathrm{step}}-1} & d_{N_{\mathrm{step}}-1}-\lambda
             \end{array} \right)
$$

<p>We can solve this equation in an iterative manner. 
We let \( P_k(\lambda) \) be the value of \( k \) subdeterminant of the above matrix of dimension
\( n\times n \). The polynomial \( P_k(\lambda) \) is clearly a polynomial of degree \( k \).
Starting with \( P_1(\lambda) \) we have \( P_1(\lambda)=d_1-\lambda \). The next polynomial reads
\( P_2(\lambda)=(d_2-\lambda)P_1(\lambda)-e_1^2 \). By expanding the determinant for \( P_k(\lambda) \) 
in terms of the minors of the $n$th column we arrive at the recursion relation
\[ 
   P_k(\lambda)=(d_k-\lambda)P_{k-1}(\lambda)-e_{k-1}^2P_{k-2}(\lambda).
\]
Together with the starting values \( P_1(\lambda) \) and \( P_2(\lambda) \) and good root searching methods
we arrive at an efficient computational scheme for finding the roots of \( P_n(\lambda) \). 
However, for large matrices this algorithm is rather inefficient and time-consuming.
</p>
<h2 id="lanczos-method">Lanczos' method </h2>

<p>Basic features with a real symmetric matrix (and normally huge \( n> 10^6 \) and sparse) 
\( \hat{A} \) of dimension \( n\times n \):
</p>

<ul>
<li> Lanczos' algorithm generates a sequence of real tridiagonal matrices \( T_k \) of dimension \( k\times k \) with \( k\le n \), with the property that the extremal eigenvalues of \( T_k \) are progressively better estimates of \( \hat{A} \)' extremal eigenvalues.* The method converges to the extremal eigenvalues.</li>
<li> The similarity transformation is</li> 
</ul>
$$
\hat{T}= \hat{Q}^{T}\hat{A}\hat{Q},
$$

<p>with the first vector \( \hat{Q}\hat{e}_1=\hat{q}_1 \).</p>

<p>We are going to solve iteratively</p>
$$
\hat{T}= \hat{Q}^{T}\hat{A}\hat{Q},
$$

<p>with the first vector \( \hat{Q}\hat{e}_1=\hat{q}_1 \).
We can write out the matrix \( \hat{Q} \) in terms of its column vectors 
</p>
$$
\hat{Q}=\left[\hat{q}_1\hat{q}_2\dots\hat{q}_n\right].
$$

<p>The matrix</p>
$$
\hat{T}= \hat{Q}^{T}\hat{A}\hat{Q},
$$

<p>can be written as </p>
$$
    \hat{T} = \left(\begin{array}{cccccc}
                           \alpha_1& \beta_1 & 0 &\dots   & \dots &0 \\
                           \beta_1 & \alpha_2 & \beta_2 &0 &\dots &0 \\
                           0& \beta_2 & \alpha_3 & \beta_3 & \dots &0 \\
                           \dots& \dots   & \dots &\dots   &\dots & 0 \\
                           \dots&   &  &\beta_{n-2}  &\alpha_{n-1}& \beta_{n-1} \\
                           0&  \dots  &\dots  &0   &\beta_{n-1} & \alpha_{n} \\
                      \end{array} \right)
$$

<p>Using the fact that </p>
$$
\hat{Q}\hat{Q}^T=\hat{I}, 
$$

<p>we can rewrite </p>
$$
\hat{T}= \hat{Q}^{T}\hat{A}\hat{Q},
$$

<p>as </p>
$$
\hat{Q}\hat{T}= \hat{A}\hat{Q}.
$$

<p>If we equate columns </p>
$$
\hat{T} = \left(\begin{array}{cccccc}
        \alpha_1& \beta_1 & 0 &\dots   & \dots &0 \\
        \beta_1 & \alpha_2 & \beta_2 &0 &\dots &0 \\
        0& \beta_2 & \alpha_3 & \beta_3 & \dots &0 \\
        \dots& \dots   & \dots &\dots   &\dots & 0 \\
        \dots&   &  &\beta_{n-2}  &\alpha_{n-1}& \beta_{n-1} \\
        0&  \dots  &\dots  &0   &\beta_{n-1} & \alpha_{n} \\
        \end{array} \right)
$$

<p>we obtain</p>
$$
\hat{A}\hat{q}_k=\beta_{k-1}\hat{q}_{k-1}+\alpha_k\hat{q}_k+\beta_k\hat{q}_{k+1}.
$$

<p>We have thus</p>
$$
\hat{A}\hat{q}_k=\beta_{k-1}\hat{q}_{k-1}+\alpha_k\hat{q}_k+\beta_k\hat{q}_{k+1},
$$

<p>with \( \beta_0\hat{q}_0=0 \) for \( k=1:n-1 \). Remember that the vectors \( \hat{q}_k \)  are orthornormal and this implies</p>
$$
\alpha_k=\hat{q}_k^T\hat{A}\hat{q}_k,
$$

<p>and these vectors are called Lanczos vectors.</p>

<p>We have thus</p>
$$
\hat{A}\hat{q}_k=\beta_{k-1}\hat{q}_{k-1}+\alpha_k\hat{q}_k+\beta_k\hat{q}_{k+1},
$$

<p>with \( \beta_0\hat{q}_0=0 \) for \( k=1:n-1 \) and </p>
$$
\alpha_k=\hat{q}_k^T\hat{A}\hat{q}_k.
$$

<p>If </p>
$$
\hat{r}_k=(\hat{A}-\alpha_k\hat{I})\hat{q}_k-\beta_{k-1}\hat{q}_{k-1},
$$

<p>is non-zero, then </p>
$$
\hat{q}_{k+1}=\hat{r}_{k}/\beta_k,
$$

<p>with \( \beta_k=\pm ||\hat{r}_{k}||_2 \).</p>
<h1 id="variational-monte-carlo-methods">Variational Monte Carlo methods </h1>
<h2 id="quantum-monte-carlo-motivation">Quantum Monte Carlo Motivation </h2>

<p>We start with the variational principle.
Given a hamiltonian \( H \) and a trial wave function \( \Psi_T \), the variational principle states that the expectation value of \( \langle H \rangle \), defined through 
</p>
$$
   E[H]= \langle H \rangle =
   \frac{\int d\boldsymbol{R}\Psi^{\ast}_T(\boldsymbol{R})H(\boldsymbol{R})\Psi_T(\boldsymbol{R})}
        {\int d\boldsymbol{R}\Psi^{\ast}_T(\boldsymbol{R})\Psi_T(\boldsymbol{R})},
$$

<p>is an upper bound to the ground state energy \( E_0 \) of the hamiltonian \( H \), that is </p>
$$
    E_0 \le \langle H \rangle .
$$

<p>In general, the integrals involved in the calculation of various  expectation values  are multi-dimensional ones. Traditional integration methods such as the Gauss-Legendre will not be adequate for say the  computation of the energy of a many-body system.</p>

<p>The trial wave function can be expanded in the eigenstates of the hamiltonian since they form a complete set, viz.,</p>
$$
   \Psi_T(\boldsymbol{R})=\sum_i a_i\Psi_i(\boldsymbol{R}),
$$

<p>and assuming the set of eigenfunctions to be normalized one obtains </p>
$$
     \frac{\sum_{nm}a^*_ma_n \int d\boldsymbol{R}\Psi^{\ast}_m(\boldsymbol{R})H(\boldsymbol{R})\Psi_n(\boldsymbol{R})}
        {\sum_{nm}a^*_ma_n \int d\boldsymbol{R}\Psi^{\ast}_m(\boldsymbol{R})\Psi_n(\boldsymbol{R})} =\frac{\sum_{n}a^2_n E_n}
        {\sum_{n}a^2_n} \ge E_0,
$$

<p>where we used that \( H(\boldsymbol{R})\Psi_n(\boldsymbol{R})=E_n\Psi_n(\boldsymbol{R}) \).
In general, the integrals involved in the calculation of various  expectation
values  are multi-dimensional ones. 
The variational principle yields the lowest state of a given symmetry.
</p>

<p>In most cases, a wave function has only small values in large parts of 
configuration space, and a straightforward procedure which uses
homogenously distributed random points in configuration space 
will most likely lead to poor results. This may suggest that some kind
of importance sampling combined with e.g., the Metropolis algorithm 
may be  a more efficient way of obtaining the ground state energy.
The hope is then that those regions of configurations space where
the wave function assumes appreciable values are sampled more 
efficiently. 
</p>

<p>The tedious part in a VMC calculation is the search for the variational
minimum. A good knowledge of the system is required in order to carry out
reasonable VMC calculations. This is not always the case, 
and often VMC calculations 
serve rather as the starting
point for so-called diffusion Monte Carlo calculations (DMC). DMC is a way of
solving exactly the many-body Schroedinger equation by means of 
a stochastic procedure. A good guess on the binding energy
and its wave function is however necessary. 
A carefully performed VMC calculation can aid in this context. 
</p>

<p>The basic recipe in a VMC calculation consists of the following elements:</p>

<ul>
<li> Construct first a trial wave function \( \psi_T(\boldsymbol{R},\boldsymbol{\alpha}) \),  for a many-body system consisting of \( N \) particles located at positions  \( \boldsymbol{R}=(\boldsymbol{R}_1,\dots ,\boldsymbol{R}_N) \). The trial wave function depends on \( \alpha \) variational parameters \( \boldsymbol{\alpha}=(\alpha_1,\dots ,\alpha_M) \).</li>
<li> Then we evaluate the expectation value of the hamiltonian \( H \)</li> 
</ul>
$$
   E[H]=\langle H \rangle =
   \frac{\int d\boldsymbol{R}\Psi^{\ast}_{T}(\boldsymbol{R},\boldsymbol{\alpha})H(\boldsymbol{R})\Psi_{T}(\boldsymbol{R},\boldsymbol{\alpha})}
        {\int d\boldsymbol{R}\Psi^{\ast}_{T}(\boldsymbol{R},\boldsymbol{\alpha})\Psi_{T}(\boldsymbol{R},\boldsymbol{\alpha})}.
$$

<ul>
<li> Thereafter we vary \( \alpha \) according to some minimization algorithm and return to the first step.</li>
</ul>
<p>With a trial wave function \( \psi_T(\boldsymbol{R}) \) we can in turn construct the quantum mechanical probability distribution</p>
$$
   P(\boldsymbol{R})= \frac{\left|\psi_T(\boldsymbol{R})\right|^2}{\int \left|\psi_T(\boldsymbol{R})\right|^2d\boldsymbol{R}}.
$$

<p>This is our new probability distribution function  (PDF).
The approximation to the expectation value of the Hamiltonian is now 
</p>
$$
   E[H(\boldsymbol{\alpha})] = 
   \frac{\int d\boldsymbol{R}\Psi^{\ast}_T(\boldsymbol{R},\boldsymbol{\alpha})H(\boldsymbol{R})\Psi_T(\boldsymbol{R},\boldsymbol{\alpha})}
        {\int d\boldsymbol{R}\Psi^{\ast}_T(\boldsymbol{R},\boldsymbol{\alpha})\Psi_T(\boldsymbol{R},\boldsymbol{\alpha})}.
$$

<p>Define a new quantity</p>
$$
   E_L(\boldsymbol{R},\boldsymbol{\alpha})=\frac{1}{\psi_T(\boldsymbol{R},\boldsymbol{\alpha})}H\psi_T(\boldsymbol{R},\boldsymbol{\alpha}),
\label{eq:locale1}
$$

<p>called the local energy, which, together with our trial PDF yields</p>
$$
  E[H(\boldsymbol{\alpha})]=\int P(\boldsymbol{R})E_L(\boldsymbol{R}) d\boldsymbol{R}\approx \frac{1}{N}\sum_{i=1}^NP(\boldsymbol{R_i},\boldsymbol{\alpha})E_L(\boldsymbol{R_i},\boldsymbol{\alpha})
\label{eq:vmc1}
$$

<p>with \( N \) being the number of Monte Carlo samples.</p>

<p>The Algorithm for performing a variational Monte Carlo calculations runs thus as this</p>

<ul>
   <li> Initialisation: Fix the number of Monte Carlo steps. Choose an initial \( \boldsymbol{R} \) and variational parameters \( \alpha \) and calculate \( \left|\psi_T^{\alpha}(\boldsymbol{R})\right|^2 \).</li> 
   <li> Initialise the energy and the variance and start the Monte Carlo calculation.</li>
<ul>
      <li> Calculate  a trial position  \( \boldsymbol{R}_p=\boldsymbol{R}+r*step \) where \( r \) is a random variable \( r \in [0,1] \).</li>
      <li> Metropolis algorithm to accept or reject this move  \( w = P(\boldsymbol{R}_p)/P(\boldsymbol{R}) \).</li>
      <li> If the step is accepted, then we set \( \boldsymbol{R}=\boldsymbol{R}_p \).</li> 
      <li> Update averages</li>
</ul>
   <li> Finish and compute final averages.</li>
</ul>
<p>Observe that the jumping in space is governed by the variable <em>step</em>. This is Called brute-force sampling.
Need importance sampling to get more relevant sampling, see lectures below.
</p>
<h3 id="quantum-monte-carlo-hydrogen-atom">Quantum Monte Carlo: hydrogen atom </h3>

<p>The radial Schroedinger equation for the hydrogen atom can be
written as
</p>
$$
-\frac{\hbar^2}{2m}\frac{\partial^2 u(r)}{\partial r^2}-
\left(\frac{ke^2}{r}-\frac{\hbar^2l(l+1)}{2mr^2}\right)u(r)=Eu(r),
$$

<p>or with dimensionless variables</p>
$$
-\frac{1}{2}\frac{\partial^2 u(\rho)}{\partial \rho^2}-
\frac{u(\rho)}{\rho}+\frac{l(l+1)}{2\rho^2}u(\rho)-\lambda u(\rho)=0,
\label{eq:hydrodimless1}
$$

<p>with the hamiltonian</p>
$$
H=-\frac{1}{2}\frac{\partial^2 }{\partial \rho^2}-
\frac{1}{\rho}+\frac{l(l+1)}{2\rho^2}.
$$

<p>Use variational parameter \( \alpha \) in the trial
wave function 
</p>
$$
   u_T^{\alpha}(\rho)=\alpha\rho e^{-\alpha\rho}. 
\label{eq:trialhydrogen}
$$

<p>Inserting this wave function into the expression for the
local energy \( E_L \) gives
</p>
$$
   E_L(\rho)=-\frac{1}{\rho}-
              \frac{\alpha}{2}\left(\alpha-\frac{2}{\rho}\right).
$$

<p>A simple variational Monte Carlo calculation results in</p>
<table class="dotable" border="1">
<thead>
<tr><td align="center">\( \alpha \)</td> <td align="center">\( \langle H \rangle  \)</td> <td align="center">\( \sigma^2 \)</td> <td align="center">\( \sigma/\sqrt{N} \)</td> </tr>
</thead>
<tbody>
<tr><td align="center">   7.00000E-01     </td> <td align="center">   -4.57759E-01                </td> <td align="center">   4.51201E-02       </td> <td align="center">   6.71715E-04              </td> </tr>
<tr><td align="center">   8.00000E-01     </td> <td align="center">   -4.81461E-01                </td> <td align="center">   3.05736E-02       </td> <td align="center">   5.52934E-04              </td> </tr>
<tr><td align="center">   9.00000E-01     </td> <td align="center">   -4.95899E-01                </td> <td align="center">   8.20497E-03       </td> <td align="center">   2.86443E-04              </td> </tr>
<tr><td align="center">   1.00000E-00     </td> <td align="center">   -5.00000E-01                </td> <td align="center">   0.00000E+00       </td> <td align="center">   0.00000E+00              </td> </tr>
<tr><td align="center">   1.10000E+00     </td> <td align="center">   -4.93738E-01                </td> <td align="center">   1.16989E-02       </td> <td align="center">   3.42036E-04              </td> </tr>
<tr><td align="center">   1.20000E+00     </td> <td align="center">   -4.75563E-01                </td> <td align="center">   8.85899E-02       </td> <td align="center">   9.41222E-04              </td> </tr>
<tr><td align="center">   1.30000E+00     </td> <td align="center">   -4.54341E-01                </td> <td align="center">   1.45171E-01       </td> <td align="center">   1.20487E-03              </td> </tr>
</tbody>
</table>

<p>We note that at \( \alpha=1 \) we obtain the exact
result, and the variance is zero, as it should. The reason is that 
we then have the exact wave function, and the action of the hamiltionan
on the wave function
</p>
$$
   H\psi = \mathrm{constant}\times \psi,
$$

<p>yields just a constant. The integral which defines various 
expectation values involving moments of the hamiltonian becomes then
</p>
$$
   \langle H^n \rangle =
   \frac{\int d\boldsymbol{R}\Psi^{\ast}_T(\boldsymbol{R})H^n(\boldsymbol{R})\Psi_T(\boldsymbol{R})}
        {\int d\boldsymbol{R}\Psi^{\ast}_T(\boldsymbol{R})\Psi_T(\boldsymbol{R})}=
\mathrm{constant}\times\frac{\int d\boldsymbol{R}\Psi^{\ast}_T(\boldsymbol{R})\Psi_T(\boldsymbol{R})}
        {\int d\boldsymbol{R}\Psi^{\ast}_T(\boldsymbol{R})\Psi_T(\boldsymbol{R})}=\mathrm{constant}.
$$

<b>This gives an important information: the exact wave function leads to zero variance!</b>
<p>Variation is then performed by minimizing both the energy and the variance.</p>

<p>For bosons in a harmonic oscillator-like  trap we will use is a spherical (S)
 or an elliptical (E) harmonic trap in one, two and finally three
 dimensions, with the latter given by
</p>
$$
 \begin{equation}
 V_{ext}(\mathbf{r}) = \Bigg\{
 \begin{array}{ll}
	 \frac{1}{2}m\omega_{ho}^2r^2 & (S)\\
 \strut
	 \frac{1}{2}m[\omega_{ho}^2(x^2+y^2) + \omega_z^2z^2] & (E)
\label{trap_eqn}
 \end{array}
 \end{equation}
$$

<p>where (S) stands for symmetric and </p>
$$
\begin{equation}
     \hat{H} = \sum_i^N \left(
	 \frac{-\hbar^2}{2m}
	 { \bigtriangledown }_{i}^2 +
	 V_{ext}({\bf{r}}_i)\right)  +
	 \sum_{i < j}^{N} V_{int}({\bf{r}}_i,{\bf{r}}_j),
\label{_auto1}
\end{equation}
$$

<p>as the two-body Hamiltonian of the system.  </p>

<p> We will represent the inter-boson interaction by a pairwise, repulsive potential</p>
$$
\begin{equation}
 V_{int}(|\mathbf{r}_i-\mathbf{r}_j|) =  \Bigg\{
 \begin{array}{ll}
	 \infty & {|\mathbf{r}_i-\mathbf{r}_j|} \leq {a}\\
	 0 & {|\mathbf{r}_i-\mathbf{r}_j|} > {a}
 \end{array}
\label{_auto2}
\end{equation}
$$

<p> where \( a \) is the so-called hard-core diameter of the bosons.
 Clearly, \( V_{int}(|\mathbf{r}_i-\mathbf{r}_j|) \) is zero if the bosons are
 separated by a distance \( |\mathbf{r}_i-\mathbf{r}_j| \) greater than \( a \) but
 infinite if they attempt to come within a distance \( |\mathbf{r}_i-\mathbf{r}_j| \leq a \).
</p>

<p> Our trial wave function for the ground state with \( N \) atoms is given by</p>
$$
 \begin{equation}
 \Psi_T(\mathbf{R})=\Psi_T(\mathbf{r}_1, \mathbf{r}_2, \dots \mathbf{r}_N,\alpha,\beta)=\prod_i g(\alpha,\beta,\mathbf{r}_i)\prod_{i < j}f(a,|\mathbf{r}_i-\mathbf{r}_j|),
\label{eq:trialwf}
 \end{equation}
$$

<p> where \( \alpha \) and \( \beta \) are variational parameters. The
 single-particle wave function is proportional to the harmonic
 oscillator function for the ground state
</p>
$$
\begin{equation}
    g(\alpha,\beta,\mathbf{r}_i)= \exp{[-\alpha(x_i^2+y_i^2+\beta z_i^2)]}.
\label{_auto3}
\end{equation}
$$

<p>For spherical traps we have \( \beta = 1 \) and for non-interacting
bosons (\( a=0 \)) we have \( \alpha = 1/2a_{ho}^2 \).  The correlation wave
 function is
</p>
$$
 \begin{equation}
    f(a,|\mathbf{r}_i-\mathbf{r}_j|)=\Bigg\{
 \begin{array}{ll}
	 0 & {|\mathbf{r}_i-\mathbf{r}_j|} \leq {a}\\
	 (1-\frac{a}{|\mathbf{r}_i-\mathbf{r}_j|}) & {|\mathbf{r}_i-\mathbf{r}_j|} > {a}.
 \end{array}
\label{_auto4}
\end{equation}  
$$
<h3 id="a-simple-python-code-that-solves-the-two-boson-or-two-fermion-case-in-two-dimensions">A simple Python code that solves the two-boson or two-fermion case in two-dimensions </h3>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #228B22"># Importing various packages</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">math</span> <span style="color: #8B008B; font-weight: bold">import</span> exp, sqrt
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">random</span> <span style="color: #8B008B; font-weight: bold">import</span> random, seed
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">mpl_toolkits.mplot3d</span> <span style="color: #8B008B; font-weight: bold">import</span> Axes3D
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">matplotlib</span> <span style="color: #8B008B; font-weight: bold">import</span> cm
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">matplotlib.ticker</span> <span style="color: #8B008B; font-weight: bold">import</span> LinearLocator, FormatStrFormatter
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">sys</span>

<span style="color: #228B22">#Trial wave function for quantum dots in two dims</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">WaveFunction</span>(r,alpha,beta):
    r1 = r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>
    r2 = r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>
    r12 = sqrt((r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>])**<span style="color: #B452CD">2</span> + (r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>])**<span style="color: #B452CD">2</span>)
    deno = r12/(<span style="color: #B452CD">1</span>+beta*r12)
    <span style="color: #8B008B; font-weight: bold">return</span> exp(-<span style="color: #B452CD">0.5</span>*alpha*(r1+r2)+deno)

<span style="color: #228B22">#Local energy  for quantum dots in two dims, using analytical local energy</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">LocalEnergy</span>(r,alpha,beta):
    
    r1 = (r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>)
    r2 = (r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>)
    r12 = sqrt((r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>])**<span style="color: #B452CD">2</span> + (r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>])**<span style="color: #B452CD">2</span>)
    deno = <span style="color: #B452CD">1.0</span>/(<span style="color: #B452CD">1</span>+beta*r12)
    deno2 = deno*deno
    <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">0.5</span>*(<span style="color: #B452CD">1</span>-alpha*alpha)*(r1 + r2) +<span style="color: #B452CD">2.0</span>*alpha + <span style="color: #B452CD">1.0</span>/r12+deno2*(alpha*r12-deno2+<span style="color: #B452CD">2</span>*beta*deno-<span style="color: #B452CD">1.0</span>/r12)

<span style="color: #228B22"># The Monte Carlo sampling with the Metropolis algo</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">MonteCarloSampling</span>():

    NumberMCcycles= <span style="color: #B452CD">100000</span>
    StepSize = <span style="color: #B452CD">1.0</span>
    <span style="color: #228B22"># positions</span>
    PositionOld = np.zeros((NumberParticles,Dimension), np.double)
    PositionNew = np.zeros((NumberParticles,Dimension), np.double)
    <span style="color: #228B22"># seed for rng generator</span>
    seed()
    <span style="color: #228B22"># start variational parameter</span>
    alpha = <span style="color: #B452CD">0.9</span>
    <span style="color: #8B008B; font-weight: bold">for</span> ia <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(MaxVariations):
        alpha += <span style="color: #B452CD">.025</span>
        AlphaValues[ia] = alpha
        beta = <span style="color: #B452CD">0.2</span> 
        <span style="color: #8B008B; font-weight: bold">for</span> jb <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(MaxVariations):
            beta += <span style="color: #B452CD">.01</span>
            BetaValues[jb] = beta
            energy = energy2 = <span style="color: #B452CD">0.0</span>
            DeltaE = <span style="color: #B452CD">0.0</span>
            <span style="color: #228B22">#Initial position</span>
            <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberParticles):
                <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
                    PositionOld[i,j] = StepSize * (random() - <span style="color: #B452CD">.5</span>)
            wfold = WaveFunction(PositionOld,alpha,beta)

            <span style="color: #228B22">#Loop over MC MCcycles</span>
            <span style="color: #8B008B; font-weight: bold">for</span> MCcycle <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberMCcycles):
                <span style="color: #228B22">#Trial position</span>
                <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberParticles):
                    <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
                        PositionNew[i,j] = PositionOld[i,j] + StepSize * (random() - <span style="color: #B452CD">.5</span>)
                wfnew = WaveFunction(PositionNew,alpha,beta)

                <span style="color: #228B22">#Metropolis test to see whether we accept the move</span>
                <span style="color: #8B008B; font-weight: bold">if</span> random() &lt; wfnew**<span style="color: #B452CD">2</span> / wfold**<span style="color: #B452CD">2</span>:
                   PositionOld = PositionNew.copy()
                   wfold = wfnew
                   DeltaE = LocalEnergy(PositionOld,alpha,beta)
                energy += DeltaE
                energy2 += DeltaE**<span style="color: #B452CD">2</span>

            <span style="color: #228B22">#We calculate mean, variance and error ...</span>
            energy /= NumberMCcycles
            energy2 /= NumberMCcycles
            variance = energy2 - energy**<span style="color: #B452CD">2</span>
            error = sqrt(variance/NumberMCcycles)
            Energies[ia,jb] = energy    
    <span style="color: #8B008B; font-weight: bold">return</span> Energies, AlphaValues, BetaValues


<span style="color: #228B22">#Here starts the main program with variable declarations</span>
NumberParticles = <span style="color: #B452CD">2</span>
Dimension = <span style="color: #B452CD">2</span>
MaxVariations = <span style="color: #B452CD">10</span>
Energies = np.zeros((MaxVariations,MaxVariations))
AlphaValues = np.zeros(MaxVariations)
BetaValues = np.zeros(MaxVariations)
(Energies, AlphaValues, BetaValues) = MonteCarloSampling()

<span style="color: #228B22"># Prepare for plots</span>
fig = plt.figure()
ax = fig.gca(projection=<span style="color: #CD5555">&#39;3d&#39;</span>)
<span style="color: #228B22"># Plot the surface.</span>
X, Y = np.meshgrid(AlphaValues, BetaValues)
surf = ax.plot_surface(X, Y, Energies,cmap=cm.coolwarm,linewidth=<span style="color: #B452CD">0</span>, antialiased=<span style="color: #8B008B; font-weight: bold">False</span>)
<span style="color: #228B22"># Customize the z axis.</span>
zmin = np.matrix(Energies).min()
zmax = np.matrix(Energies).max()
ax.set_zlim(zmin, zmax)
ax.set_xlabel(<span style="color: #CD5555">r&#39;$\alpha$&#39;</span>)
ax.set_ylabel(<span style="color: #CD5555">r&#39;$\beta$&#39;</span>)
ax.set_zlabel(<span style="color: #CD5555">r&#39;$\langle E \rangle$&#39;</span>)
ax.zaxis.set_major_locator(LinearLocator(<span style="color: #B452CD">10</span>))
ax.zaxis.set_major_formatter(FormatStrFormatter(<span style="color: #CD5555">&#39;%.02f&#39;</span>))
<span style="color: #228B22"># Add a color bar which maps values to colors.</span>
fig.colorbar(surf, shrink=<span style="color: #B452CD">0.5</span>, aspect=<span style="color: #B452CD">5</span>)
plt.show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
<h2 id="quantum-monte-carlo-the-helium-atom">Quantum Monte Carlo: the helium atom </h2>

<p>The helium atom consists of two electrons and a nucleus with
charge \( Z=2 \). 
The contribution  
to the potential energy due to the attraction from the nucleus is
</p>
$$
   -\frac{2ke^2}{r_1}-\frac{2ke^2}{r_2},
$$

<p>and if we add the repulsion arising from the two 
interacting electrons, we obtain the potential energy
</p>
$$
 V(r_1, r_2)=-\frac{2ke^2}{r_1}-\frac{2ke^2}{r_2}+
               \frac{ke^2}{r_{12}},
$$

<p>with the electrons separated at a distance 
\( r_{12}=|\boldsymbol{r}_1-\boldsymbol{r}_2| \).
</p>

<p>The hamiltonian becomes then</p>
$$
   \hat{H}=-\frac{\hbar^2\nabla_1^2}{2m}-\frac{\hbar^2\nabla_2^2}{2m}
          -\frac{2ke^2}{r_1}-\frac{2ke^2}{r_2}+
               \frac{ke^2}{r_{12}},
$$

<p>and  Schroedingers equation reads</p>
$$
   \hat{H}\psi=E\psi.
$$

<p>All observables are evaluated with respect to the probability distribution</p>
$$
   P(\boldsymbol{R})= \frac{\left|\psi_T(\boldsymbol{R})\right|^2}{\int \left|\psi_T(\boldsymbol{R})\right|^2d\boldsymbol{R}}.
$$

<p>generated by the trial wave function.   
The trial wave function must approximate an exact 
eigenstate in order that accurate results are to be obtained. 
</p>

<p>Choice of trial wave function for Helium:
Assume \( r_1 \rightarrow 0 \).
</p>
$$
   E_L(\boldsymbol{R})=\frac{1}{\psi_T(\boldsymbol{R})}H\psi_T(\boldsymbol{R})=
     \frac{1}{\psi_T(\boldsymbol{R})}\left(-\frac{1}{2}\nabla^2_1
     -\frac{Z}{r_1}\right)\psi_T(\boldsymbol{R}) + \mathrm{finite \hspace{0.1cm}terms}.
$$

$$ 
    E_L(R)=
    \frac{1}{\mathbf{R}_T(r_1)}\left(-\frac{1}{2}\frac{d^2}{dr_1^2}-
     \frac{1}{r_1}\frac{d}{dr_1}
     -\frac{Z}{r_1}\right)\mathbf{R}_T(r_1) + \mathrm{finite\hspace{0.1cm} terms}
$$

<p>For small values of \( r_1 \), the terms which dominate are</p>
$$ 
    \lim_{r_1 \rightarrow 0}E_L(R)=
    \frac{1}{\mathbf{R}_T(r_1)}\left(-
     \frac{1}{r_1}\frac{d}{dr_1}
     -\frac{Z}{r_1}\right)\mathbf{R}_T(r_1),
$$

<p>since the second derivative does not diverge due to the finiteness of  \( \Psi \) at the origin.</p>

<p>This results in</p>
$$
     \frac{1}{\mathbf{R}_T(r_1)}\frac{d \mathbf{R}_T(r_1)}{dr_1}=-Z,
$$

<p>and</p>
$$
   \mathbf{R}_T(r_1)\propto e^{-Zr_1}.
$$

<p>A similar condition applies to electron 2 as well. 
For orbital momenta \( l > 0 \) we have 
</p>
$$
     \frac{1}{\mathbf{R}_T(r)}\frac{d \mathbf{R}_T(r)}{dr}=-\frac{Z}{l+1}.
$$

<p>Similarly, studying the case \( r_{12}\rightarrow 0 \) we can write 
a possible trial wave function as
</p>
$$
   \psi_T(\boldsymbol{R})=e^{-\alpha(r_1+r_2)}e^{\beta r_{12}}.
\label{eq:wavehelium2}
$$

<p>The last equation can be generalized to</p>
$$
   \psi_T(\boldsymbol{R})=\phi(\boldsymbol{r}_1)\phi(\boldsymbol{r}_2)\dots\phi(\boldsymbol{r}_N)
                   \prod_{i < j}f(r_{ij}),
$$

<p>for a system with \( N \) electrons or particles. </p>

<p>During the development of our code we need to make several checks. It is also very instructive to compute a closed form expression for the local energy. Since our wave function is rather simple  it is straightforward
to find an analytic expressions.  Consider first the case of the simple helium function 
</p>
$$
   \Psi_T(\boldsymbol{r}_1,\boldsymbol{r}_2) = e^{-\alpha(r_1+r_2)}
$$

<p>The local energy is for this case </p>
$$ 
E_{L1} = \left(\alpha-Z\right)\left(\frac{1}{r_1}+\frac{1}{r_2}\right)+\frac{1}{r_{12}}-\alpha^2
$$

<p>which gives an expectation value for the local energy given by</p>
$$
\langle E_{L1} \rangle = \alpha^2-2\alpha\left(Z-\frac{5}{16}\right)
$$

<p>With closed form formulae we  can speed up the computation of the correlation. In our case
we write it as 
</p>
$$
\Psi_C= \exp{\left\{\sum_{i < j}\frac{ar_{ij}}{1+\beta r_{ij}}\right\}},
$$

<p>which means that the gradient needed for the so-called quantum force and local energy 
can be calculated analytically.
This will speed up your code since the computation of the correlation part and the Slater determinant are the most 
time consuming parts in your code.  
</p>

<p>We will refer to this correlation function as \( \Psi_C \) or the <em>linear Pade-Jastrow</em>.</p>

<p>We can test this by computing the local energy for our helium wave function</p>
$$
   \psi_{T}(\boldsymbol{r}_1,\boldsymbol{r}_2) = 
   \exp{\left(-\alpha(r_1+r_2)\right)}
   \exp{\left(\frac{r_{12}}{2(1+\beta r_{12})}\right)}, 
$$

<p>with \( \alpha \) and \( \beta \) as variational parameters.</p>

<p>The local energy is for this case </p>
$$ 
E_{L2} = E_{L1}+\frac{1}{2(1+\beta r_{12})^2}\left\{\frac{\alpha(r_1+r_2)}{r_{12}}(1-\frac{\boldsymbol{r}_1\boldsymbol{r}_2}{r_1r_2})-\frac{1}{2(1+\beta r_{12})^2}-\frac{2}{r_{12}}+\frac{2\beta}{1+\beta r_{12}}\right\}
$$

<p>It is very useful to test your code against these expressions. It means also that you don't need to
compute a derivative numerically as discussed in the code example below. 
</p>

<p>For the computation of various derivatives with different types of wave functions, you will find it useful to use python with symbolic python, that is sympy, see <a href="http://docs.sympy.org/latest/index.html" target="_blank">online manual</a>.  Using sympy allows you autogenerate both Latex code as well c++, python or Fortran codes. Here you will find some simple examples. We choose 
the \( 2s \) hydrogen-orbital  (not normalized) as an example
</p>
$$
 \phi_{2s}(\boldsymbol{r}) = (Zr - 2)\exp{-(\frac{1}{2}Zr)},
$$

<p>with $ r^2 = x^2 + y^2 + z^2$.</p>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sympy</span> <span style="color: #8B008B; font-weight: bold">import</span> symbols, diff, exp, sqrt
x, y, z, Z = symbols(<span style="color: #CD5555">&#39;x y z Z&#39;</span>)
r = sqrt(x*x + y*y + z*z)
r
phi = (Z*r - <span style="color: #B452CD">2</span>)*exp(-Z*r/<span style="color: #B452CD">2</span>)
phi
diff(phi, x)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>This doesn't look very nice, but sympy provides several functions that allow for improving and simplifying the output.</p>

<p>We can improve our output by factorizing and substituting expressions</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sympy</span> <span style="color: #8B008B; font-weight: bold">import</span> symbols, diff, exp, sqrt, factor, Symbol, printing
x, y, z, Z = symbols(<span style="color: #CD5555">&#39;x y z Z&#39;</span>)
r = sqrt(x*x + y*y + z*z)
phi = (Z*r - <span style="color: #B452CD">2</span>)*exp(-Z*r/<span style="color: #B452CD">2</span>)
R = Symbol(<span style="color: #CD5555">&#39;r&#39;</span>) <span style="color: #228B22">#Creates a symbolic equivalent of r</span>
<span style="color: #228B22">#print latex and c++ code</span>
<span style="color: #658b00">print</span> printing.latex(diff(phi, x).factor().subs(r, R))
<span style="color: #658b00">print</span> printing.ccode(diff(phi, x).factor().subs(r, R))
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>We can in turn look at second derivatives</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sympy</span> <span style="color: #8B008B; font-weight: bold">import</span> symbols, diff, exp, sqrt, factor, Symbol, printing
x, y, z, Z = symbols(<span style="color: #CD5555">&#39;x y z Z&#39;</span>)
r = sqrt(x*x + y*y + z*z)
phi = (Z*r - <span style="color: #B452CD">2</span>)*exp(-Z*r/<span style="color: #B452CD">2</span>)
R = Symbol(<span style="color: #CD5555">&#39;r&#39;</span>) <span style="color: #228B22">#Creates a symbolic equivalent of r</span>
(diff(diff(phi, x), x) + diff(diff(phi, y), y) + diff(diff(phi, z), z)).factor().subs(r, R)
<span style="color: #228B22"># Collect the Z values</span>
(diff(diff(phi, x), x) + diff(diff(phi, y), y) +diff(diff(phi, z), z)).factor().collect(Z).subs(r, R)
<span style="color: #228B22"># Factorize also the r**2 terms</span>
(diff(diff(phi, x), x) + diff(diff(phi, y), y) + diff(diff(phi, z), z)).factor().collect(Z).subs(r, R).subs(r**<span style="color: #B452CD">2</span>, R**<span style="color: #B452CD">2</span>).factor()
<span style="color: #658b00">print</span> printing.ccode((diff(diff(phi, x), x) + diff(diff(phi, y), y) + diff(diff(phi, z), z)).factor().collect(Z).subs(r, R).subs(r**<span style="color: #B452CD">2</span>, R**<span style="color: #B452CD">2</span>).factor())
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>With some practice this allows one to be able to check one's own calculation and translate automatically into code lines.</p>
<h2 id="the-metropolis-algorithm">The Metropolis algorithm </h2>

<p>The Metropolis algorithm , see <a href="http://scitation.aip.org/content/aip/journal/jcp/21/6/10.1063/1.1699114" target="_blank">the original article</a> was invented by Metropolis et. al
and is often simply called the Metropolis algorithm.
It is a method to sample a normalized probability
distribution by a stochastic process. We define \( \mathbf{P}_i^{(n)} \) to
be the probability for finding the system in the state \( i \) at step \( n \).
The algorithm is then
</p>

<ul>
<li> Sample a possible new state \( j \) with some probability \( T_{i\rightarrow j} \).</li>
<li> Accept the new state \( j \) with probability \( A_{i \rightarrow j} \) and use it as the next sample. With probability \( 1-A_{i\rightarrow j} \) the move is rejected and the original state \( i \) is used again as a sample.</li>
</ul>
<p>We wish to derive the required properties of \( T \) and \( A \) such that
\( \mathbf{P}_i^{(n\rightarrow \infty)} \rightarrow p_i \) so that starting
from any distribution, the method converges to the correct distribution.
Note that the description here is for a discrete probability distribution.
Replacing probabilities \( p_i \) with expressions like \( p(x_i)dx_i \) will
take all of these over to the corresponding continuum expressions.
</p>

<p>The dynamical equation for \( \mathbf{P}_i^{(n)} \) can be written directly from
the description above. The probability of being in the state \( i \) at step \( n \)
is given by the probability of being in any state \( j \) at the previous step,
and making an accepted transition to \( i \) added to the probability of
being in the state \( i \), making a transition to any state \( j \) and
rejecting the move:
</p>
$$
\mathbf{P}^{(n)}_i = \sum_j \left [
\mathbf{P}^{(n-1)}_jT_{j\rightarrow i} A_{j\rightarrow i} 
+\mathbf{P}^{(n-1)}_iT_{i\rightarrow j}\left ( 1- A_{i\rightarrow j} \right)
\right ] \,.
$$

<p>Since the probability of making some transition must be 1,
\( \sum_j T_{i\rightarrow j} = 1 \), and the above equation becomes
</p>
$$
\mathbf{P}^{(n)}_i = \mathbf{P}^{(n-1)}_i +
 \sum_j \left [
\mathbf{P}^{(n-1)}_jT_{j\rightarrow i} A_{j\rightarrow i} 
-\mathbf{P}^{(n-1)}_iT_{i\rightarrow j}A_{i\rightarrow j}
\right ] \,.
$$

<p>For large \( n \) we require that \( \mathbf{P}^{(n\rightarrow \infty)}_i = p_i \),
the desired probability distribution. Taking this limit, gives the
balance requirement
</p>
$$
 \sum_j \left [
p_jT_{j\rightarrow i} A_{j\rightarrow i}
-p_iT_{i\rightarrow j}A_{i\rightarrow j}
\right ] = 0 \,.
$$

<p>The balance requirement is very weak. Typically the much stronger detailed
balance requirement is enforced, that is rather than the sum being
set to zero, we set each term separately to zero and use this
to determine the acceptance probabilities. Rearranging, the result is
</p>
$$
\frac{ A_{j\rightarrow i}}{A_{i\rightarrow j}}
= \frac{p_iT_{i\rightarrow j}}{ p_jT_{j\rightarrow i}} \,.
$$

<p>The Metropolis choice is to maximize the \( A \) values, that is</p>
$$
A_{j \rightarrow i} = \min \left ( 1,
\frac{p_iT_{i\rightarrow j}}{ p_jT_{j\rightarrow i}}\right ).
$$

<p>Other choices are possible, but they all correspond to multilplying
\( A_{i\rightarrow j} \) and \( A_{j\rightarrow i} \) by the same constant
smaller than unity.\footnote{The penalty function method uses just such
a factor to compensate for \( p_i \) that are evaluated stochastically
and are therefore noisy.}
</p>

<p>Having chosen the acceptance probabilities, we have guaranteed that
if the  \( \mathbf{P}_i^{(n)} \) has equilibrated, that is if it is equal to \( p_i \),
it will remain equilibrated. Next we need to find the circumstances for
convergence to equilibrium.
</p>

<p>The dynamical equation can be written as</p>
$$
\mathbf{P}^{(n)}_i = \sum_j M_{ij}\mathbf{P}^{(n-1)}_j
$$

<p>with the matrix \( M \) given by</p>
$$
M_{ij} = \delta_{ij}\left [ 1 -\sum_k T_{i\rightarrow k} A_{i \rightarrow k}
\right ] + T_{j\rightarrow i} A_{j\rightarrow i} \,.
$$

<p>Summing over \( i \) shows that \( \sum_i M_{ij} = 1 \), and since
\( \sum_k T_{i\rightarrow k} = 1 \), and \( A_{i \rightarrow k} \leq 1 \), the
elements of the matrix satisfy \( M_{ij} \geq 0 \). The matrix \( M \) is therefore
a stochastic matrix.
</p>

<p>The Metropolis method is simply the power method for computing the
right eigenvector of \( M \) with the largest magnitude eigenvalue.
By construction, the correct probability distribution is a right eigenvector
with eigenvalue 1. Therefore, for the Metropolis method to converge
to this result, we must show that \( M \) has only one eigenvalue with this
magnitude, and all other eigenvalues are smaller.
</p>
<h2 id="importance-sampling">Importance sampling </h2>

<p>We need to replace the brute force
Metropolis algorithm with a walk in coordinate space biased by the trial wave function.
This approach is based on the Fokker-Planck equation and the Langevin equation for generating a trajectory in coordinate space.  The link between the Fokker-Planck equation and the Langevin equations are explained, only partly, in the slides below.
An excellent reference on topics like Brownian motion, Markov chains, the Fokker-Planck equation and the Langevin equation is the text by  <a href="http://www.elsevier.com/books/stochastic-processes-in-physics-and-chemistry/van-kampen/978-0-444-52965-7" target="_blank">Van Kampen</a>
Here we will focus first on the implementation part first.
</p>

<p>For a diffusion process characterized by a time-dependent probability density \( P(x,t) \) in one dimension the Fokker-Planck
equation reads (for one particle /walker) 
</p>
$$
   \frac{\partial P}{\partial t} = D\frac{\partial }{\partial x}\left(\frac{\partial }{\partial x} -F\right)P(x,t),
$$

<p>where \( F \) is a drift term and \( D \) is the diffusion coefficient. </p>

<p>The new positions in coordinate space are given as the solutions of the Langevin equation using Euler's method, namely,
we go from the Langevin equation
</p>
$$ 
   \frac{\partial x(t)}{\partial t} = DF(x(t)) +\eta,
$$

<p>with \( \eta \) a random variable,
yielding a new position 
</p>
$$
   y = x+DF(x)\Delta t +\xi\sqrt{\Delta t},
$$

<p>where \( \xi \) is gaussian random variable and \( \Delta t \) is a chosen time step. 
The quantity \( D \) is, in atomic units, equal to \( 1/2 \) and comes from the factor \( 1/2 \) in the kinetic energy operator. Note that \( \Delta t \) is to be viewed as a parameter. Values of \( \Delta t \in [0.001,0.01] \) yield in general rather stable values of the ground state energy.  
</p>

<p>The process of isotropic diffusion characterized by a time-dependent probability density \( P(\mathbf{x},t) \) obeys (as an approximation) the so-called Fokker-Planck equation </p>
$$
   \frac{\partial P}{\partial t} = \sum_i D\frac{\partial }{\partial \mathbf{x_i}}\left(\frac{\partial }{\partial \mathbf{x_i}} -\mathbf{F_i}\right)P(\mathbf{x},t),
$$

<p>where \( \mathbf{F_i} \) is the \( i^{th} \) component of the drift term (drift velocity) caused by an external potential, and \( D \) is the diffusion coefficient. The convergence to a stationary probability density can be obtained by setting the left hand side to zero. The resulting equation will be satisfied if and only if all the terms of the sum are equal zero,</p>
$$
\frac{\partial^2 P}{\partial {\mathbf{x_i}^2}} = P\frac{\partial}{\partial {\mathbf{x_i}}}\mathbf{F_i} + \mathbf{F_i}\frac{\partial}{\partial {\mathbf{x_i}}}P.
$$

<p>The drift vector should be of the form \( \mathbf{F} = g(\mathbf{x}) \frac{\partial P}{\partial \mathbf{x}} \). Then,</p>
$$
\frac{\partial^2 P}{\partial {\mathbf{x_i}^2}} = P\frac{\partial g}{\partial P}\left( \frac{\partial P}{\partial {\mathbf{x}_i}}  \right)^2 + P g \frac{\partial ^2 P}{\partial {\mathbf{x}_i^2}}  + g \left( \frac{\partial P}{\partial {\mathbf{x}_i}}  \right)^2.
$$

<p>The condition of stationary density means that the left hand side equals zero. In other words, the terms containing first and second derivatives have to cancel each other. It is possible only if \( g = \frac{1}{P} \), which yields</p>
$$
\mathbf{F} = 2\frac{1}{\Psi_T}\nabla\Psi_T,
$$

<p>which is known as the so-called <em>quantum force</em>. This term is responsible for pushing the walker towards regions of configuration space where the trial wave function is large, increasing the efficiency of the simulation in contrast to the Metropolis algorithm where the walker has the same probability of moving in every direction.</p>

<p>The Fokker-Planck equation yields a (the solution to the equation) transition probability given by the Green's function</p>
$$
  G(y,x,\Delta t) = \frac{1}{(4\pi D\Delta t)^{3N/2}} \exp{\left(-(y-x-D\Delta t F(x))^2/4D\Delta t\right)}
$$

<p>which in turn means that our brute force Metropolis algorithm</p>
$$ 
    A(y,x) = \mathrm{min}(1,q(y,x))),
$$

<p>with \( q(y,x) = |\Psi_T(y)|^2/|\Psi_T(x)|^2 \) is now replaced by the <a href="http://scitation.aip.org/content/aip/journal/jcp/21/6/10.1063/1.1699114" target="_blank">Metropolis-Hastings algorithm</a> as well as <a href="http://biomet.oxfordjournals.org/content/57/1/97.abstract" target="_blank">Hasting's article</a>, </p>
$$
q(y,x) = \frac{G(x,y,\Delta t)|\Psi_T(y)|^2}{G(y,x,\Delta t)|\Psi_T(x)|^2}
$$
<h2 id="importance-sampling-program-elements">Importance sampling, program elements </h2>

<p>The general derivative formula of the Jastrow factor is (the subscript \( C \) stands for Correlation)</p>
$$
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k}
+
\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_k}
$$

<p>However, 
with our written in way which can be reused later as
</p>
$$
\Psi_C=\prod_{i < j}g(r_{ij})= \exp{\left\{\sum_{i < j}f(r_{ij})\right\}},
$$

<p>the gradient needed for the quantum force and local energy is easy to compute.  
The function \( f(r_{ij}) \) will depends on the system under study. In the equations below we will keep this general form.
</p>

<p>In the Metropolis/Hasting algorithm, the <em>acceptance ratio</em> determines the probability for a particle  to be accepted at a new position. The ratio of the trial wave functions evaluated at the new and current positions is given by (\( OB \) for the onebody  part)</p>
$$
R \equiv \frac{\Psi_{T}^{new}}{\Psi_{T}^{old}} = 
\frac{\Psi_{OB}^{new}}{\Psi_{OB}^{old}}\frac{\Psi_{C}^{new}}{\Psi_{C}^{old}}
$$

<p>Here \( \Psi_{OB} \) is our onebody part (Slater determinant or product of boson single-particle states)  while \( \Psi_{C} \) is our correlation function, or Jastrow factor. 
We need to optimize the \( \nabla \Psi_T / \Psi_T \) ratio and the second derivative as well, that is
the \( \mathbf{\nabla}^2 \Psi_T/\Psi_T \) ratio. The first is needed when we compute the so-called quantum force in importance sampling.
The second is needed when we compute the kinetic energy term of the local energy.
</p>
$$
\frac{\mathbf{\mathbf{\nabla}}  \Psi}{\Psi}  = \frac{\mathbf{\nabla}  (\Psi_{OB} \, \Psi_{C})}{\Psi_{OB} \, \Psi_{C}}  =  \frac{ \Psi_C \mathbf{\nabla}  \Psi_{OB} + \Psi_{OB} \mathbf{\nabla}  \Psi_{C}}{\Psi_{OB} \Psi_{C}} = \frac{\mathbf{\nabla}  \Psi_{OB}}{\Psi_{OB}} + \frac{\mathbf{\nabla}   \Psi_C}{ \Psi_C}
$$

<p>The expectation value of the kinetic energy expressed in atomic units for electron \( i \) is </p>
$$
 \langle \hat{K}_i \rangle = -\frac{1}{2}\frac{\langle\Psi|\mathbf{\nabla}_{i}^2|\Psi \rangle}{\langle\Psi|\Psi \rangle},
$$

$$
\hat{K}_i = -\frac{1}{2}\frac{\mathbf{\nabla}_{i}^{2} \Psi}{\Psi}.
$$

<p>The second derivative which enters the definition of the local energy is </p>
$$
\frac{\mathbf{\nabla}^2 \Psi}{\Psi}=\frac{\mathbf{\nabla}^2 \Psi_{OB}}{\Psi_{OB}} + \frac{\mathbf{\nabla}^2  \Psi_C}{ \Psi_C} + 2 \frac{\mathbf{\nabla}  \Psi_{OB}}{\Psi_{OB}}\cdot\frac{\mathbf{\nabla}   \Psi_C}{ \Psi_C}
$$

<p>We discuss here how to calculate these quantities in an optimal way,</p>

<p>We have defined the correlated function as</p>
$$
\Psi_C=\prod_{i < j}g(r_{ij})=\prod_{i < j}^Ng(r_{ij})= \prod_{i=1}^N\prod_{j=i+1}^Ng(r_{ij}),
$$

<p>with 
\( r_{ij}=|\mathbf{r}_i-\mathbf{r}_j|=\sqrt{(x_i-x_j)^2+(y_i-y_j)^2+(z_i-z_j)^2} \) in three dimensions or
\( r_{ij}=|\mathbf{r}_i-\mathbf{r}_j|=\sqrt{(x_i-x_j)^2+(y_i-y_j)^2} \) if we work with two-dimensional systems.
</p>

<p>In our particular case we have</p>
$$
\Psi_C=\prod_{i < j}g(r_{ij})=\exp{\left\{\sum_{i < j}f(r_{ij})\right\}}.
$$

<p>The total number of different relative distances \( r_{ij} \) is \( N(N-1)/2 \). In a matrix storage format, the relative distances  form a strictly upper triangular matrix</p>
$$
 \mathbf{r} \equiv \begin{pmatrix}
  0 & r_{1,2} & r_{1,3} & \cdots & r_{1,N} \\
  \vdots & 0       & r_{2,3} & \cdots & r_{2,N} \\
  \vdots & \vdots  & 0  & \ddots & \vdots  \\
  \vdots & \vdots  & \vdots  & \ddots  & r_{N-1,N} \\
  0 & 0  & 0  & \cdots  & 0
 \end{pmatrix}.
$$

<p>This applies to  \( \mathbf{g} = \mathbf{g}(r_{ij}) \) as well. </p>

<p>In our algorithm we will move one particle  at the time, say the \( kth \)-particle.  This sampling will be seen to be particularly efficient when we are going to compute a Slater determinant. </p>

<p>We have that the ratio between Jastrow factors \( R_C \) is given by</p>
$$
R_{C} = \frac{\Psi_{C}^\mathrm{new}}{\Psi_{C}^\mathrm{cur}} =
\prod_{i=1}^{k-1}\frac{g_{ik}^\mathrm{new}}{g_{ik}^\mathrm{cur}}
\prod_{i=k+1}^{N}\frac{ g_{ki}^\mathrm{new}} {g_{ki}^\mathrm{cur}}.
$$

<p>For the Pade-Jastrow form</p>
$$
 R_{C} = \frac{\Psi_{C}^\mathrm{new}}{\Psi_{C}^\mathrm{cur}} = 
\frac{\exp{U_{new}}}{\exp{U_{cur}}} = \exp{\Delta U},
$$

<p>where</p>
$$
\Delta U =
\sum_{i=1}^{k-1}\big(f_{ik}^\mathrm{new}-f_{ik}^\mathrm{cur}\big)
+
\sum_{i=k+1}^{N}\big(f_{ki}^\mathrm{new}-f_{ki}^\mathrm{cur}\big)
$$

<p>One needs to develop a special algorithm 
that runs only through the elements of the upper triangular
matrix \( \mathbf{g} \) and have \( k \) as an index. 
</p>

<p>The expression to be derived in the following is of interest when computing the quantum force and the kinetic energy. It has the form</p>
$$
\frac{\mathbf{\nabla}_i\Psi_C}{\Psi_C} = \frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_i},
$$

<p>for all dimensions and with \( i \) running over all particles.</p>

<p>For the first derivative only \( N-1 \) terms survive the ratio because the \( g \)-terms that are not differentiated cancel with their corresponding ones in the denominator. Then,</p>
$$
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{1}{g_{ik}}\frac{\partial g_{ik}}{\partial x_k}
+
\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\frac{\partial g_{ki}}{\partial x_k}.
$$

<p>An equivalent equation is obtained for the exponential form after replacing \( g_{ij} \) by \( \exp(f_{ij}) \), yielding:</p>
$$
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k}
+
\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_k},
$$

<p>with both expressions scaling as \( \mathcal{O}(N) \).</p>

<p>Using the identity </p>
$$
\frac{\partial}{\partial x_i}g_{ij} = -\frac{\partial}{\partial x_j}g_{ij},
$$

<p>we get expressions where all the derivatives acting on the particle  are represented by the <em>second</em> index of \( g \):</p>
$$
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{1}{g_{ik}}\frac{\partial g_{ik}}{\partial x_k}
-\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\frac{\partial g_{ki}}{\partial x_i},
$$

<p>and for the exponential case:</p>
$$
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k}
-\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_i}.
$$

<p>For correlation forms depending only on the scalar distances \( r_{ij} \) we can use the chain rule. Noting that </p>
$$
\frac{\partial g_{ij}}{\partial x_j} = \frac{\partial g_{ij}}{\partial r_{ij}} \frac{\partial r_{ij}}{\partial x_j} = \frac{x_j - x_i}{r_{ij}} \frac{\partial g_{ij}}{\partial r_{ij}},
$$

<p>we arrive at</p>
$$
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} = 
\sum_{i=1}^{k-1}\frac{1}{g_{ik}} \frac{\mathbf{r_{ik}}}{r_{ik}} \frac{\partial g_{ik}}{\partial r_{ik}}
-\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\frac{\mathbf{r_{ki}}}{r_{ki}}\frac{\partial g_{ki}}{\partial r_{ki}}.
$$

<p>Note that for the Pade-Jastrow form we can set \( g_{ij} \equiv g(r_{ij}) = e^{f(r_{ij})} = e^{f_{ij}} \) and </p>
$$
\frac{\partial g_{ij}}{\partial r_{ij}} = g_{ij} \frac{\partial f_{ij}}{\partial r_{ij}}.
$$

<p>Therefore, </p>
$$
\frac{1}{\Psi_{C}}\frac{\partial \Psi_{C}}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{\mathbf{r_{ik}}}{r_{ik}}\frac{\partial f_{ik}}{\partial r_{ik}}
-\sum_{i=k+1}^{N}\frac{\mathbf{r_{ki}}}{r_{ki}}\frac{\partial f_{ki}}{\partial r_{ki}},
$$

<p>where </p>
$$
 \mathbf{r}_{ij} = |\mathbf{r}_j - \mathbf{r}_i| = (x_j - x_i)\mathbf{e}_1 + (y_j - y_i)\mathbf{e}_2 + (z_j - z_i)\mathbf{e}_3
$$

<p>is the relative distance. </p>

<p>The second derivative of the Jastrow factor divided by the Jastrow factor (the way it enters the kinetic energy) is</p>
$$
\left[\frac{\mathbf{\nabla}^2 \Psi_C}{\Psi_C}\right]_x =\  
2\sum_{k=1}^{N}
\sum_{i=1}^{k-1}\frac{\partial^2 g_{ik}}{\partial x_k^2}\ +\ 
\sum_{k=1}^N
\left(
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k} -
\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_i}
\right)^2
$$

<p>But we have a simple form for the function, namely</p>
$$
\Psi_{C}=\prod_{i < j}\exp{f(r_{ij})},
$$

<p>and it is easy to see that for particle  \( k \)
we have
</p>
$$
  \frac{\mathbf{\nabla}^2_k \Psi_C}{\Psi_C }=
\sum_{ij\ne k}\frac{(\mathbf{r}_k-\mathbf{r}_i)(\mathbf{r}_k-\mathbf{r}_j)}{r_{ki}r_{kj}}f'(r_{ki})f'(r_{kj})+
\sum_{j\ne k}\left( f''(r_{kj})+\frac{2}{r_{kj}}f'(r_{kj})\right)
$$
<h2 id="importance-sampling-fokker-planck-and-langevin-equations">Importance sampling, Fokker-Planck and Langevin equations </h2>

<p>A stochastic process is simply a function of two variables, one is the time,
the other is a stochastic variable \( X \), defined by specifying
</p>
<ul>
<li> the set \( \left\{x\right\} \) of possible values for \( X \);</li>
<li> the probability distribution, \( w_X(x) \),  over this set, or briefly \( w(x) \)</li>
</ul>
<p>The set of values \( \left\{x\right\} \) for \( X \) 
may be discrete, or continuous. If the set of
values is continuous, then \( w_X (x) \) is a probability density so that 
\( w_X (x)dx \)
is the probability that one finds the stochastic variable \( X \) to have values
in the range \( [x, x + dx] \) .
</p>

<p>     An arbitrary number of other stochastic variables may be derived from
\( X \). For example, any \( Y \) given by a mapping of \( X \), is also a stochastic
variable. The mapping may also be time-dependent, that is, the mapping
depends on an additional variable \( t \)
</p>
$$
                              Y_X (t) = f (X, t) .
$$

<p>The quantity \( Y_X (t) \) is called a random function, or, since \( t \) often is time,
a stochastic process. A stochastic process is a function of two variables,
one is the time, the other is a stochastic variable \( X \). Let \( x \) be one of the
possible values of \( X \) then
</p>
$$
                               y(t) = f (x, t),
$$

<p>is a function of \( t \), called a sample function or realization of the process.
In physics one considers the stochastic process to be an ensemble of such
sample functions.
</p>

<p>     For many physical systems initial distributions of a stochastic 
variable \( y \) tend to equilibrium distributions: \( w(y, t)\rightarrow w_0(y) \) 
as \( t\rightarrow\infty \). In
equilibrium detailed balance constrains the transition rates
</p>
$$
     W(y\rightarrow y')w(y ) = W(y'\rightarrow y)w_0 (y),
$$

<p>where \( W(y'\rightarrow y) \) 
is the probability, per unit time, that the system changes
from a state \( |y\rangle \) , characterized by the value \( y \) 
for the stochastic variable \( Y \) , to a state \( |y'\rangle \).
</p>

<p>Note that for a system in equilibrium the transition rate 
\( W(y'\rightarrow y) \) and
the reverse \( W(y\rightarrow y') \) may be very different. 
</p>

<p>Consider, for instance, a simple
system that has only two energy levels \( \epsilon_0 = 0 \) and 
\( \epsilon_1 = \Delta E \). 
</p>

<p>For a system governed by the Boltzmann distribution we find (the partition function has been taken out)</p>
$$
     W(0\rightarrow 1)\exp{-(\epsilon_0/kT)} = W(1\rightarrow 0)\exp{-(\epsilon_1/kT)}
$$

<p>We get then</p>
$$
     \frac{W(1\rightarrow 0)}{W(0 \rightarrow 1)}=\exp{-(\Delta E/kT)},
$$

<p>which goes to zero when \( T \) tends to zero.</p>

<p>If we assume a discrete set of events,
our initial probability
distribution function can be  given by 
</p>
$$
   w_i(0) = \delta_{i,0},
$$

<p>and its time-development after a given time step \( \Delta t=\epsilon \) is</p>
$$ 
   w_i(t) = \sum_{j}W(j\rightarrow i)w_j(t=0).
$$

<p>The continuous analog to \( w_i(0) \) is</p>
$$
   w(\mathbf{x})\rightarrow \delta(\mathbf{x}),
$$

<p>where we now have generalized the one-dimensional position \( x \) to a generic-dimensional  
vector \( \mathbf{x} \). The Kroenecker \( \delta \) function is replaced by the \( \delta \) distribution
function \( \delta(\mathbf{x}) \) at  \( t=0 \).  
</p>

<p>The transition from a state \( j \) to a state \( i \) is now replaced by a transition
to a state with position \( \mathbf{y} \) from a state with position \( \mathbf{x} \). 
The discrete sum of transition probabilities can then be replaced by an integral
and we obtain the new distribution at a time \( t+\Delta t \) as 
</p>
$$
   w(\mathbf{y},t+\Delta t)= \int W(\mathbf{y},t+\Delta t| \mathbf{x},t)w(\mathbf{x},t)d\mathbf{x},
$$

<p>and after \( m \) time steps we have</p>
$$
   w(\mathbf{y},t+m\Delta t)= \int W(\mathbf{y},t+m\Delta t| \mathbf{x},t)w(\mathbf{x},t)d\mathbf{x}.
$$

<p>When equilibrium is reached we have</p>
$$
   w(\mathbf{y})= \int W(\mathbf{y}|\mathbf{x}, t)w(\mathbf{x})d\mathbf{x},
$$

<p>that is no time-dependence. Note our change of notation for \( W \)</p>

<p>We can solve the equation for \( w(\mathbf{y},t) \) by making a Fourier transform to
momentum space. 
The PDF \( w(\mathbf{x},t) \) is related to its Fourier transform
\( \tilde{w}(\mathbf{k},t) \) through
</p>
$$
   w(\mathbf{x},t) = \int_{-\infty}^{\infty}d\mathbf{k} \exp{(i\mathbf{kx})}\tilde{w}(\mathbf{k},t),
$$

<p>and using the definition of the 
\( \delta \)-function 
</p>
$$
   \delta(\mathbf{x}) = \frac{1}{2\pi} \int_{-\infty}^{\infty}d\mathbf{k} \exp{(i\mathbf{kx})},
$$

<p> we see that</p>
$$
   \tilde{w}(\mathbf{k},0)=1/2\pi.
$$

<p>We can then use the Fourier-transformed diffusion equation </p>
$$
    \frac{\partial \tilde{w}(\mathbf{k},t)}{\partial t} = -D\mathbf{k}^2\tilde{w}(\mathbf{k},t),
$$

<p>with the obvious solution</p>
$$
   \tilde{w}(\mathbf{k},t)=\tilde{w}(\mathbf{k},0)\exp{\left[-(D\mathbf{k}^2t)\right)}=
    \frac{1}{2\pi}\exp{\left[-(D\mathbf{k}^2t)\right]}. 
$$

<p>With the Fourier transform we obtain </p>
$$
   w(\mathbf{x},t)=\int_{-\infty}^{\infty}d\mathbf{k} \exp{\left[i\mathbf{kx}\right]}\frac{1}{2\pi}\exp{\left[-(D\mathbf{k}^2t)\right]}=
    \frac{1}{\sqrt{4\pi Dt}}\exp{\left[-(\mathbf{x}^2/4Dt)\right]}, 
$$

<p>with the normalization condition</p>
$$
   \int_{-\infty}^{\infty}w(\mathbf{x},t)d\mathbf{x}=1.
$$

<p>The solution represents the probability of finding
our random walker at position \( \mathbf{x} \) at time \( t \) if the initial distribution 
was placed at \( \mathbf{x}=0 \) at \( t=0 \). 
</p>

<p>There is another interesting feature worth observing. The discrete transition probability \( W \)
itself is given by a binomial distribution.
The results from the central limit theorem state that 
transition probability in the limit \( n\rightarrow \infty \) converges to the normal 
distribution. It is then possible to show that
</p>
$$
    W(il-jl,n\epsilon)\rightarrow W(\mathbf{y},t+\Delta t|\mathbf{x},t)=
    \frac{1}{\sqrt{4\pi D\Delta t}}\exp{\left[-((\mathbf{y}-\mathbf{x})^2/4D\Delta t)\right]},
$$

<p>and that it satisfies the normalization condition and is itself a solution
to the diffusion equation.
</p>

<p>Let us now assume that we have three PDFs for times \( t_0 < t' < t \), that is
\( w(\mathbf{x}_0,t_0) \), \( w(\mathbf{x}',t') \) and \( w(\mathbf{x},t) \).
We have then  
</p>
$$
   w(\mathbf{x},t)= \int_{-\infty}^{\infty} W(\mathbf{x}.t|\mathbf{x}'.t')w(\mathbf{x}',t')d\mathbf{x}',
$$

<p>and</p>
$$
   w(\mathbf{x},t)= \int_{-\infty}^{\infty} W(\mathbf{x}.t|\mathbf{x}_0.t_0)w(\mathbf{x}_0,t_0)d\mathbf{x}_0,
$$

<p>and</p>
$$
   w(\mathbf{x}',t')= \int_{-\infty}^{\infty} W(\mathbf{x}'.t'|\mathbf{x}_0,t_0)w(\mathbf{x}_0,t_0)d\mathbf{x}_0.
$$

<p>We can combine these equations and arrive at the famous Einstein-Smoluchenski-Kolmogorov-Chapman (ESKC) relation</p>
$$
 W(\mathbf{x}t|\mathbf{x}_0t_0)  = \int_{-\infty}^{\infty} W(\mathbf{x},t|\mathbf{x}',t')W(\mathbf{x}',t'|\mathbf{x}_0,t_0)d\mathbf{x}'.
$$

<p>We can replace the spatial dependence with a dependence upon say the velocity
(or momentum), that is we have
</p>
$$
 W(\mathbf{v},t|\mathbf{v}_0,t_0)  = \int_{-\infty}^{\infty} W(\mathbf{v},t|\mathbf{v}',t')W(\mathbf{v}',t'|\mathbf{v}_0,t_0)d\mathbf{x}'.
$$

<p>We will now derive the Fokker-Planck equation. 
We start from the ESKC equation
</p>
$$
 W(\mathbf{x},t|\mathbf{x}_0,t_0)  = \int_{-\infty}^{\infty} W(\mathbf{x},t|\mathbf{x}',t')W(\mathbf{x}',t'|\mathbf{x}_0,t_0)d\mathbf{x}'.
$$

<p>Define \( s=t'-t_0 \), \( \tau=t-t' \) and \( t-t_0=s+\tau \). We have then</p>
$$
 W(\mathbf{x},s+\tau|\mathbf{x}_0)  = \int_{-\infty}^{\infty} W(\mathbf{x},\tau|\mathbf{x}')W(\mathbf{x}',s|\mathbf{x}_0)d\mathbf{x}'.
$$

<p>Assume now that \( \tau \) is very small so that we can make an expansion in terms of a small step \( xi \), with \( \mathbf{x}'=\mathbf{x}-\xi \), that is</p>
$$
 W(\mathbf{x},s|\mathbf{x}_0)+\frac{\partial W}{\partial s}\tau +O(\tau^2) = \int_{-\infty}^{\infty} W(\mathbf{x},\tau|\mathbf{x}-\xi)W(\mathbf{x}-\xi,s|\mathbf{x}_0)d\mathbf{x}'.
$$

<p>We assume that \( W(\mathbf{x},\tau|\mathbf{x}-\xi) \) takes non-negligible values only when \( \xi \) is small. This is just another way of stating the Master equation!!</p>

<p>We say thus that \( \mathbf{x} \) changes only by a small amount in the time interval \( \tau \). 
This means that we can make a Taylor expansion in terms of \( \xi \), that is we
expand
</p>
$$
W(\mathbf{x},\tau|\mathbf{x}-\xi)W(\mathbf{x}-\xi,s|\mathbf{x}_0) =
\sum_{n=0}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}\left[W(\mathbf{x}+\xi,\tau|\mathbf{x})W(\mathbf{x},s|\mathbf{x}_0)
\right].
$$

<p>We can then rewrite the ESKC equation as </p>
$$
\frac{\partial W}{\partial s}\tau=-W(\mathbf{x},s|\mathbf{x}_0)+
\sum_{n=0}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}
\left[W(\mathbf{x},s|\mathbf{x}_0)\int_{-\infty}^{\infty} \xi^nW(\mathbf{x}+\xi,\tau|\mathbf{x})d\xi\right].
$$

<p>We have neglected higher powers of \( \tau \) and have used that for \( n=0 \) 
we get simply \( W(\mathbf{x},s|\mathbf{x}_0) \) due to normalization.
</p>

<p>We say thus that \( \mathbf{x} \) changes only by a small amount in the time interval \( \tau \). 
This means that we can make a Taylor expansion in terms of \( \xi \), that is we
expand
</p>
$$
W(\mathbf{x},\tau|\mathbf{x}-\xi)W(\mathbf{x}-\xi,s|\mathbf{x}_0) =
\sum_{n=0}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}\left[W(\mathbf{x}+\xi,\tau|\mathbf{x})W(\mathbf{x},s|\mathbf{x}_0)
\right].
$$

<p>We can then rewrite the ESKC equation as </p>
$$
\frac{\partial W(\mathbf{x},s|\mathbf{x}_0)}{\partial s}\tau=-W(\mathbf{x},s|\mathbf{x}_0)+
\sum_{n=0}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}
\left[W(\mathbf{x},s|\mathbf{x}_0)\int_{-\infty}^{\infty} \xi^nW(\mathbf{x}+\xi,\tau|\mathbf{x})d\xi\right].
$$

<p>We have neglected higher powers of \( \tau \) and have used that for \( n=0 \) 
we get simply \( W(\mathbf{x},s|\mathbf{x}_0) \) due to normalization.
</p>

<p>We simplify the above by introducing the moments </p>
$$
M_n=\frac{1}{\tau}\int_{-\infty}^{\infty} \xi^nW(\mathbf{x}+\xi,\tau|\mathbf{x})d\xi=
\frac{\langle [\Delta x(\tau)]^n\rangle}{\tau},
$$

<p>resulting in</p>
$$
\frac{\partial W(\mathbf{x},s|\mathbf{x}_0)}{\partial s}=
\sum_{n=1}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}
\left[W(\mathbf{x},s|\mathbf{x}_0)M_n\right].
$$

<p>When \( \tau \rightarrow 0 \) we assume that \( \langle [\Delta x(\tau)]^n\rangle \rightarrow 0 \) more rapidly than \( \tau \) itself if \( n > 2 \). 
When \( \tau \) is much larger than the standard correlation time of 
system then \( M_n \) for \( n > 2 \) can normally be neglected.
This means that fluctuations become negligible at large time scales.
</p>

<p>If we neglect such terms we can rewrite the ESKC equation as </p>
$$
\frac{\partial W(\mathbf{x},s|\mathbf{x}_0)}{\partial s}=
-\frac{\partial M_1W(\mathbf{x},s|\mathbf{x}_0)}{\partial x}+
\frac{1}{2}\frac{\partial^2 M_2W(\mathbf{x},s|\mathbf{x}_0)}{\partial x^2}.
$$

<p>In a more compact form we have</p>
$$
\frac{\partial W}{\partial s}=
-\frac{\partial M_1W}{\partial x}+
\frac{1}{2}\frac{\partial^2 M_2W}{\partial x^2},
$$

<p>which is the Fokker-Planck equation!  It is trivial to replace 
position with velocity (momentum).
</p>

<p>Consider a particle  suspended in a liquid. On its path through the liquid it will continuously collide with the liquid molecules. Because on average the particle  will collide more often on the front side than on the back side, it will experience a systematic force proportional with its velocity, and directed opposite to its velocity. Besides this systematic force the particle  will experience a stochastic force  \( \mathbf{F}(t) \). 
The equations of motion are 
</p>
<ul>
<li> \( \frac{d\mathbf{r}}{dt}=\mathbf{v} \) and</li> 
<li> \( \frac{d\mathbf{v}}{dt}=-\xi \mathbf{v}+\mathbf{F} \).</li>
</ul>
<p>From hydrodynamics  we know that the friction constant  \( \xi \) is given by</p>
$$
\xi =6\pi \eta a/m 
$$

<p>where \( \eta \) is the viscosity  of the solvent and a is the radius of the particle .</p>

<p>Solving the second equation in the previous slide we get </p>
$$
\mathbf{v}(t)=\mathbf{v}_{0}e^{-\xi t}+\int_{0}^{t}d\tau e^{-\xi (t-\tau )}\mathbf{F }(\tau ). 
$$

<p>If we want to get some useful information out of this, we have to average over all possible realizations of 
\( \mathbf{F}(t) \), with the initial velocity as a condition. A useful quantity for example is
</p>
$$ 
\langle \mathbf{v}(t)\cdot \mathbf{v}(t)\rangle_{\mathbf{v}_{0}}=v_{0}^{-\xi 2t}
+2\int_{0}^{t}d\tau e^{-\xi (2t-\tau)}\mathbf{v}_{0}\cdot \langle \mathbf{F}(\tau )\rangle_{\mathbf{v}_{0}}
$$

$$  	  	
 +\int_{0}^{t}d\tau ^{\prime }\int_{0}^{t}d\tau e^{-\xi (2t-\tau -\tau ^{\prime })}
\langle \mathbf{F}(\tau )\cdot \mathbf{F}(\tau ^{\prime })\rangle_{ \mathbf{v}_{0}}.
$$

<p>In order to continue we have to make some assumptions about the conditional averages of the stochastic forces. 
In view of the chaotic character of the stochastic forces the following 
assumptions seem to be appropriate
</p>
$$ 
\langle \mathbf{F}(t)\rangle=0, 
$$

<p>and</p>
$$
\langle \mathbf{F}(t)\cdot \mathbf{F}(t^{\prime })\rangle_{\mathbf{v}_{0}}=  C_{\mathbf{v}_{0}}\delta (t-t^{\prime }).
$$

<p>We omit the subscript \( \mathbf{v}_{0} \), when the quantity of interest turns out to be independent of \( \mathbf{v}_{0} \). Using the last three equations we get</p>
$$
\langle \mathbf{v}(t)\cdot \mathbf{v}(t)\rangle_{\mathbf{v}_{0}}=v_{0}^{2}e^{-2\xi t}+\frac{C_{\mathbf{v}_{0}}}{2\xi }(1-e^{-2\xi t}).
$$

<p>For large t this should be equal to 3kT/m, from which it follows that</p>
$$
\langle \mathbf{F}(t)\cdot \mathbf{F}(t^{\prime })\rangle =6\frac{kT}{m}\xi \delta (t-t^{\prime }). 
$$

<p>This result is called the fluctuation-dissipation theorem .</p>

<p>Integrating </p>
$$ 
\mathbf{v}(t)=\mathbf{v}_{0}e^{-\xi t}+\int_{0}^{t}d\tau e^{-\xi (t-\tau )}\mathbf{F }(\tau ), 
$$

<p>we get</p>
$$
\mathbf{r}(t)=\mathbf{r}_{0}+\mathbf{v}_{0}\frac{1}{\xi }(1-e^{-\xi t})+
\int_0^td\tau \int_0^{\tau}\tau ^{\prime } e^{-\xi (\tau -\tau ^{\prime })}\mathbf{F}(\tau ^{\prime }), 
$$

<p>from which we calculate the mean square displacement </p>
$$
\langle ( \mathbf{r}(t)-\mathbf{r}_{0})^{2}\rangle _{\mathbf{v}_{0}}=\frac{v_0^2}{\xi}(1-e^{-\xi t})^{2}+\frac{3kT}{m\xi ^{2}}(2\xi t-3+4e^{-\xi t}-e^{-2\xi t}). 
$$

<p>For very large \( t \) this becomes</p>
$$
\langle (\mathbf{r}(t)-\mathbf{r}_{0})^{2}\rangle =\frac{6kT}{m\xi }t 
$$

<p>from which we get the Einstein relation  </p>
$$ 
D= \frac{kT}{m\xi } 
$$

<p>where we have used \( \langle (\mathbf{r}(t)-\mathbf{r}_{0})^{2}\rangle =6Dt \).</p>
<h2 id="code-example-for-two-electrons-in-a-quantum-dots">Code example for two electrons in a quantum dots </h2>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #228B22"># 2-electron VMC code for 2dim quantum dot with importance sampling</span>
<span style="color: #228B22"># Using gaussian rng for new positions and Metropolis- Hastings </span>
<span style="color: #228B22"># No energy minimization</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">math</span> <span style="color: #8B008B; font-weight: bold">import</span> exp, sqrt
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">random</span> <span style="color: #8B008B; font-weight: bold">import</span> random, seed, normalvariate
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">mpl_toolkits.mplot3d</span> <span style="color: #8B008B; font-weight: bold">import</span> Axes3D
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">matplotlib</span> <span style="color: #8B008B; font-weight: bold">import</span> cm
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">matplotlib.ticker</span> <span style="color: #8B008B; font-weight: bold">import</span> LinearLocator, FormatStrFormatter
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">sys</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">numba</span> <span style="color: #8B008B; font-weight: bold">import</span> jit,njit


<span style="color: #228B22">#Read name of output file from command line</span>
<span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #658b00">len</span>(sys.argv) == <span style="color: #B452CD">2</span>:
    outfilename = sys.argv[<span style="color: #B452CD">1</span>]
<span style="color: #8B008B; font-weight: bold">else</span>:
    <span style="color: #658b00">print</span>(<span style="color: #CD5555">&#39;\nError: Name of output file must be given as command line argument.\n&#39;</span>)
outfile = <span style="color: #658b00">open</span>(outfilename,<span style="color: #CD5555">&#39;w&#39;</span>)

<span style="color: #228B22"># Trial wave function for the 2-electron quantum dot in two dims</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">WaveFunction</span>(r,alpha,beta):
    r1 = r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>
    r2 = r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>
    r12 = sqrt((r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>])**<span style="color: #B452CD">2</span> + (r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>])**<span style="color: #B452CD">2</span>)
    deno = r12/(<span style="color: #B452CD">1</span>+beta*r12)
    <span style="color: #8B008B; font-weight: bold">return</span> exp(-<span style="color: #B452CD">0.5</span>*alpha*(r1+r2)+deno)

<span style="color: #228B22"># Local energy  for the 2-electron quantum dot in two dims, using analytical local energy</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">LocalEnergy</span>(r,alpha,beta):
    
    r1 = (r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>)
    r2 = (r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>)
    r12 = sqrt((r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>])**<span style="color: #B452CD">2</span> + (r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>])**<span style="color: #B452CD">2</span>)
    deno = <span style="color: #B452CD">1.0</span>/(<span style="color: #B452CD">1</span>+beta*r12)
    deno2 = deno*deno
    <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">0.5</span>*(<span style="color: #B452CD">1</span>-alpha*alpha)*(r1 + r2) +<span style="color: #B452CD">2.0</span>*alpha + <span style="color: #B452CD">1.0</span>/r12+deno2*(alpha*r12-deno2+<span style="color: #B452CD">2</span>*beta*deno-<span style="color: #B452CD">1.0</span>/r12)

<span style="color: #228B22"># Setting up the quantum force for the two-electron quantum dot, recall that it is a vector</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">QuantumForce</span>(r,alpha,beta):

    qforce = np.zeros((NumberParticles,Dimension), np.double)
    r12 = sqrt((r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>])**<span style="color: #B452CD">2</span> + (r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>])**<span style="color: #B452CD">2</span>)
    deno = <span style="color: #B452CD">1.0</span>/(<span style="color: #B452CD">1</span>+beta*r12)
    qforce[<span style="color: #B452CD">0</span>,:] = -<span style="color: #B452CD">2</span>*r[<span style="color: #B452CD">0</span>,:]*alpha*(r[<span style="color: #B452CD">0</span>,:]-r[<span style="color: #B452CD">1</span>,:])*deno*deno/r12
    qforce[<span style="color: #B452CD">1</span>,:] = -<span style="color: #B452CD">2</span>*r[<span style="color: #B452CD">1</span>,:]*alpha*(r[<span style="color: #B452CD">1</span>,:]-r[<span style="color: #B452CD">0</span>,:])*deno*deno/r12
    <span style="color: #8B008B; font-weight: bold">return</span> qforce
    
<span style="color: #228B22"># The Monte Carlo sampling with the Metropolis algo</span>
<span style="color: #228B22"># jit decorator tells Numba to compile this function.</span>
<span style="color: #228B22"># The argument types will be inferred by Numba when function is called.</span>
<span style="color: #707a7c">@jit</span>()
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">MonteCarloSampling</span>():

    NumberMCcycles= <span style="color: #B452CD">100000</span>
    <span style="color: #228B22"># Parameters in the Fokker-Planck simulation of the quantum force</span>
    D = <span style="color: #B452CD">0.5</span>
    TimeStep = <span style="color: #B452CD">0.05</span>
    <span style="color: #228B22"># positions</span>
    PositionOld = np.zeros((NumberParticles,Dimension), np.double)
    PositionNew = np.zeros((NumberParticles,Dimension), np.double)
    <span style="color: #228B22"># Quantum force</span>
    QuantumForceOld = np.zeros((NumberParticles,Dimension), np.double)
    QuantumForceNew = np.zeros((NumberParticles,Dimension), np.double)

    <span style="color: #228B22"># seed for rng generator </span>
    seed()
    <span style="color: #228B22"># start variational parameter  loops, two parameters here</span>
    alpha = <span style="color: #B452CD">0.9</span>
    <span style="color: #8B008B; font-weight: bold">for</span> ia <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(MaxVariations):
        alpha += <span style="color: #B452CD">.025</span>
        AlphaValues[ia] = alpha
        beta = <span style="color: #B452CD">0.2</span> 
        <span style="color: #8B008B; font-weight: bold">for</span> jb <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(MaxVariations):
            beta += <span style="color: #B452CD">.01</span>
            BetaValues[jb] = beta
            energy = energy2 = <span style="color: #B452CD">0.0</span>
            DeltaE = <span style="color: #B452CD">0.0</span>
            <span style="color: #228B22">#Initial position</span>
            <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberParticles):
                <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
                    PositionOld[i,j] = normalvariate(<span style="color: #B452CD">0.0</span>,<span style="color: #B452CD">1.0</span>)*sqrt(TimeStep)
            wfold = WaveFunction(PositionOld,alpha,beta)
            QuantumForceOld = QuantumForce(PositionOld,alpha, beta)

            <span style="color: #228B22">#Loop over MC MCcycles</span>
            <span style="color: #8B008B; font-weight: bold">for</span> MCcycle <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberMCcycles):
                <span style="color: #228B22">#Trial position moving one particle at the time</span>
                <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberParticles):
                    <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
                        PositionNew[i,j] = PositionOld[i,j]+normalvariate(<span style="color: #B452CD">0.0</span>,<span style="color: #B452CD">1.0</span>)*sqrt(TimeStep)+\
                                           QuantumForceOld[i,j]*TimeStep*D
                    wfnew = WaveFunction(PositionNew,alpha,beta)
                    QuantumForceNew = QuantumForce(PositionNew,alpha, beta)
                    GreensFunction = <span style="color: #B452CD">0.0</span>
                    <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
                        GreensFunction += <span style="color: #B452CD">0.5</span>*(QuantumForceOld[i,j]+QuantumForceNew[i,j])*\
	                              (D*TimeStep*<span style="color: #B452CD">0.5</span>*(QuantumForceOld[i,j]-QuantumForceNew[i,j])-\
                                      PositionNew[i,j]+PositionOld[i,j])
      
                    GreensFunction = exp(GreensFunction)
                    ProbabilityRatio = GreensFunction*wfnew**<span style="color: #B452CD">2</span>/wfold**<span style="color: #B452CD">2</span>
                    <span style="color: #228B22">#Metropolis-Hastings test to see whether we accept the move</span>
                    <span style="color: #8B008B; font-weight: bold">if</span> random() &lt;= ProbabilityRatio:
                       <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
                           PositionOld[i,j] = PositionNew[i,j]
                           QuantumForceOld[i,j] = QuantumForceNew[i,j]
                       wfold = wfnew
                DeltaE = LocalEnergy(PositionOld,alpha,beta)
                energy += DeltaE
                energy2 += DeltaE**<span style="color: #B452CD">2</span>
            <span style="color: #228B22"># We calculate mean, variance and error (no blocking applied)</span>
            energy /= NumberMCcycles
            energy2 /= NumberMCcycles
            variance = energy2 - energy**<span style="color: #B452CD">2</span>
            error = sqrt(variance/NumberMCcycles)
            Energies[ia,jb] = energy    
            outfile.write(<span style="color: #CD5555">&#39;%f %f %f %f %f\n&#39;</span> %(alpha,beta,energy,variance,error))
    <span style="color: #8B008B; font-weight: bold">return</span> Energies, AlphaValues, BetaValues


<span style="color: #228B22">#Here starts the main program with variable declarations</span>
NumberParticles = <span style="color: #B452CD">2</span>
Dimension = <span style="color: #B452CD">2</span>
MaxVariations = <span style="color: #B452CD">10</span>
Energies = np.zeros((MaxVariations,MaxVariations))
AlphaValues = np.zeros(MaxVariations)
BetaValues = np.zeros(MaxVariations)
(Energies, AlphaValues, BetaValues) = MonteCarloSampling()
outfile.close()
<span style="color: #228B22"># Prepare for plots</span>
fig = plt.figure()
ax = fig.gca(projection=<span style="color: #CD5555">&#39;3d&#39;</span>)
<span style="color: #228B22"># Plot the surface.</span>
X, Y = np.meshgrid(AlphaValues, BetaValues)
surf = ax.plot_surface(X, Y, Energies,cmap=cm.coolwarm,linewidth=<span style="color: #B452CD">0</span>, antialiased=<span style="color: #8B008B; font-weight: bold">False</span>)
<span style="color: #228B22"># Customize the z axis.</span>
zmin = np.matrix(Energies).min()
zmax = np.matrix(Energies).max()
ax.set_zlim(zmin, zmax)
ax.set_xlabel(<span style="color: #CD5555">r&#39;$\alpha$&#39;</span>)
ax.set_ylabel(<span style="color: #CD5555">r&#39;$\beta$&#39;</span>)
ax.set_zlabel(<span style="color: #CD5555">r&#39;$\langle E \rangle$&#39;</span>)
ax.zaxis.set_major_locator(LinearLocator(<span style="color: #B452CD">10</span>))
ax.zaxis.set_major_formatter(FormatStrFormatter(<span style="color: #CD5555">&#39;%.02f&#39;</span>))
<span style="color: #228B22"># Add a color bar which maps values to colors.</span>
fig.colorbar(surf, shrink=<span style="color: #B452CD">0.5</span>, aspect=<span style="color: #B452CD">5</span>)
plt.show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
<h3 id="bringing-the-gradient-optmization">Bringing the gradient optmization </h3>
<p>The simple one-particle case in a harmonic oscillator trap</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #228B22"># Gradient descent stepping with analytical derivative</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">scipy.optimize</span> <span style="color: #8B008B; font-weight: bold">import</span> minimize
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">DerivativeE</span>(x):
    <span style="color: #8B008B; font-weight: bold">return</span> x-<span style="color: #B452CD">1.0</span>/(<span style="color: #B452CD">4</span>*x*x*x);

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">Energy</span>(x):
   <span style="color: #8B008B; font-weight: bold">return</span> x*x*<span style="color: #B452CD">0.5</span>+<span style="color: #B452CD">1.0</span>/(<span style="color: #B452CD">8</span>*x*x);
x0 = <span style="color: #B452CD">1.0</span>
eta = <span style="color: #B452CD">0.1</span>
Niterations = <span style="color: #B452CD">100</span>

<span style="color: #8B008B; font-weight: bold">for</span> <span style="color: #658b00">iter</span> <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Niterations):
    gradients = DerivativeE(x0)
    x0 -= eta*gradients

<span style="color: #658b00">print</span>(x0)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #228B22"># 2-electron VMC code for 2dim quantum dot with importance sampling</span>
<span style="color: #228B22"># Using gaussian rng for new positions and Metropolis- Hastings </span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">math</span> <span style="color: #8B008B; font-weight: bold">import</span> exp, sqrt
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">random</span> <span style="color: #8B008B; font-weight: bold">import</span> random, seed, normalvariate
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">mpl_toolkits.mplot3d</span> <span style="color: #8B008B; font-weight: bold">import</span> Axes3D
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">matplotlib</span> <span style="color: #8B008B; font-weight: bold">import</span> cm
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">matplotlib.ticker</span> <span style="color: #8B008B; font-weight: bold">import</span> LinearLocator, FormatStrFormatter
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">sys</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">numba</span> <span style="color: #8B008B; font-weight: bold">import</span> jit


<span style="color: #228B22"># Trial wave function for the 2-electron quantum dot in two dims</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">WaveFunction</span>(r,alpha):
    r1 = r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>
    r2 = r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>
    <span style="color: #8B008B; font-weight: bold">return</span> exp(-<span style="color: #B452CD">0.5</span>*alpha*(r1+r2))

<span style="color: #228B22"># Local energy  for the 2-electron quantum dot in two dims, using analytical local energy</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">LocalEnergy</span>(r,alpha):
    
    r1 = (r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>)
    r2 = (r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>)
    <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">0.5</span>*(<span style="color: #B452CD">1</span>-alpha*alpha)*(r1 + r2) +<span style="color: #B452CD">2.0</span>*alpha

<span style="color: #228B22"># Derivate of wave function ansatz as function of variational parameters</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">DerivativeWFansatz</span>(r,alpha):
    
    r1 = (r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>)
    r2 = (r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>)
    WfDer = -(r1+r2)
    <span style="color: #8B008B; font-weight: bold">return</span>  WfDer

<span style="color: #228B22"># Setting up the quantum force for the two-electron quantum dot, recall that it is a vector</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">QuantumForce</span>(r,alpha):

    qforce = np.zeros((NumberParticles,Dimension), np.double)
    qforce[<span style="color: #B452CD">0</span>,:] = -<span style="color: #B452CD">2</span>*r[<span style="color: #B452CD">0</span>,:]*alpha
    qforce[<span style="color: #B452CD">1</span>,:] = -<span style="color: #B452CD">2</span>*r[<span style="color: #B452CD">1</span>,:]*alpha
    <span style="color: #8B008B; font-weight: bold">return</span> qforce
    
<span style="color: #228B22"># Computing the derivative of the energy and the energy </span>
<span style="color: #228B22"># jit decorator tells Numba to compile this function.</span>
<span style="color: #228B22"># The argument types will be inferred by Numba when function is called.</span>
<span style="color: #707a7c">@jit</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">EnergyMinimization</span>(alpha):

    NumberMCcycles= <span style="color: #B452CD">1000</span>
    <span style="color: #228B22"># Parameters in the Fokker-Planck simulation of the quantum force</span>
    D = <span style="color: #B452CD">0.5</span>
    TimeStep = <span style="color: #B452CD">0.05</span>
    <span style="color: #228B22"># positions</span>
    PositionOld = np.zeros((NumberParticles,Dimension), np.double)
    PositionNew = np.zeros((NumberParticles,Dimension), np.double)
    <span style="color: #228B22"># Quantum force</span>
    QuantumForceOld = np.zeros((NumberParticles,Dimension), np.double)
    QuantumForceNew = np.zeros((NumberParticles,Dimension), np.double)

    <span style="color: #228B22"># seed for rng generator </span>
    seed()
    energy = <span style="color: #B452CD">0.0</span>
    DeltaE = <span style="color: #B452CD">0.0</span>
    EnergyDer = <span style="color: #B452CD">0.0</span>
    DeltaPsi = <span style="color: #B452CD">0.0</span>
    DerivativePsiE = <span style="color: #B452CD">0.0</span>
    <span style="color: #228B22">#Initial position</span>
    <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberParticles):
        <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
            PositionOld[i,j] = normalvariate(<span style="color: #B452CD">0.0</span>,<span style="color: #B452CD">1.0</span>)*sqrt(TimeStep)
    wfold = WaveFunction(PositionOld,alpha)
    QuantumForceOld = QuantumForce(PositionOld,alpha)

    <span style="color: #228B22">#Loop over MC MCcycles</span>
    <span style="color: #8B008B; font-weight: bold">for</span> MCcycle <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberMCcycles):
        <span style="color: #228B22">#Trial position moving one particle at the time</span>
        <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberParticles):
            <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
                PositionNew[i,j] = PositionOld[i,j]+normalvariate(<span style="color: #B452CD">0.0</span>,<span style="color: #B452CD">1.0</span>)*sqrt(TimeStep)+\
                                       QuantumForceOld[i,j]*TimeStep*D
            wfnew = WaveFunction(PositionNew,alpha)
            QuantumForceNew = QuantumForce(PositionNew,alpha)
            GreensFunction = <span style="color: #B452CD">0.0</span>
            <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
                GreensFunction += <span style="color: #B452CD">0.5</span>*(QuantumForceOld[i,j]+QuantumForceNew[i,j])*\
	                              (D*TimeStep*<span style="color: #B452CD">0.5</span>*(QuantumForceOld[i,j]-QuantumForceNew[i,j])-\
                                      PositionNew[i,j]+PositionOld[i,j])
      
            GreensFunction = exp(GreensFunction)
            ProbabilityRatio = GreensFunction*wfnew**<span style="color: #B452CD">2</span>/wfold**<span style="color: #B452CD">2</span>
            <span style="color: #228B22">#Metropolis-Hastings test to see whether we accept the move</span>
            <span style="color: #8B008B; font-weight: bold">if</span> random() &lt;= ProbabilityRatio:
                <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
                    PositionOld[i,j] = PositionNew[i,j]
                    QuantumForceOld[i,j] = QuantumForceNew[i,j]
                wfold = wfnew
        DeltaE = LocalEnergy(PositionOld,alpha)
        DeltaPsi = DerivativeWFansatz(PositionOld,alpha)
        energy += DeltaE
        DerivativePsiE += DeltaPsi*DeltaE
            
    <span style="color: #228B22"># We calculate mean, variance and error (no blocking applied)</span>
    energy /= NumberMCcycles
    DerivativePsiE /= NumberMCcycles
    DeltaPsi /= NumberMCcycles
    EnergyDer  = <span style="color: #B452CD">2</span>*(DerivativePsiE-DeltaPsi*energy)
    <span style="color: #8B008B; font-weight: bold">return</span> energy, EnergyDer


<span style="color: #228B22">#Here starts the main program with variable declarations</span>
NumberParticles = <span style="color: #B452CD">2</span>
Dimension = <span style="color: #B452CD">2</span>
<span style="color: #228B22"># guess for variational parameters</span>
x0 = <span style="color: #B452CD">1.5</span>
<span style="color: #228B22"># Set up iteration using stochastic gradient method</span>
Energy =<span style="color: #B452CD">0</span> ; EnergyDer = <span style="color: #B452CD">0</span>
Energy, EnergyDer = EnergyMinimization(x0)
<span style="color: #658b00">print</span>(Energy, EnergyDer)

eta = <span style="color: #B452CD">0.01</span>
Niterations = <span style="color: #B452CD">100</span>

<span style="color: #8B008B; font-weight: bold">for</span> <span style="color: #658b00">iter</span> <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Niterations):
    gradients = EnergyDer
    x0 -= eta*gradients
    Energy, EnergyDer = EnergyMinimization(x0)

<span style="color: #658b00">print</span>(x0)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
<h2 id="vmc-for-fermions-efficient-calculation-of-slater-determinants">VMC for fermions: Efficient calculation of Slater determinants </h2>
<p>The potentially most time-consuming part is the
evaluation of the gradient and the Laplacian of an \( N \)-particle  Slater
determinant. 
</p>

<p>We have to differentiate the determinant with respect to
all spatial coordinates of all particles. A brute force
differentiation would involve \( N\cdot d \) evaluations of the entire
determinant which would even worsen the already undesirable time
scaling, making it \( Nd\cdot O(N^3)\sim O(d\cdot N^4) \).
</p>

<p>This poses serious hindrances to the overall efficiency of our code.</p>

<p>The efficiency can be improved however if we move only one electron at the time.
The Slater determinant matrix \( \hat{D} \) is defined by the matrix elements
</p>
$$
d_{ij}=\phi_j(x_i)
$$

<p>where \( \phi_j(\mathbf{r}_i) \) is a single particle  wave function.
The columns correspond to the position of a given particle 
while the rows stand for the various quantum numbers.
</p>

<p>What we need to realize is that when differentiating a Slater
determinant with respect to some given coordinate, only one row of the
corresponding Slater matrix is changed. 
</p>

<p>Therefore, by recalculating
the whole determinant we risk producing redundant information. The
solution turns out to be an algorithm that requires to keep track of
the <em>inverse</em> of the Slater matrix.
</p>

<p>Let the current position in phase space be represented by the \( (N\cdot d) \)-element 
vector \( \mathbf{r}^{\mathrm{old}} \) and the new suggested
position by the vector \( \mathbf{r}^{\mathrm{new}} \).
</p>

<p>The inverse of \( \hat{D} \) can be expressed in terms of its
cofactors \( C_{ij} \) and its determinant (this our notation for a determinant) \( \vert\hat{D}\vert \):
</p>
$$
\begin{equation}
d_{ij}^{-1} = \frac{C_{ji}}{\vert\hat{D}\vert}
\label{eq:inverse_cofactor}
\end{equation}
$$

<p>Notice that the interchanged indices indicate that the matrix of cofactors is to be transposed.</p>

<p>If \( \hat{D} \) is invertible, then we must obviously have \( \hat{D}^{-1}\hat{D}= \mathbf{1} \), or explicitly in terms of the individual
elements of \( \hat{D} \) and \( \hat{D}^{-1} \):
</p>
$$
\begin{equation}
\sum_{k=1}^N d_{ik}^{\phantom X}d_{kj}^{-1} = \delta_{ij}^{\phantom X}
\label{eq:unity_explicitely}
\end{equation}
$$

<p>Consider the ratio, which we shall call \( R \), between \( \vert\hat{D}(\mathbf{r}^{\mathrm{new}})\vert \) and \( \vert\hat{D}(\mathbf{r}^{\mathrm{old}})\vert \). 
By definition, each of these determinants can
individually be expressed in terms of the <em>i</em>-th row of its cofactor
matrix
</p>
$$
\begin{equation}
R\equiv\frac{\vert\hat{D}(\mathbf{r}^{\mathrm{new}})\vert}
{\vert\hat{D}(\mathbf{r}^{\mathrm{old}})\vert} =
\frac{\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{new}})\,
C_{ij}(\mathbf{r}^{\mathrm{new}})}
{\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{old}})\,
C_{ij}(\mathbf{r}^{\mathrm{old}})}
\label{eq:detratio_cofactors}
\end{equation}
$$

<p>Suppose now that we move only one particle  at a time, meaning that
\( \mathbf{r}^{\mathrm{new}} \) differs from \( \mathbf{r}^{\mathrm{old}} \) by the
position of only one, say the <em>i</em>-th, particle . This means that \( \hat{D}(\mathbf{r}^{\mathrm{new}}) \) and \( \hat{D}(\mathbf{r}^{\mathrm{old}}) \) differ
only by the entries of the <em>i</em>-th row.  Recall also that the <em>i</em>-th row
of a cofactor matrix \( \hat{C} \) is independent of the entries of the
<em>i</em>-th row of its corresponding matrix \( \hat{D} \). In this particular
case we therefore get that the <em>i</em>-th row of \( \hat{C}(\mathbf{r}^{\mathrm{new}}) \) 
and \( \hat{C}(\mathbf{r}^{\mathrm{old}}) \) must be
equal. Explicitly, we have:
</p>
$$
\begin{equation}
C_{ij}(\mathbf{r}^{\mathrm{new}}) = C_{ij}(\mathbf{r}^{\mathrm{old}})\quad
\forall\ j\in\{1,\dots,N\}
\label{_auto5}
\end{equation}
$$

<p>Inserting this into the numerator of eq.&nbsp;\eqref{eq:detratio_cofactors}
and using eq.&nbsp;\eqref{eq:inverse_cofactor} to substitute the cofactors
with the elements of the inverse matrix, we get:
</p>
$$
\begin{equation}
R =\frac{\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{new}})\,
C_{ij}(\mathbf{r}^{\mathrm{old}})}
{\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{old}})\,
C_{ij}(\mathbf{r}^{\mathrm{old}})} =
\frac{\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{new}})\,
d_{ji}^{-1}(\mathbf{r}^{\mathrm{old}})}
{\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{old}})\,
d_{ji}^{-1}(\mathbf{r}^{\mathrm{old}})}
\label{_auto6}
\end{equation}
$$

<p>Now by eq.&nbsp;\eqref{eq:unity_explicitely} the denominator of the rightmost
expression must be unity, so that we finally arrive at:
</p>
$$
\begin{equation}
R =
\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{new}})\,
d_{ji}^{-1}(\mathbf{r}^{\mathrm{old}}) = 
\sum_{j=1}^N \phi_j(\mathbf{r}_i^{\mathrm{new}})\,
d_{ji}^{-1}(\mathbf{r}^{\mathrm{old}})
\label{eq:detratio_inverse}
\end{equation}
$$

<p>What this means is that in order to get the ratio when only the <em>i</em>-th
particle  has been moved, we only need to calculate the dot
product of the vector \( \left(\phi_1(\mathbf{r}_i^\mathrm{new}),\,\dots,\,\phi_N(\mathbf{r}_i^\mathrm{new})\right) \) of single particle  wave functions
evaluated at this new position with the <em>i</em>-th column of the inverse
matrix \( \hat{D}^{-1} \) evaluated at the original position. Such
an operation has a time scaling of \( O(N) \). The only extra thing we
need to do is to maintain the inverse matrix \( \hat{D}^{-1}(\mathbf{x}^{\mathrm{old}}) \).
</p>

<p>If the new position \( \mathbf{r}^{\mathrm{new}} \) is accepted, then the
inverse matrix can by suitably updated by an algorithm having a time
scaling of \( O(N^2) \).  This algorithm goes as
follows. First we update all but the <em>i</em>-th column of \( \hat{D}^{-1} \). For each column \( j\neq i \), we first calculate the quantity:
</p>
$$
\begin{equation}
S_j =
(\hat{D}(\mathbf{r}^{\mathrm{new}})\times
\hat{D}^{-1}(\mathbf{r}^{\mathrm{old}}))_{ij} =
\sum_{l=1}^N d_{il}(\mathbf{r}^{\mathrm{new}})\,
d^{-1}_{lj}(\mathbf{r}^{\mathrm{old}})
\label{eq:inverse_update_1}
\end{equation}
$$

<p>The new elements of the <em>j</em>-th column of \( \hat{D}^{-1} \) are then given
by:
</p>
$$
\begin{equation}
d_{kj}^{-1}(\mathbf{r}^{\mathrm{new}}) =
d_{kj}^{-1}(\mathbf{r}^{\mathrm{old}}) -
\frac{S_j}{R}\,d_{ki}^{-1}(\mathbf{r}^{\mathrm{old}})\quad
\begin{array}{ll}
\forall\ \ k\in\{1,\dots,N\}\\j\neq i
\end{array}
\label{eq:inverse_update_2}
\end{equation}
$$

<p>Finally the elements of the <em>i</em>-th column of \( \hat{D}^{-1} \) are updated
simply as follows:
</p>
$$
\begin{equation}
d_{ki}^{-1}(\mathbf{r}^{\mathrm{new}}) =
\frac{1}{R}\,d_{ki}^{-1}(\mathbf{r}^{\mathrm{old}})\quad
\forall\ \ k\in\{1,\dots,N\}
\label{eq:inverse_update_3}
\end{equation}
$$

<p>We see from these formulas that the time scaling of an update of
\( \hat{D}^{-1} \) after changing one row of \( \hat{D} \) is \( O(N^2) \).
</p>

<p>The scheme is also applicable for the calculation of the ratios
involving derivatives. It turns
out that differentiating the Slater determinant with respect
to the coordinates of a single particle  \( \mathbf{r}_i \) changes only the
<em>i</em>-th row of the corresponding Slater matrix. 
</p>
<h3 id="the-gradient-and-the-laplacian">The gradient and the Laplacian </h3>
<p>The gradient and the Laplacian can therefore be calculated as follows:</p>
$$
\frac{\vec\nabla_i\vert\hat{D}(\mathbf{r})\vert}{\vert\hat{D}(\mathbf{r})\vert} =
\sum_{j=1}^N \vec\nabla_i d_{ij}(\mathbf{r})d_{ji}^{-1}(\mathbf{r}) =
\sum_{j=1}^N \vec\nabla_i \phi_j(\mathbf{r}_i)d_{ji}^{-1}(\mathbf{r})
$$

<p>and</p>
$$
\frac{\nabla^2_i\vert\hat{D}(\mathbf{r})\vert}{\vert\hat{D}(\mathbf{r})\vert} =
\sum_{j=1}^N \nabla^2_i d_{ij}(\mathbf{r})d_{ji}^{-1}(\mathbf{r}) =
\sum_{j=1}^N \nabla^2_i \phi_j(\mathbf{r}_i)\,d_{ji}^{-1}(\mathbf{r})
$$

<p>Thus, to calculate all the derivatives of the Slater determinant, we
only need the derivatives of the single particle  wave functions
(\( \vec\nabla_i \phi_j(\mathbf{r}_i) \) and \( \nabla^2_i \phi_j(\mathbf{r}_i) \))
and the elements of the corresponding inverse Slater matrix (\( \hat{D}^{-1}(\mathbf{r}_i) \)). A calculation of a single derivative is by the
above result an \( O(N) \) operation. Since there are \( d\cdot N \)
derivatives, the time scaling of the total evaluation becomes
\( O(d\cdot N^2) \). With an \( O(N^2) \) updating algorithm for the
inverse matrix, the total scaling is no worse, which is far better
than the brute force approach yielding \( O(d\cdot N^4) \).
</p>

<p><b>Important note</b>: In most cases you end with closed form expressions for the single-particle  wave functions. It is then useful to calculate the various derivatives and make separate functions
for them.
</p>

<p>The Slater determinant takes the form  </p>
$$
   \Phi(\mathbf{r}_1,\mathbf{r}_2,,\mathbf{r}_3,\mathbf{r}_4, \alpha,\beta,\gamma,\delta)=\frac{1}{\sqrt{4!}}
\left| \begin{array}{cccc} \psi_{100\uparrow}(\mathbf{r}_1)& \psi_{100\uparrow}(\mathbf{r}_2)& \psi_{100\uparrow}(\mathbf{r}_3)&\psi_{100\uparrow}(\mathbf{r}_4) \\
\psi_{100\downarrow}(\mathbf{r}_1)& \psi_{100\downarrow}(\mathbf{r}_2)& \psi_{100\downarrow}(\mathbf{r}_3)&\psi_{100\downarrow}(\mathbf{r}_4) \\
\psi_{200\uparrow}(\mathbf{r}_1)& \psi_{200\uparrow}(\mathbf{r}_2)& \psi_{200\uparrow}(\mathbf{r}_3)&\psi_{200\uparrow}(\mathbf{r}_4) \\
\psi_{200\downarrow}(\mathbf{r}_1)& \psi_{200\downarrow}(\mathbf{r}_2)& \psi_{200\downarrow}(\mathbf{r}_3)&\psi_{200\downarrow}(\mathbf{r}_4) \end{array} \right|.
$$

<p>The Slater determinant as written is zero since the spatial wave functions for the spin up and spin down 
states are equal.  
But we can rewrite it as the product of two Slater determinants, one for spin up and one for spin down.
</p>

<p>We can rewrite it as </p>
$$
   \Phi(\mathbf{r}_1,\mathbf{r}_2,,\mathbf{r}_3,\mathbf{r}_4, \alpha,\beta,\gamma,\delta)=\det\uparrow(1,2)\det\downarrow(3,4)-\det\uparrow(1,3)\det\downarrow(2,4)
$$

$$
-\det\uparrow(1,4)\det\downarrow(3,2)+\det\uparrow(2,3)\det\downarrow(1,4)-\det\uparrow(2,4)\det\downarrow(1,3)
$$

$$
+\det\uparrow(3,4)\det\downarrow(1,2),
$$

<p>where we have defined</p>
$$
\det\uparrow(1,2)=\frac{1}{\sqrt{2}}\left| \begin{array}{cc} \psi_{100\uparrow}(\mathbf{r}_1)& \psi_{100\uparrow}(\mathbf{r}_2)\\
\psi_{200\uparrow}(\mathbf{r}_1)& \psi_{200\uparrow}(\mathbf{r}_2) \end{array} \right|,
$$

<p>and </p>
$$
\det\downarrow(3,4)=\frac{1}{\sqrt{2}}\left| \begin{array}{cc} \psi_{100\downarrow}(\mathbf{r}_3)& \psi_{100\downarrow}(\mathbf{r}_4)\\
\psi_{200\downarrow}(\mathbf{r}_3)& \psi_{200\downarrow}(\mathbf{r}_4) \end{array} \right|.
$$

<p>We want to avoid to sum over spin variables, in particular when the interaction does not depend on spin.</p>

<p>It can be shown, see for example Moskowitz and Kalos, <a href="http://onlinelibrary.wiley.com/doi/10.1002/qua.560200508/abstract" target="_blank">Int.&nbsp;J.&nbsp;Quantum Chem. <b>20</b> 1107 (1981)</a>, that for the variational energy
we can approximate the Slater determinant as  
</p>
$$
   \Phi(\mathbf{r}_1,\mathbf{r}_2,,\mathbf{r}_3,\mathbf{r}_4, \alpha,\beta,\gamma,\delta) \propto \det\uparrow(1,2)\det\downarrow(3,4),
$$

<p>or more generally as </p>
$$
   \Phi(\mathbf{r}_1,\mathbf{r}_2,\dots \mathbf{r}_N) \propto \det\uparrow \det\downarrow,
$$

<p>where we have the Slater determinant as the product of a spin up part involving the number of electrons with spin up only (2 for beryllium and 5 for neon) and a spin down part involving the electrons with spin down.</p>

<p>This ansatz is not antisymmetric under the exchange of electrons with  opposite spins but it can be shown (show this) that it gives the same
expectation value for the energy as the full Slater determinant.
</p>

<p>As long as the Hamiltonian is spin independent, the above is correct. It is rather straightforward to see this if you go back to the equations for the energy discussed earlier  this semester.</p>

<p>We will thus
factorize the full determinant \( \vert\hat{D}\vert \) into two smaller ones, where 
each can be identified with \( \uparrow \) and \( \downarrow \)
respectively:
</p>
$$
\vert\hat{D}\vert = \vert\hat{D}\vert_\uparrow\cdot \vert\hat{D}\vert_\downarrow
$$

<p>The combined dimensionality of the two smaller determinants equals the
dimensionality of the full determinant. Such a factorization is
advantageous in that it makes it possible to perform the calculation
of the ratio \( R \) and the updating of the inverse matrix separately for
\( \vert\hat{D}\vert_\uparrow \) and \( \vert\hat{D}\vert_\downarrow \):
</p>
$$
\frac{\vert\hat{D}\vert^\mathrm{new}}{\vert\hat{D}\vert^\mathrm{old}} =
\frac{\vert\hat{D}\vert^\mathrm{new}_\uparrow}
{\vert\hat{D}\vert^\mathrm{old}_\uparrow}\cdot
\frac{\vert\hat{D}\vert^\mathrm{new}_\downarrow
}{\vert\hat{D}\vert^\mathrm{old}_\downarrow}
$$

<p>This reduces the calculation time by a constant factor. The maximal
time reduction happens in a system of equal numbers of \( \uparrow \) and
\( \downarrow \) particles, so that the two factorized determinants are
half the size of the original one.
</p>

<p>Consider the case of moving only one particle  at a time which
originally had the following time scaling for one transition:
</p>
$$
O_R(N)+O_\mathrm{inverse}(N^2)
$$

<p>For the factorized determinants one of the two determinants is
obviously unaffected by the change so that it cancels from the ratio
\( R \). 
</p>

<p>Therefore, only one determinant of size \( N/2 \) is involved in each
calculation of \( R \) and update of the inverse matrix. The scaling of
each transition then becomes:
</p>
$$
O_R(N/2)+O_\mathrm{inverse}(N^2/4)
$$

<p>and the time scaling when the transitions for all \( N \) particles are
put together:
</p>
$$
O_R(N^2/2)+O_\mathrm{inverse}(N^3/4)
$$

<p>which gives the same reduction as in the case of moving all particles
at once.
</p>

<p>Computing the ratios discussed above requires that we maintain 
the inverse of the Slater matrix evaluated at the current position. 
Each time a trial position is accepted, the row number \( i \) of the Slater 
matrix changes and updating its inverse has to be carried out. 
Getting the inverse of an \( N \times N \) matrix by Gaussian elimination has a 
complexity of order of \( \mathcal{O}(N^3) \) operations, a luxury that we 
cannot afford for each time a particle  move is accepted.
We will use the expression
</p>
$$
\begin{equation*}
\label{updatingInverse}
d^{-1}_{kj}(\mathbf{x^{new}}) = \left\{\begin{array}{l l}
  d^{-1}_{kj}(\mathbf{x^{old}}) - \frac{d^{-1}_{ki}(\mathbf{x^{old}})}{R} \sum_{l=1}^{N} d_{il}(\mathbf{x^{new}})  d^{-1}_{lj}(\mathbf{x^{old}}) & \mbox{if $j \neq i$}\nonumber \\ \\
 \frac{d^{-1}_{ki}(\mathbf{x^{old}})}{R} \sum_{l=1}^{N} d_{il}(\mathbf{x^{old}}) d^{-1}_{lj}(\mathbf{x^{old}}) & \mbox{if $j=i$}
\end{array} \right.
\end{equation*}
$$

<p>This equation scales as \( O(N^2) \).
The evaluation of the determinant of an \( N \times N \) matrix by standard Gaussian elimination 
requires \( \mathbf{O}(N^3) \)
calculations. 
As there are \( Nd \) independent coordinates we need to evaluate \( Nd \) Slater determinants 
for the gradient (quantum force) and \( Nd \) for the Laplacian (kinetic energy). 
With the updating algorithm we need only to invert the Slater 
determinant matrix once. This can be done by standard LU decomposition methods.
</p>
<h3 id="expectation-value-of-the-kinetic-energy">Expectation value of the kinetic energy </h3>

<p>The expectation value of the kinetic energy expressed in atomic units for electron \( i \) is </p>
$$
 \langle \hat{K}_i \rangle = -\frac{1}{2}\frac{\langle\Psi|\nabla_{i}^2|\Psi \rangle}{\langle\Psi|\Psi \rangle},
$$

$$
\begin{equation}
\label{kineticE}
K_i = -\frac{1}{2}\frac{\nabla_{i}^{2} \Psi}{\Psi}.
\end{equation}
$$

$$
\begin{align}
\frac{\nabla^2 \Psi}{\Psi} & =  \frac{\nabla^2 ({\Psi_{D} \,  \Psi_C})}{\Psi_{D} \,  \Psi_C} = \frac{\nabla  \cdot [\nabla  {(\Psi_{D} \,  \Psi_C)}]}{\Psi_{D} \,  \Psi_C} = \frac{\nabla  \cdot [ \Psi_C \nabla  \Psi_{D} + \Psi_{D} \nabla   \Psi_C]}{\Psi_{D} \,  \Psi_C}\nonumber\\
&  =  \frac{\nabla   \Psi_C \cdot \nabla  \Psi_{D} +  \Psi_C \nabla^2 \Psi_{D} + \nabla  \Psi_{D} \cdot \nabla   \Psi_C + \Psi_{D} \nabla^2  \Psi_C}{\Psi_{D} \,  \Psi_C}\nonumber\\
\label{_auto7}
\end{align}
$$

$$
\begin{align}
\frac{\nabla^2 \Psi}{\Psi}
& =  \frac{\nabla^2 \Psi_{D}}{\Psi_{D}} + \frac{\nabla^2  \Psi_C}{ \Psi_C} + 2 \frac{\nabla  \Psi_{D}}{\Psi_{D}}\cdot\frac{\nabla   \Psi_C}{ \Psi_C}
\label{_auto8}
\end{align}
$$

<p>The second derivative of the Jastrow factor divided by the Jastrow factor (the way it enters the kinetic energy) is</p>
$$
\left[\frac{\nabla^2 \Psi_C}{\Psi_C}\right]_x =\  
2\sum_{k=1}^{N}
\sum_{i=1}^{k-1}\frac{\partial^2 g_{ik}}{\partial x_k^2}\ +\ 
\sum_{k=1}^N
\left(
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k} -
\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_i}
\right)^2
$$

<p>But we have a simple form for the function, namely</p>
$$
\Psi_{C}=\prod_{i < j}\exp{f(r_{ij})}= \exp{\left\{\sum_{i < j}\frac{ar_{ij}}{1+\beta r_{ij}}\right\}},
$$

<p>and it is easy to see that for particle  \( k \)
we have
</p>
$$
  \frac{\nabla^2_k \Psi_C}{\Psi_C }=
\sum_{ij\ne k}\frac{(\mathbf{r}_k-\mathbf{r}_i)(\mathbf{r}_k-\mathbf{r}_j)}{r_{ki}r_{kj}}f'(r_{ki})f'(r_{kj})+
\sum_{j\ne k}\left( f''(r_{kj})+\frac{2}{r_{kj}}f'(r_{kj})\right)
$$

<p>Using </p>
$$
f(r_{ij})= \frac{ar_{ij}}{1+\beta r_{ij}},
$$

<p>and \( g'(r_{kj})=dg(r_{kj})/dr_{kj} \) and 
\( g''(r_{kj})=d^2g(r_{kj})/dr_{kj}^2 \)  we find that for particle  \( k \)
we have
</p>
$$
  \frac{\nabla^2_k \Psi_C}{\Psi_C }=
\sum_{ij\ne k}\frac{(\mathbf{r}_k-\mathbf{r}_i)(\mathbf{r}_k-\mathbf{r}_j)}{r_{ki}r_{kj}}\frac{a}{(1+\beta r_{ki})^2}
\frac{a}{(1+\beta r_{kj})^2}+
\sum_{j\ne k}\left(\frac{2a}{r_{kj}(1+\beta r_{kj})^2}-\frac{2a\beta}{(1+\beta r_{kj})^3}\right)
$$

<p>The gradient and
Laplacian can be calculated as follows:
</p>
$$
\frac{\mathbf{\nabla}_i\vert\hat{D}(\mathbf{r})\vert}
{\vert\hat{D}(\mathbf{r})\vert} =
\sum_{j=1}^N \vec\nabla_i d_{ij}(\mathbf{r})\,
d_{ji}^{-1}(\mathbf{r}) =
\sum_{j=1}^N \vec\nabla_i \phi_j(\mathbf{r}_i)\,
d_{ji}^{-1}(\mathbf{r})
$$

<p>and</p>
$$
\frac{\nabla^2_i\vert\hat{D}(\mathbf{r})\vert}
{\vert\hat{D}(\mathbf{r})\vert} =
\sum_{j=1}^N \nabla^2_i d_{ij}(\mathbf{r})\,
d_{ji}^{-1}(\mathbf{r}) =
\sum_{j=1}^N \nabla^2_i \phi_j(\mathbf{r}_i)\,
d_{ji}^{-1}(\mathbf{r})
$$

<p>The gradient for the determinant is </p>
$$
\frac{\mathbf{\nabla}_i\vert\hat{D}(\mathbf{r})\vert}
{\vert\hat{D}(\mathbf{r})\vert} =
\sum_{j=1}^N \mathbf{\nabla}_i d_{ij}(\mathbf{r})\,
d_{ji}^{-1}(\mathbf{r}) =
\sum_{j=1}^N \mathbf{\nabla}_i \phi_j(\mathbf{r}_i)\,
d_{ji}^{-1}(\mathbf{r}).
$$

<p>We have</p>
$$
\Psi_C=\prod_{i < j}g(r_{ij})= \exp{\left\{\sum_{i < j}\frac{ar_{ij}}{1+\beta r_{ij}}\right\}},
$$

<p>the gradient needed for the quantum force and local energy is easy to compute.  
We get for particle  \( k \)
</p>
$$
\frac{ \nabla_k \Psi_C}{ \Psi_C }= \sum_{j\ne k}\frac{\mathbf{r}_{kj}}{r_{kj}}\frac{a}{(1+\beta r_{kj})^2},
$$

<p>which is rather easy to code.  Remember to sum over all particles  when you compute the local energy.</p>

<p>We need to compute the ratio between wave functions, in particular  for the Slater determinants.</p>
$$
R =\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{new}})\,
d_{ji}^{-1}(\mathbf{r}^{\mathrm{old}}) = 
\sum_{j=1}^N \phi_j(\mathbf{r}_i^{\mathrm{new}})\,
d_{ji}^{-1}(\mathbf{r}^{\mathrm{old}})
$$

<p>What this means is that in order to get the ratio when only the <em>i</em>-th
particle  has been moved, we only need to calculate the dot
product of the vector \( \left(\phi_1(\mathbf{r}_i^\mathrm{new}),\,\dots,\,
\phi_N(\mathbf{r}_i^\mathrm{new})\right) \) of single particle  wave functions
evaluated at this new position with the <em>i</em>-th column of the inverse
matrix \( \hat{D}^{-1} \) evaluated at the original position. Such
an operation has a time scaling of \( O(N) \). The only extra thing we
need to do is to maintain the inverse matrix 
\( \hat{D}^{-1}(\mathbf{x}^{\mathrm{old}}) \).
</p>
<h1 id="gradient-methods">Gradient Methods </h1>
<h2 id="top-down-start">Top-down start </h2>

<ul>
<li> We will start with a top-down view, with a simple harmonic oscillator problem in one dimension as case.</li>
<li> Thereafter we continue with implementing the simplest possible steepest descent approach to our two-electron problem with an electrostatic (Coulomb) interaction. Our code includes also importance sampling. The simple Python code here illustrates the basic elements which need to be included in our own code.</li>
<li> Then we move on to the mathematical description of various gradient methods.</li>
</ul>
<h2 id="motivation">Motivation  </h2>

<p>Our aim with this part of the project is to be able to</p>
<ul>
<li> find an optimal value for the variational parameters using only some few Monte Carlo cycles</li>
<li> use these optimal values for the variational parameters to perform a large-scale Monte Carlo calculation</li>
</ul>
<p>To achieve this will look at methods like <em>Steepest descent</em> and the <em>conjugate gradient method</em>. Both these methods allow us to find
the minima of a multivariable  function like our energy (function of several variational parameters). 
Alternatively, you can always use Newton's method. In particular, since we will normally have one variational parameter,
Newton's method can be easily used in finding the minimum of the local energy.
</p>
<h2 id="simple-example-and-demonstration">Simple example and demonstration </h2>

<p>Let us illustrate what is needed in our calculations using a simple example, the harmonic oscillator in one dimension.
For the harmonic oscillator in one-dimension we have a  trial wave function and probability
</p>
$$
\psi_T(x;\alpha) = \exp{-(\frac{1}{2}\alpha^2x^2)},
$$

<p>which results in a local energy </p>
$$
\frac{1}{2}\left(\alpha^2+x^2(1-\alpha^4)\right).
$$

<p>We can compare our numerically calculated energies with the exact energy as function of \( \alpha \)</p>
$$
\overline{E}[\alpha] = \frac{1}{4}\left(\alpha^2+\frac{1}{\alpha^2}\right).
$$
<h2 id="simple-example-and-demonstration">Simple example and demonstration </h2>

<p>The derivative of the energy with respect to \( \alpha \) gives</p>
$$
\begin{equation*}
\frac{d\langle  E_L[\alpha]\rangle}{d\alpha} = \frac{1}{2}\alpha-\frac{1}{2\alpha^3}
\end{equation*}
$$

<p>and a second derivative which is always positive (meaning that we find a minimum)</p>
$$
\begin{equation*}
\frac{d^2\langle  E_L[\alpha]\rangle}{d\alpha^2} = \frac{1}{2}+\frac{3}{2\alpha^4}
\end{equation*}
$$

<p>The condition</p>
$$
\begin{equation*}
\frac{d\langle  E_L[\alpha]\rangle}{d\alpha} = 0,
\end{equation*}
$$

<p>gives the optimal \( \alpha=1 \), as expected.</p>

<!-- --- begin exercise --- -->
<h2 id="exercise-1-find-the-local-energy-for-the-harmonic-oscillator">Exercise 1: Find the local energy for the harmonic oscillator </h2>

<!-- --- begin subexercise --- -->
<p>
<b>a)</b>
Derive the local energy for the harmonic oscillator in one dimension and find its expectation value.
</p>

<!-- --- end subexercise --- -->

<!-- --- begin subexercise --- -->
<p>
<b>b)</b>
Show also that the optimal value of optimal \( \alpha=1 \)
</p>

<!-- --- end subexercise --- -->

<!-- --- begin subexercise --- -->
<p>
<b>c)</b>
Repeat the above steps in two dimensions for \( N \) bosons or electrons. What is the optimal value of \( \alpha \)?
</p>

<!-- --- end subexercise --- -->

<!-- --- end exercise --- -->
<h2 id="variance-in-the-simple-model">Variance in the simple model </h2>

<p>We can also minimize the variance. In our simple model the variance is</p>

$$
\sigma^2[\alpha]=\frac{1}{4}\left(1+(1-\alpha^4)^2\frac{3}{4\alpha^4}\right)-\overline{E}^2.
$$

<p>which yields a second derivative which is always positive.</p>
<h2 id="computing-the-derivatives">Computing the derivatives </h2>

<p>In general we end up computing the expectation value of the energy in terms 
of some parameters \( \alpha_0,\alpha_1,\dots,\alpha_n \)
and we search for a minimum in this multi-variable parameter space.  
This leads to an energy minimization problem <em>where we need the derivative of the energy as a function of the variational parameters</em>.
</p>

<p>In the above example this was easy and we were able to find the expression for the derivative by simple derivations. 
However, in our actual calculations the energy is represented by a multi-dimensional integral with several variational parameters.
How can we can then obtain the derivatives of the energy with respect to the variational parameters without having 
to resort to expensive numerical derivations? 
</p>
<h2 id="expressions-for-finding-the-derivatives-of-the-local-energy">Expressions for finding the derivatives of the local energy </h2>

<p>To find the derivatives of the local energy expectation value as function of the variational parameters, we can use the chain rule and the hermiticity of the Hamiltonian.  </p>

<p>Let us define </p>
$$
\bar{E}_{\alpha}=\frac{d\langle  E_L[\alpha]\rangle}{d\alpha}.
$$

<p>as the derivative of the energy with respect to the variational parameter \( \alpha \) (we limit ourselves to one parameter only).
In the above example this was easy and we obtain a simple expression for the derivative.
We define also the derivative of the trial function (skipping the subindex \( T \)) as 
</p>
$$
\bar{\psi}_{\alpha}=\frac{d\psi[\alpha]\rangle}{d\alpha}.
$$
<h2 id="derivatives-of-the-local-energy">Derivatives of the local energy </h2>

<p>The elements of the gradient of the local energy are then (using the chain rule and the hermiticity of the Hamiltonian)</p>
$$
\bar{E}_{\alpha} = 2\left( \langle \frac{\bar{\psi}_{\alpha}}{\psi[\alpha]}E_L[\alpha]\rangle -\langle \frac{\bar{\psi}_{\alpha}}{\psi[\alpha]}\rangle\langle E_L[\alpha] \rangle\right).
$$

<p>From a computational point of view it means that you need to compute the expectation values of </p>
$$
\langle \frac{\bar{\psi}_{\alpha}}{\psi[\alpha]}E_L[\alpha]\rangle,
$$

<p>and</p>
$$
\langle \frac{\bar{\psi}_{\alpha}}{\psi[\alpha]}\rangle\langle E_L[\alpha]\rangle
$$


<!-- --- begin exercise --- -->
<h2 id="exercise-2-general-expression-for-the-derivative-of-the-energy">Exercise 2: General expression for the derivative of the energy </h2>

<!-- --- begin subexercise --- -->
<p>
<b>a)</b>
Show that 
</p>
$$
\bar{E}_{\alpha} = 2\left( \langle \frac{\bar{\psi}_{\alpha}}{\psi[\alpha]}E_L[\alpha]\rangle -\langle \frac{\bar{\psi}_{\alpha}}{\psi[\alpha]}\rangle\langle E_L[\alpha] \rangle\right).
$$


<!-- --- end subexercise --- -->

<!-- --- begin subexercise --- -->
<p>
<b>b)</b>
Find the corresponding expression for the variance.
</p>

<!-- --- end subexercise --- -->

<!-- --- end exercise --- -->
<h2 id="python-program-for-2-electrons-in-2-dimensions">Python program for 2-electrons in 2 dimensions </h2>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #228B22"># 2-electron VMC code for 2dim quantum dot with importance sampling</span>
<span style="color: #228B22"># Using gaussian rng for new positions and Metropolis- Hastings </span>
<span style="color: #228B22"># Added energy minimization with gradient descent using fixed step size</span>
<span style="color: #228B22"># To do: replace with optimization codes from scipy and/or use stochastic gradient descent</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">math</span> <span style="color: #8B008B; font-weight: bold">import</span> exp, sqrt
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">random</span> <span style="color: #8B008B; font-weight: bold">import</span> random, seed, normalvariate
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">mpl_toolkits.mplot3d</span> <span style="color: #8B008B; font-weight: bold">import</span> Axes3D
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">matplotlib</span> <span style="color: #8B008B; font-weight: bold">import</span> cm
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">matplotlib.ticker</span> <span style="color: #8B008B; font-weight: bold">import</span> LinearLocator, FormatStrFormatter
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">sys</span>



<span style="color: #228B22"># Trial wave function for the 2-electron quantum dot in two dims</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">WaveFunction</span>(r,alpha,beta):
    r1 = r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>
    r2 = r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>
    r12 = sqrt((r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>])**<span style="color: #B452CD">2</span> + (r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>])**<span style="color: #B452CD">2</span>)
    deno = r12/(<span style="color: #B452CD">1</span>+beta*r12)
    <span style="color: #8B008B; font-weight: bold">return</span> exp(-<span style="color: #B452CD">0.5</span>*alpha*(r1+r2)+deno)

<span style="color: #228B22"># Local energy  for the 2-electron quantum dot in two dims, using analytical local energy</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">LocalEnergy</span>(r,alpha,beta):
    
    r1 = (r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>)
    r2 = (r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>)
    r12 = sqrt((r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>])**<span style="color: #B452CD">2</span> + (r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>])**<span style="color: #B452CD">2</span>)
    deno = <span style="color: #B452CD">1.0</span>/(<span style="color: #B452CD">1</span>+beta*r12)
    deno2 = deno*deno
    <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">0.5</span>*(<span style="color: #B452CD">1</span>-alpha*alpha)*(r1 + r2) +<span style="color: #B452CD">2.0</span>*alpha + <span style="color: #B452CD">1.0</span>/r12+deno2*(alpha*r12-deno2+<span style="color: #B452CD">2</span>*beta*deno-<span style="color: #B452CD">1.0</span>/r12)

<span style="color: #228B22"># Derivate of wave function ansatz as function of variational parameters</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">DerivativeWFansatz</span>(r,alpha,beta):
    
    WfDer  = np.zeros((<span style="color: #B452CD">2</span>), np.double)
    r1 = (r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>)
    r2 = (r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>)
    r12 = sqrt((r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>])**<span style="color: #B452CD">2</span> + (r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>])**<span style="color: #B452CD">2</span>)
    deno = <span style="color: #B452CD">1.0</span>/(<span style="color: #B452CD">1</span>+beta*r12)
    deno2 = deno*deno
    WfDer[<span style="color: #B452CD">0</span>] = -<span style="color: #B452CD">0.5</span>*(r1+r2)
    WfDer[<span style="color: #B452CD">1</span>] = -r12*r12*deno2
    <span style="color: #8B008B; font-weight: bold">return</span>  WfDer

<span style="color: #228B22"># Setting up the quantum force for the two-electron quantum dot, recall that it is a vector</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">QuantumForce</span>(r,alpha,beta):

    qforce = np.zeros((NumberParticles,Dimension), np.double)
    r12 = sqrt((r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>])**<span style="color: #B452CD">2</span> + (r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>])**<span style="color: #B452CD">2</span>)
    deno = <span style="color: #B452CD">1.0</span>/(<span style="color: #B452CD">1</span>+beta*r12)
    qforce[<span style="color: #B452CD">0</span>,:] = -<span style="color: #B452CD">2</span>*r[<span style="color: #B452CD">0</span>,:]*alpha*(r[<span style="color: #B452CD">0</span>,:]-r[<span style="color: #B452CD">1</span>,:])*deno*deno/r12
    qforce[<span style="color: #B452CD">1</span>,:] = -<span style="color: #B452CD">2</span>*r[<span style="color: #B452CD">1</span>,:]*alpha*(r[<span style="color: #B452CD">1</span>,:]-r[<span style="color: #B452CD">0</span>,:])*deno*deno/r12
    <span style="color: #8B008B; font-weight: bold">return</span> qforce
    

<span style="color: #228B22"># Computing the derivative of the energy and the energy </span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">EnergyMinimization</span>(alpha, beta):

    NumberMCcycles= <span style="color: #B452CD">10000</span>
    <span style="color: #228B22"># Parameters in the Fokker-Planck simulation of the quantum force</span>
    D = <span style="color: #B452CD">0.5</span>
    TimeStep = <span style="color: #B452CD">0.05</span>
    <span style="color: #228B22"># positions</span>
    PositionOld = np.zeros((NumberParticles,Dimension), np.double)
    PositionNew = np.zeros((NumberParticles,Dimension), np.double)
    <span style="color: #228B22"># Quantum force</span>
    QuantumForceOld = np.zeros((NumberParticles,Dimension), np.double)
    QuantumForceNew = np.zeros((NumberParticles,Dimension), np.double)

    <span style="color: #228B22"># seed for rng generator </span>
    seed()
    energy = <span style="color: #B452CD">0.0</span>
    DeltaE = <span style="color: #B452CD">0.0</span>
    EnergyDer = np.zeros((<span style="color: #B452CD">2</span>), np.double)
    DeltaPsi = np.zeros((<span style="color: #B452CD">2</span>), np.double)
    DerivativePsiE = np.zeros((<span style="color: #B452CD">2</span>), np.double)
    <span style="color: #228B22">#Initial position</span>
    <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberParticles):
        <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
            PositionOld[i,j] = normalvariate(<span style="color: #B452CD">0.0</span>,<span style="color: #B452CD">1.0</span>)*sqrt(TimeStep)
    wfold = WaveFunction(PositionOld,alpha,beta)
    QuantumForceOld = QuantumForce(PositionOld,alpha, beta)

    <span style="color: #228B22">#Loop over MC MCcycles</span>
    <span style="color: #8B008B; font-weight: bold">for</span> MCcycle <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberMCcycles):
        <span style="color: #228B22">#Trial position moving one particle at the time</span>
        <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberParticles):
            <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
                PositionNew[i,j] = PositionOld[i,j]+normalvariate(<span style="color: #B452CD">0.0</span>,<span style="color: #B452CD">1.0</span>)*sqrt(TimeStep)+\
                                       QuantumForceOld[i,j]*TimeStep*D
            wfnew = WaveFunction(PositionNew,alpha,beta)
            QuantumForceNew = QuantumForce(PositionNew,alpha, beta)
            GreensFunction = <span style="color: #B452CD">0.0</span>
            <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
                GreensFunction += <span style="color: #B452CD">0.5</span>*(QuantumForceOld[i,j]+QuantumForceNew[i,j])*\
	                              (D*TimeStep*<span style="color: #B452CD">0.5</span>*(QuantumForceOld[i,j]-QuantumForceNew[i,j])-\
                                      PositionNew[i,j]+PositionOld[i,j])
      
            GreensFunction = exp(GreensFunction)
            ProbabilityRatio = GreensFunction*wfnew**<span style="color: #B452CD">2</span>/wfold**<span style="color: #B452CD">2</span>
            <span style="color: #228B22">#Metropolis-Hastings test to see whether we accept the move</span>
            <span style="color: #8B008B; font-weight: bold">if</span> random() &lt;= ProbabilityRatio:
                <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
                    PositionOld[i,j] = PositionNew[i,j]
                    QuantumForceOld[i,j] = QuantumForceNew[i,j]
                wfold = wfnew
        DeltaE = LocalEnergy(PositionOld,alpha,beta)
        DerPsi = DerivativeWFansatz(PositionOld,alpha,beta)
        DeltaPsi += DerPsi
        energy += DeltaE
        DerivativePsiE += DerPsi*DeltaE
            
    <span style="color: #228B22"># We calculate mean values</span>
    energy /= NumberMCcycles
    DerivativePsiE /= NumberMCcycles
    DeltaPsi /= NumberMCcycles
    EnergyDer  = <span style="color: #B452CD">2</span>*(DerivativePsiE-DeltaPsi*energy)
    <span style="color: #8B008B; font-weight: bold">return</span> energy, EnergyDer


<span style="color: #228B22">#Here starts the main program with variable declarations</span>
NumberParticles = <span style="color: #B452CD">2</span>
Dimension = <span style="color: #B452CD">2</span>
<span style="color: #228B22"># guess for variational parameters</span>
alpha = <span style="color: #B452CD">0.9</span>
beta = <span style="color: #B452CD">0.2</span>
<span style="color: #228B22"># Set up iteration using gradient descent method</span>
Energy = <span style="color: #B452CD">0</span>
EDerivative = np.zeros((<span style="color: #B452CD">2</span>), np.double)
eta = <span style="color: #B452CD">0.01</span>
Niterations = <span style="color: #B452CD">50</span>
<span style="color: #228B22"># </span>
<span style="color: #8B008B; font-weight: bold">for</span> <span style="color: #658b00">iter</span> <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Niterations):
    Energy, EDerivative = EnergyMinimization(alpha,beta)
    alphagradient = EDerivative[<span style="color: #B452CD">0</span>]
    betagradient = EDerivative[<span style="color: #B452CD">1</span>]
    alpha -= eta*alphagradient
    beta -= eta*betagradient 

<span style="color: #658b00">print</span>(alpha, beta)
<span style="color: #658b00">print</span>(Energy, EDerivative[<span style="color: #B452CD">0</span>], EDerivative[<span style="color: #B452CD">1</span>])
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
<h2 id="using-broyden-s-algorithm-in-scipy">Using Broyden's algorithm in scipy </h2>
<p>The following function uses the above described BFGS algorithm. Here we have defined a function which calculates the energy and a function which computes the first derivative.</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #228B22"># 2-electron VMC code for 2dim quantum dot with importance sampling</span>
<span style="color: #228B22"># Using gaussian rng for new positions and Metropolis- Hastings </span>
<span style="color: #228B22"># Added energy minimization using the BFGS algorithm, see p. 136 of https://www.springer.com/it/book/9780387303031</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">math</span> <span style="color: #8B008B; font-weight: bold">import</span> exp, sqrt
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">random</span> <span style="color: #8B008B; font-weight: bold">import</span> random, seed, normalvariate
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">mpl_toolkits.mplot3d</span> <span style="color: #8B008B; font-weight: bold">import</span> Axes3D
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">matplotlib</span> <span style="color: #8B008B; font-weight: bold">import</span> cm
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">matplotlib.ticker</span> <span style="color: #8B008B; font-weight: bold">import</span> LinearLocator, FormatStrFormatter
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">scipy.optimize</span> <span style="color: #8B008B; font-weight: bold">import</span> minimize
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">sys</span>


<span style="color: #228B22"># Trial wave function for the 2-electron quantum dot in two dims</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">WaveFunction</span>(r,alpha,beta):
    r1 = r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>
    r2 = r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>
    r12 = sqrt((r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>])**<span style="color: #B452CD">2</span> + (r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>])**<span style="color: #B452CD">2</span>)
    deno = r12/(<span style="color: #B452CD">1</span>+beta*r12)
    <span style="color: #8B008B; font-weight: bold">return</span> exp(-<span style="color: #B452CD">0.5</span>*alpha*(r1+r2)+deno)

<span style="color: #228B22"># Local energy  for the 2-electron quantum dot in two dims, using analytical local energy</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">LocalEnergy</span>(r,alpha,beta):
    
    r1 = (r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>)
    r2 = (r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>)
    r12 = sqrt((r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>])**<span style="color: #B452CD">2</span> + (r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>])**<span style="color: #B452CD">2</span>)
    deno = <span style="color: #B452CD">1.0</span>/(<span style="color: #B452CD">1</span>+beta*r12)
    deno2 = deno*deno
    <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">0.5</span>*(<span style="color: #B452CD">1</span>-alpha*alpha)*(r1 + r2) +<span style="color: #B452CD">2.0</span>*alpha + <span style="color: #B452CD">1.0</span>/r12+deno2*(alpha*r12-deno2+<span style="color: #B452CD">2</span>*beta*deno-<span style="color: #B452CD">1.0</span>/r12)

<span style="color: #228B22"># Derivate of wave function ansatz as function of variational parameters</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">DerivativeWFansatz</span>(r,alpha,beta):
    
    WfDer  = np.zeros((<span style="color: #B452CD">2</span>), np.double)
    r1 = (r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>)
    r2 = (r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>]**<span style="color: #B452CD">2</span> + r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>]**<span style="color: #B452CD">2</span>)
    r12 = sqrt((r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>])**<span style="color: #B452CD">2</span> + (r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>])**<span style="color: #B452CD">2</span>)
    deno = <span style="color: #B452CD">1.0</span>/(<span style="color: #B452CD">1</span>+beta*r12)
    deno2 = deno*deno
    WfDer[<span style="color: #B452CD">0</span>] = -<span style="color: #B452CD">0.5</span>*(r1+r2)
    WfDer[<span style="color: #B452CD">1</span>] = -r12*r12*deno2
    <span style="color: #8B008B; font-weight: bold">return</span>  WfDer

<span style="color: #228B22"># Setting up the quantum force for the two-electron quantum dot, recall that it is a vector</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">QuantumForce</span>(r,alpha,beta):

    qforce = np.zeros((NumberParticles,Dimension), np.double)
    r12 = sqrt((r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">0</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">0</span>])**<span style="color: #B452CD">2</span> + (r[<span style="color: #B452CD">0</span>,<span style="color: #B452CD">1</span>]-r[<span style="color: #B452CD">1</span>,<span style="color: #B452CD">1</span>])**<span style="color: #B452CD">2</span>)
    deno = <span style="color: #B452CD">1.0</span>/(<span style="color: #B452CD">1</span>+beta*r12)
    qforce[<span style="color: #B452CD">0</span>,:] = -<span style="color: #B452CD">2</span>*r[<span style="color: #B452CD">0</span>,:]*alpha*(r[<span style="color: #B452CD">0</span>,:]-r[<span style="color: #B452CD">1</span>,:])*deno*deno/r12
    qforce[<span style="color: #B452CD">1</span>,:] = -<span style="color: #B452CD">2</span>*r[<span style="color: #B452CD">1</span>,:]*alpha*(r[<span style="color: #B452CD">1</span>,:]-r[<span style="color: #B452CD">0</span>,:])*deno*deno/r12
    <span style="color: #8B008B; font-weight: bold">return</span> qforce
    

<span style="color: #228B22"># Computing the derivative of the energy and the energy </span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">EnergyDerivative</span>(x0):

    
    <span style="color: #228B22"># Parameters in the Fokker-Planck simulation of the quantum force</span>
    D = <span style="color: #B452CD">0.5</span>
    TimeStep = <span style="color: #B452CD">0.05</span>
    NumberMCcycles= <span style="color: #B452CD">10000</span>
    <span style="color: #228B22"># positions</span>
    PositionOld = np.zeros((NumberParticles,Dimension), np.double)
    PositionNew = np.zeros((NumberParticles,Dimension), np.double)
    <span style="color: #228B22"># Quantum force</span>
    QuantumForceOld = np.zeros((NumberParticles,Dimension), np.double)
    QuantumForceNew = np.zeros((NumberParticles,Dimension), np.double)

    energy = <span style="color: #B452CD">0.0</span>
    DeltaE = <span style="color: #B452CD">0.0</span>
    alpha = x0[<span style="color: #B452CD">0</span>]
    beta = x0[<span style="color: #B452CD">1</span>]
    EnergyDer = <span style="color: #B452CD">0.0</span>
    DeltaPsi = <span style="color: #B452CD">0.0</span>
    DerivativePsiE = <span style="color: #B452CD">0.0</span> 
    <span style="color: #228B22">#Initial position</span>
    <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberParticles):
        <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
            PositionOld[i,j] = normalvariate(<span style="color: #B452CD">0.0</span>,<span style="color: #B452CD">1.0</span>)*sqrt(TimeStep)
    wfold = WaveFunction(PositionOld,alpha,beta)
    QuantumForceOld = QuantumForce(PositionOld,alpha, beta)

    <span style="color: #228B22">#Loop over MC MCcycles</span>
    <span style="color: #8B008B; font-weight: bold">for</span> MCcycle <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberMCcycles):
        <span style="color: #228B22">#Trial position moving one particle at the time</span>
        <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberParticles):
            <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
                PositionNew[i,j] = PositionOld[i,j]+normalvariate(<span style="color: #B452CD">0.0</span>,<span style="color: #B452CD">1.0</span>)*sqrt(TimeStep)+\
                                       QuantumForceOld[i,j]*TimeStep*D
            wfnew = WaveFunction(PositionNew,alpha,beta)
            QuantumForceNew = QuantumForce(PositionNew,alpha, beta)
            GreensFunction = <span style="color: #B452CD">0.0</span>
            <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
                GreensFunction += <span style="color: #B452CD">0.5</span>*(QuantumForceOld[i,j]+QuantumForceNew[i,j])*\
	                              (D*TimeStep*<span style="color: #B452CD">0.5</span>*(QuantumForceOld[i,j]-QuantumForceNew[i,j])-\
                                      PositionNew[i,j]+PositionOld[i,j])
      
            GreensFunction = exp(GreensFunction)
            ProbabilityRatio = GreensFunction*wfnew**<span style="color: #B452CD">2</span>/wfold**<span style="color: #B452CD">2</span>
            <span style="color: #228B22">#Metropolis-Hastings test to see whether we accept the move</span>
            <span style="color: #8B008B; font-weight: bold">if</span> random() &lt;= ProbabilityRatio:
                <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
                    PositionOld[i,j] = PositionNew[i,j]
                    QuantumForceOld[i,j] = QuantumForceNew[i,j]
                wfold = wfnew
        DeltaE = LocalEnergy(PositionOld,alpha,beta)
        DerPsi = DerivativeWFansatz(PositionOld,alpha,beta)
        DeltaPsi += DerPsi
        energy += DeltaE
        DerivativePsiE += DerPsi*DeltaE
            
    <span style="color: #228B22"># We calculate mean values</span>
    energy /= NumberMCcycles
    DerivativePsiE /= NumberMCcycles
    DeltaPsi /= NumberMCcycles
    EnergyDer  = <span style="color: #B452CD">2</span>*(DerivativePsiE-DeltaPsi*energy)
    <span style="color: #8B008B; font-weight: bold">return</span> EnergyDer


<span style="color: #228B22"># Computing the expectation value of the local energy </span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">Energy</span>(x0):
    <span style="color: #228B22"># Parameters in the Fokker-Planck simulation of the quantum force</span>
    D = <span style="color: #B452CD">0.5</span>
    TimeStep = <span style="color: #B452CD">0.05</span>
    <span style="color: #228B22"># positions</span>
    PositionOld = np.zeros((NumberParticles,Dimension), np.double)
    PositionNew = np.zeros((NumberParticles,Dimension), np.double)
    <span style="color: #228B22"># Quantum force</span>
    QuantumForceOld = np.zeros((NumberParticles,Dimension), np.double)
    QuantumForceNew = np.zeros((NumberParticles,Dimension), np.double)

    energy = <span style="color: #B452CD">0.0</span>
    DeltaE = <span style="color: #B452CD">0.0</span>
    alpha = x0[<span style="color: #B452CD">0</span>]
    beta = x0[<span style="color: #B452CD">1</span>]
    NumberMCcycles= <span style="color: #B452CD">10000</span>
    <span style="color: #228B22">#Initial position</span>
    <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberParticles):
        <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
            PositionOld[i,j] = normalvariate(<span style="color: #B452CD">0.0</span>,<span style="color: #B452CD">1.0</span>)*sqrt(TimeStep)
    wfold = WaveFunction(PositionOld,alpha,beta)
    QuantumForceOld = QuantumForce(PositionOld,alpha, beta)

    <span style="color: #228B22">#Loop over MC MCcycles</span>
    <span style="color: #8B008B; font-weight: bold">for</span> MCcycle <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberMCcycles):
        <span style="color: #228B22">#Trial position moving one particle at the time</span>
        <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberParticles):
            <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
                PositionNew[i,j] = PositionOld[i,j]+normalvariate(<span style="color: #B452CD">0.0</span>,<span style="color: #B452CD">1.0</span>)*sqrt(TimeStep)+\
                                       QuantumForceOld[i,j]*TimeStep*D
            wfnew = WaveFunction(PositionNew,alpha,beta)
            QuantumForceNew = QuantumForce(PositionNew,alpha, beta)
            GreensFunction = <span style="color: #B452CD">0.0</span>
            <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
                GreensFunction += <span style="color: #B452CD">0.5</span>*(QuantumForceOld[i,j]+QuantumForceNew[i,j])*\
	                              (D*TimeStep*<span style="color: #B452CD">0.5</span>*(QuantumForceOld[i,j]-QuantumForceNew[i,j])-\
                                      PositionNew[i,j]+PositionOld[i,j])
      
            GreensFunction = exp(GreensFunction)
            ProbabilityRatio = GreensFunction*wfnew**<span style="color: #B452CD">2</span>/wfold**<span style="color: #B452CD">2</span>
            <span style="color: #228B22">#Metropolis-Hastings test to see whether we accept the move</span>
            <span style="color: #8B008B; font-weight: bold">if</span> random() &lt;= ProbabilityRatio:
                <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
                    PositionOld[i,j] = PositionNew[i,j]
                    QuantumForceOld[i,j] = QuantumForceNew[i,j]
                wfold = wfnew
        DeltaE = LocalEnergy(PositionOld,alpha,beta)
        energy += DeltaE
            
    <span style="color: #228B22"># We calculate mean values</span>
    energy /= NumberMCcycles
    <span style="color: #8B008B; font-weight: bold">return</span> energy




<span style="color: #228B22">#Here starts the main program with variable declarations</span>
NumberParticles = <span style="color: #B452CD">2</span>
Dimension = <span style="color: #B452CD">2</span>
<span style="color: #228B22"># seed for rng generator </span>
seed()
<span style="color: #228B22"># guess for variational parameters</span>
x0 = np.array([<span style="color: #B452CD">0.9</span>,<span style="color: #B452CD">0.2</span>])
<span style="color: #228B22"># Using Broydens method</span>
res = minimize(Energy, x0, method=<span style="color: #CD5555">&#39;BFGS&#39;</span>, jac=EnergyDerivative, options={<span style="color: #CD5555">&#39;gtol&#39;</span>: <span style="color: #B452CD">1e-4</span>,<span style="color: #CD5555">&#39;disp&#39;</span>: <span style="color: #8B008B; font-weight: bold">True</span>})
<span style="color: #658b00">print</span>(res.x)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Note that the <b>minimize</b> function returns the finale values for the variable \( \alpha=x0[0] \) and \( \beta=x0[1] \) in the array \( x \). </p>
<h2 id="brief-reminder-on-newton-raphson-s-method">Brief reminder on Newton-Raphson's method </h2>

<p>Let us quickly remind ourselves how we derive the above method.</p>

<p>Perhaps the most celebrated of all one-dimensional root-finding
routines is Newton's method, also called the Newton-Raphson
method. This method  requires the evaluation of both the
function \( f \) and its derivative \( f' \) at arbitrary points. 
If you can only calculate the derivative
numerically and/or your function is not of the smooth type, we
normally discourage the use of this method.
</p>
<h2 id="the-equations">The equations </h2>

<p>The Newton-Raphson formula consists geometrically of extending the
tangent line at a current point until it crosses zero, then setting
the next guess to the abscissa of that zero-crossing.  The mathematics
behind this method is rather simple. Employing a Taylor expansion for
\( x \) sufficiently close to the solution \( s \), we have
</p>

$$
    f(s)=0=f(x)+(s-x)f'(x)+\frac{(s-x)^2}{2}f''(x) +\dots.
    \label{eq:taylornr}
$$

<p>For small enough values of the function and for well-behaved
functions, the terms beyond linear are unimportant, hence we obtain
</p>

$$
   f(x)+(s-x)f'(x)\approx 0,
$$

<p>yielding</p>
$$
   s\approx x-\frac{f(x)}{f'(x)}.
$$

<p>Having in mind an iterative procedure, it is natural to start iterating with</p>
$$
   x_{n+1}=x_n-\frac{f(x_n)}{f'(x_n)}.
$$
<h2 id="simple-geometric-interpretation">Simple geometric interpretation </h2>

<p>The above is Newton-Raphson's method. It has a simple geometric
interpretation, namely \( x_{n+1} \) is the point where the tangent from
\( (x_n,f(x_n)) \) crosses the \( x \)-axis.  Close to the solution,
Newton-Raphson converges fast to the desired result. However, if we
are far from a root, where the higher-order terms in the series are
important, the Newton-Raphson formula can give grossly inaccurate
results. For instance, the initial guess for the root might be so far
from the true root as to let the search interval include a local
maximum or minimum of the function.  If an iteration places a trial
guess near such a local extremum, so that the first derivative nearly
vanishes, then Newton-Raphson may fail totally
</p>
<h2 id="extending-to-more-than-one-variable">Extending to more than one variable </h2>

<p>Newton's method can be generalized to systems of several non-linear equations
and variables. Consider the case with two equations
</p>
$$
   \begin{array}{cc} f_1(x_1,x_2) &=0\\
                     f_2(x_1,x_2) &=0,\end{array}
$$

<p>which we Taylor expand to obtain</p>

$$
   \begin{array}{cc} 0=f_1(x_1+h_1,x_2+h_2)=&f_1(x_1,x_2)+h_1
                     \partial f_1/\partial x_1+h_2
                     \partial f_1/\partial x_2+\dots\\
                     0=f_2(x_1+h_1,x_2+h_2)=&f_2(x_1,x_2)+h_1
                     \partial f_2/\partial x_1+h_2
                     \partial f_2/\partial x_2+\dots
                       \end{array}.
$$

<p>Defining the Jacobian matrix \( \hat{J} \) we have</p>
$$
 \hat{J}=\left( \begin{array}{cc}
                         \partial f_1/\partial x_1  & \partial f_1/\partial x_2 \\
                          \partial f_2/\partial x_1     &\partial f_2/\partial x_2
             \end{array} \right),
$$

<p>we can rephrase Newton's method as</p>
$$
\left(\begin{array}{c} x_1^{n+1} \\ x_2^{n+1} \end{array} \right)=
\left(\begin{array}{c} x_1^{n} \\ x_2^{n} \end{array} \right)+
\left(\begin{array}{c} h_1^{n} \\ h_2^{n} \end{array} \right),
$$

<p>where we have defined</p>
$$
   \left(\begin{array}{c} h_1^{n} \\ h_2^{n} \end{array} \right)=
   -{\bf \hat{J}}^{-1}
   \left(\begin{array}{c} f_1(x_1^{n},x_2^{n}) \\ f_2(x_1^{n},x_2^{n}) \end{array} \right).
$$

<p>We need thus to compute the inverse of the Jacobian matrix and it
is to understand that difficulties  may
arise in case \( \hat{J} \) is nearly singular.
</p>

<p>It is rather straightforward to extend the above scheme to systems of
more than two non-linear equations. In our case, the Jacobian matrix is given by the Hessian that represents the second derivative of cost function. 
</p>
<h2 id="steepest-descent">Steepest descent </h2>

<p>The basic idea of gradient descent is
that a function \( F(\mathbf{x}) \), 
\( \mathbf{x} \equiv (x_1,\cdots,x_n) \), decreases fastest if one goes from \( \bf {x} \) in the
direction of the negative gradient \( -\nabla F(\mathbf{x}) \).
</p>

<p>It can be shown that if </p>
$$
\mathbf{x}_{k+1} = \mathbf{x}_k - \gamma_k \nabla F(\mathbf{x}_k),
$$

<p>with \( \gamma_k > 0 \).</p>

<p>For \( \gamma_k \) small enough, then \( F(\mathbf{x}_{k+1}) \leq
F(\mathbf{x}_k) \). This means that for a sufficiently small \( \gamma_k \)
we are always moving towards smaller function values, i.e a minimum.
</p>
<h2 id="more-on-steepest-descent">More on Steepest descent </h2>

<p>The previous observation is the basis of the method of steepest
descent, which is also referred to as just gradient descent (GD). One
starts with an initial guess \( \mathbf{x}_0 \) for a minimum of \( F \) and
computes new approximations according to
</p>

$$
\mathbf{x}_{k+1} = \mathbf{x}_k - \gamma_k \nabla F(\mathbf{x}_k), \ \ k \geq 0.
$$

<p>The parameter \( \gamma_k \) is often referred to as the step length or
the learning rate within the context of Machine Learning.
</p>
<h2 id="the-ideal">The ideal </h2>

<p>Ideally the sequence \( \{\mathbf{x}_k \}_{k=0} \) converges to a global
minimum of the function \( F \). In general we do not know if we are in a
global or local minimum. In the special case when \( F \) is a convex
function, all local minima are also global minima, so in this case
gradient descent can converge to the global solution. The advantage of
this scheme is that it is conceptually simple and straightforward to
implement. However the method in this form has some severe
limitations:
</p>

<p>In machine learing we are often faced with non-convex high dimensional
cost functions with many local minima. Since GD is deterministic we
will get stuck in a local minimum, if the method converges, unless we
have a very good intial guess. This also implies that the scheme is
sensitive to the chosen initial condition.
</p>

<p>Note that the gradient is a function of \( \mathbf{x} =
(x_1,\cdots,x_n) \) which makes it expensive to compute numerically.
</p>
<h2 id="the-sensitiveness-of-the-gradient-descent">The sensitiveness of the gradient descent </h2>

<p>The gradient descent method 
is sensitive to the choice of learning rate \( \gamma_k \). This is due
to the fact that we are only guaranteed that \( F(\mathbf{x}_{k+1}) \leq
F(\mathbf{x}_k) \) for sufficiently small \( \gamma_k \). The problem is to
determine an optimal learning rate. If the learning rate is chosen too
small the method will take a long time to converge and if it is too
large we can experience erratic behavior.
</p>

<p>Many of these shortcomings can be alleviated by introducing
randomness. One such method is that of Stochastic Gradient Descent
(SGD), see below.
</p>
<h2 id="convex-functions">Convex functions </h2>

<p>Ideally we want our cost/loss function to be convex(concave).</p>

<p>First we give the definition of a convex set: A set \( C \) in
\( \mathbb{R}^n \) is said to be convex if, for all \( x \) and \( y \) in \( C \) and
all \( t \in (0,1) \) , the point \( (1 &#8722; t)x + ty \) also belongs to
C. Geometrically this means that every point on the line segment
connecting \( x \) and \( y \) is in \( C \) as discussed below.
</p>

<p>The convex subsets of \( \mathbb{R} \) are the intervals of
\( \mathbb{R} \). Examples of convex sets of \( \mathbb{R}^2 \) are the
regular polygons (triangles, rectangles, pentagons, etc...).
</p>
<h2 id="convex-function">Convex function </h2>

<p><b>Convex function</b>: Let \( X \subset \mathbb{R}^n \) be a convex set. Assume that the function \( f: X \rightarrow \mathbb{R} \) is continuous, then \( f \) is said to be convex if $$f(tx_1 + (1-t)x_2) \leq tf(x_1) + (1-t)f(x_2) $$ for all \( x_1, x_2 \in X \) and for all \( t \in [0,1] \). If \( \leq \) is replaced with a strict inequaltiy in the definition, we demand \( x_1 \neq x_2 \) and \( t\in(0,1) \) then \( f \) is said to be strictly convex. For a single variable function, convexity means that if you draw a straight line connecting \( f(x_1) \) and \( f(x_2) \), the value of the function on the interval \( [x_1,x_2] \) is always below the line as illustrated below.</p>
<h2 id="conditions-on-convex-functions">Conditions on convex functions </h2>

<p>In the following we state first and second-order conditions which
ensures convexity of a function \( f \). We write \( D_f \) to denote the
domain of \( f \), i.e the subset of \( R^n \) where \( f \) is defined. For more
details and proofs we refer to: <a href="http://stanford.edu/boyd/cvxbook/, 2004" target="_blank">S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press</a>.
</p>

<p> First order condition
Suppose \( f \) is differentiable (i.e \( \nabla f(x) \) is well defined for
all \( x \) in the domain of \( f \)). Then \( f \) is convex if and only if \( D_f \)
is a convex set and $$f(y) \geq f(x) + \nabla f(x)^T (y-x) $$ holds
for all \( x,y \in D_f \). This condition means that for a convex function
the first order Taylor expansion (right hand side above) at any point
a global under estimator of the function. To convince yourself you can
make a drawing of \( f(x) = x^2+1 \) and draw the tangent line to \( f(x) \) and
note that it is always below the graph.  
</p>

<p> Second order condition 
Assume that \( f \) is twice
differentiable, i.e the Hessian matrix exists at each point in
\( D_f \). Then \( f \) is convex if and only if \( D_f \) is a convex set and its
Hessian is positive semi-definite for all \( x\in D_f \). For a
single-variable function this reduces to \( f''(x) \geq 0 \). Geometrically this means that \( f \) has nonnegative curvature
everywhere.
</p>

<p>This condition is particularly useful since it gives us an procedure for determining if the function under consideration is convex, apart from using the definition.</p>
<h2 id="more-on-convex-functions">More on convex functions </h2>

<p>The next result is of great importance to us and the reason why we are
going on about convex functions. In machine learning we frequently
have to minimize a loss/cost function in order to find the best
parameters for the model we are considering. 
</p>

<p>Ideally we want the
global minimum (for high-dimensional models it is hard to know
if we have local or global minimum). However, if the cost/loss function
is convex the following result provides invaluable information:
</p>

<p> Any minimum is global for convex functions
Consider the problem of finding \( x \in \mathbb{R}^n \) such that \( f(x) \)
is minimal, where \( f \) is convex and differentiable. Then, any point
\( x^* \) that satisfies \( \nabla f(x^*) = 0 \) is a global minimum.
</p>

<p>This result means that if we know that the cost/loss function is convex and we are able to find a minimum, we are guaranteed that it is a global minimum.</p>
<h2 id="some-simple-problems">Some simple problems </h2>

<ol>
<li> Show that \( f(x)=x^2 \) is convex for \( x \in \mathbb{R} \) using the definition of convexity. Hint: If you re-write the definition, \( f \) is convex if the following holds for all \( x,y \in D_f \) and any \( \lambda \in [0,1] \) $\lambda f(x)+(1-\lambda)f(y)-f(\lambda x + (1-\lambda) y ) \geq 0$.</li>
<li> Using the second order condition show that the following functions are convex on the specified domain.</li>
<ul>
 <li> \( f(x) = e^x \) is convex for \( x \in \mathbb{R} \).</li>
 <li> \( g(x) = -\ln(x) \) is convex for \( x \in (0,\infty) \).</li>
</ul>
<li> Let \( f(x) = x^2 \) and \( g(x) = e^x \). Show that \( f(g(x)) \) and \( g(f(x)) \) is convex for \( x \in \mathbb{R} \). Also show that if \( f(x) \) is any convex function than \( h(x) = e^{f(x)} \) is convex.</li>
<li> A norm is any function that satisfy the following properties</li>
<ul>
 <li> \( f(\alpha x) = |\alpha| f(x) \) for all \( \alpha \in \mathbb{R} \).</li>
 <li> \( f(x+y) \leq f(x) + f(y) \)</li>
 <li> \( f(x) \leq 0 \) for all \( x \in \mathbb{R}^n \) with equality if and only if \( x = 0 \)</li>
</ul>
</ol>
<p>Using the definition of convexity, try to show that a function satisfying the properties above is convex (the third condition is not needed to show this).</p>
<h2 id="standard-steepest-descent">Standard steepest descent </h2>

<p>Before we proceed, we would like to discuss the approach called the
<b>standard Steepest descent</b>, which again leads to us having to be able
to compute a matrix. It belongs to the class of Conjugate Gradient methods (CG).
</p>

<a href="https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf" target="_blank">The success of the CG method</a>
<p>for finding solutions of non-linear problems is based on the theory
of conjugate gradients for linear systems of equations. It belongs to
the class of iterative methods for solving problems from linear
algebra of the type 
</p>
$$
\begin{equation*} 
\hat{A}\hat{x} = \hat{b}.
\end{equation*} 
$$

<p>In the iterative process we end up with a problem like</p>

$$
\begin{equation*}
  \hat{r}= \hat{b}-\hat{A}\hat{x},
\end{equation*}
$$

<p>where \( \hat{r} \) is the so-called residual or error in the iterative process.</p>

<p>When we have found the exact solution, \( \hat{r}=0 \).</p>
<h2 id="gradient-method">Gradient method </h2>

<p>The residual is zero when we reach the minimum of the quadratic equation</p>
$$
\begin{equation*}
  P(\hat{x})=\frac{1}{2}\hat{x}^T\hat{A}\hat{x} - \hat{x}^T\hat{b},
\end{equation*}
$$

<p>with the constraint that the matrix \( \hat{A} \) is positive definite and
symmetric.  This defines also the Hessian and we want it to be  positive definite.  
</p>
<h2 id="steepest-descent-method">Steepest descent  method </h2>

<p>We denote the initial guess for \( \hat{x} \) as \( \hat{x}_0 \). 
We can assume without loss of generality that
</p>
$$
\begin{equation*}
\hat{x}_0=0,
\end{equation*}
$$

<p>or consider the system</p>
$$
\begin{equation*}
\hat{A}\hat{z} = \hat{b}-\hat{A}\hat{x}_0,
\end{equation*}
$$

<p>instead.</p>
<h2 id="steepest-descent-method">Steepest descent  method </h2>

<p>One can show that the solution \( \hat{x} \) is also the unique minimizer of the quadratic form</p>
$$
\begin{equation*}
  f(\hat{x}) = \frac{1}{2}\hat{x}^T\hat{A}\hat{x} - \hat{x}^T \hat{x} , \quad \hat{x}\in\mathbf{R}^n. 
\end{equation*}
$$

<p>This suggests taking the first basis vector \( \hat{r}_1 \) (see below for definition) 
to be the gradient of \( f \) at \( \hat{x}=\hat{x}_0 \), 
which equals
</p>
$$
\begin{equation*}
\hat{A}\hat{x}_0-\hat{b},
\end{equation*}
$$

<p>and 
\( \hat{x}_0=0 \) it is equal \( -\hat{b} \).
</p>
<h2 id="final-expressions">Final expressions </h2>

<p>We can compute the residual iteratively as</p>
$$
\begin{equation*}
\hat{r}_{k+1}=\hat{b}-\hat{A}\hat{x}_{k+1},
 \end{equation*}
$$

<p>which equals</p>
$$
\begin{equation*}
\hat{b}-\hat{A}(\hat{x}_k+\alpha_k\hat{r}_k),
 \end{equation*}
$$

<p>or</p>
$$
\begin{equation*}
(\hat{b}-\hat{A}\hat{x}_k)-\alpha_k\hat{A}\hat{r}_k,
 \end{equation*}
$$

<p>which gives</p>

$$
\alpha_k = \frac{\hat{r}_k^T\hat{r}_k}{\hat{r}_k^T\hat{A}\hat{r}_k}
$$

<p>leading to the iterative scheme</p>
$$
\begin{equation*}
\hat{x}_{k+1}=\hat{x}_k-\alpha_k\hat{r}_{k},
 \end{equation*}
$$
<h2 id="conjugate-gradient-method">Conjugate gradient method </h2>

<p>In the CG method we define so-called conjugate directions and two vectors 
\( \hat{s} \) and \( \hat{t} \)
are said to be
conjugate if
</p>
$$
\begin{equation*}
\hat{s}^T\hat{A}\hat{t}= 0.
\end{equation*}
$$

<p>The philosophy of the CG method is to perform searches in various conjugate directions
of our vectors \( \hat{x}_i \) obeying the above criterion, namely
</p>
$$
\begin{equation*}
\hat{x}_i^T\hat{A}\hat{x}_j= 0.
\end{equation*}
$$

<p>Two vectors are conjugate if they are orthogonal with respect to 
this inner product. Being conjugate is a symmetric relation: if \( \hat{s} \) is conjugate to \( \hat{t} \), then \( \hat{t} \) is conjugate to \( \hat{s} \).
</p>
<h2 id="conjugate-gradient-method">Conjugate gradient method </h2>

<p>An example is given by the eigenvectors of the matrix</p>
$$
\begin{equation*}
\hat{v}_i^T\hat{A}\hat{v}_j= \lambda\hat{v}_i^T\hat{v}_j,
\end{equation*}
$$

<p>which is zero unless \( i=j \). </p>
<h2 id="conjugate-gradient-method">Conjugate gradient method </h2>

<p>Assume now that we have a symmetric positive-definite matrix \( \hat{A} \) of size
\( n\times n \). At each iteration \( i+1 \) we obtain the conjugate direction of a vector
</p>
$$
\begin{equation*}
\hat{x}_{i+1}=\hat{x}_{i}+\alpha_i\hat{p}_{i}. 
\end{equation*}
$$

<p>We assume that \( \hat{p}_{i} \) is a sequence of \( n \) mutually conjugate directions. 
Then the \( \hat{p}_{i} \)  form a basis of \( R^n \) and we can expand the solution 
$  \hat{A}\hat{x} = \hat{b}$ in this basis, namely
</p>

$$
\begin{equation*}
  \hat{x}  = \sum^{n}_{i=1} \alpha_i \hat{p}_i.
\end{equation*}
$$
<h2 id="conjugate-gradient-method">Conjugate gradient method </h2>

<p>The coefficients are given by</p>
$$
\begin{equation*}
    \mathbf{A}\mathbf{x} = \sum^{n}_{i=1} \alpha_i \mathbf{A} \mathbf{p}_i = \mathbf{b}.
\end{equation*}
$$

<p>Multiplying with \( \hat{p}_k^T \)  from the left gives</p>

$$
\begin{equation*}
  \hat{p}_k^T \hat{A}\hat{x} = \sum^{n}_{i=1} \alpha_i\hat{p}_k^T \hat{A}\hat{p}_i= \hat{p}_k^T \hat{b},
\end{equation*}
$$

<p>and we can define the coefficients \( \alpha_k \) as</p>

$$
\begin{equation*}
    \alpha_k = \frac{\hat{p}_k^T \hat{b}}{\hat{p}_k^T \hat{A} \hat{p}_k}
\end{equation*}
$$
<h2 id="conjugate-gradient-method-and-iterations">Conjugate gradient method and iterations </h2>

<p>If we choose the conjugate vectors \( \hat{p}_k \) carefully, 
then we may not need all of them to obtain a good approximation to the solution 
\( \hat{x} \). 
We want to regard the conjugate gradient method as an iterative method. 
This will us to solve systems where \( n \) is so large that the direct 
method would take too much time.
</p>

<p>We denote the initial guess for \( \hat{x} \) as \( \hat{x}_0 \). 
We can assume without loss of generality that
</p>
$$
\begin{equation*}
\hat{x}_0=0,
\end{equation*}
$$

<p>or consider the system</p>
$$
\begin{equation*}
\hat{A}\hat{z} = \hat{b}-\hat{A}\hat{x}_0,
\end{equation*}
$$

<p>instead.</p>
<h2 id="conjugate-gradient-method">Conjugate gradient method </h2>

<p>One can show that the solution \( \hat{x} \) is also the unique minimizer of the quadratic form</p>
$$
\begin{equation*}
  f(\hat{x}) = \frac{1}{2}\hat{x}^T\hat{A}\hat{x} - \hat{x}^T \hat{x} , \quad \hat{x}\in\mathbf{R}^n. 
\end{equation*}
$$

<p>This suggests taking the first basis vector \( \hat{p}_1 \) 
to be the gradient of \( f \) at \( \hat{x}=\hat{x}_0 \), 
which equals
</p>
$$
\begin{equation*}
\hat{A}\hat{x}_0-\hat{b},
\end{equation*}
$$

<p>and 
\( \hat{x}_0=0 \) it is equal \( -\hat{b} \).
The other vectors in the basis will be conjugate to the gradient, 
hence the name conjugate gradient method.
</p>
<h2 id="conjugate-gradient-method">Conjugate gradient method </h2>

<p>Let  \( \hat{r}_k \) be the residual at the \( k \)-th step:</p>
$$
\begin{equation*}
\hat{r}_k=\hat{b}-\hat{A}\hat{x}_k.
\end{equation*}
$$

<p>Note that \( \hat{r}_k \) is the negative gradient of \( f \) at 
\( \hat{x}=\hat{x}_k \), 
so the gradient descent method would be to move in the direction \( \hat{r}_k \). 
Here, we insist that the directions \( \hat{p}_k \) are conjugate to each other, 
so we take the direction closest to the gradient \( \hat{r}_k \)  
under the conjugacy constraint. 
This gives the following expression
</p>
$$
\begin{equation*}
\hat{p}_{k+1}=\hat{r}_k-\frac{\hat{p}_k^T \hat{A}\hat{r}_k}{\hat{p}_k^T\hat{A}\hat{p}_k} \hat{p}_k.
\end{equation*}
$$
<h2 id="conjugate-gradient-method">Conjugate gradient method </h2>

<p>We can also  compute the residual iteratively as</p>
$$
\begin{equation*}
\hat{r}_{k+1}=\hat{b}-\hat{A}\hat{x}_{k+1},
 \end{equation*}
$$

<p>which equals</p>
$$
\begin{equation*}
\hat{b}-\hat{A}(\hat{x}_k+\alpha_k\hat{p}_k),
 \end{equation*}
$$

<p>or</p>
$$
\begin{equation*}
(\hat{b}-\hat{A}\hat{x}_k)-\alpha_k\hat{A}\hat{p}_k,
 \end{equation*}
$$

<p>which gives</p>

$$
\begin{equation*}
\hat{r}_{k+1}=\hat{r}_k-\hat{A}\hat{p}_{k},
 \end{equation*}
$$
<h2 id="broyden-fletcher-goldfarb-shanno-algorithm">Broyden&#8211;Fletcher&#8211;Goldfarb&#8211;Shanno algorithm </h2>

<p>The optimization problem is to minimize \( f(\mathbf {x} ) \) where \( \mathbf {x} \)  is a vector in \( R^{n} \), and \( f \) is a differentiable scalar function. There are no constraints on the values that  \( \mathbf {x} \)  can take.</p>

<p>The algorithm begins at an initial estimate for the optimal value \( \mathbf {x}_{0} \) and proceeds iteratively to get a better estimate at each stage.</p>

<p>The search direction \( p_k \) at stage \( k \) is given by the solution of the analogue of the Newton equation</p>
$$
B_{k}\mathbf {p} _{k}=-\nabla f(\mathbf {x}_{k}),
$$

<p>where \( B_{k} \) is an approximation to the Hessian matrix, which is
updated iteratively at each stage, and \( \nabla f(\mathbf {x} _{k}) \)
is the gradient of the function
evaluated at \( x_k \). 
A line search in the direction \( p_k \) is then used to
find the next point \( x_{k+1} \) by minimising 
</p>
$$
f(\mathbf {x}_{k}+\alpha \mathbf {p}_{k}),
$$

<p>over the scalar \( \alpha > 0 \).</p>
<h2 id="stochastic-gradient-descent">Stochastic Gradient Descent </h2>

<p>Stochastic gradient descent (SGD) and variants thereof address some of
the shortcomings of the Gradient descent method discussed above.
</p>

<p>The underlying idea of SGD comes from the observation that a given 
function, which we want to minimize, can almost always be written as a
sum over \( n \) data points \( \{\mathbf{x}_i\}_{i=1}^n \),
</p>
$$
C(\mathbf{\beta}) = \sum_{i=1}^n c_i(\mathbf{x}_i,
\mathbf{\beta}). 
$$
<h2 id="computation-of-gradients">Computation of gradients </h2>

<p>This in turn means that the gradient can be
computed as a sum over \( i \)-gradients 
</p>
$$
\nabla_\beta C(\mathbf{\beta}) = \sum_i^n \nabla_\beta c_i(\mathbf{x}_i,
\mathbf{\beta}).
$$

<p>Stochasticity/randomness is introduced by only taking the
gradient on a subset of the data called minibatches.  If there are \( n \)
data points and the size of each minibatch is \( M \), there will be \( n/M \)
minibatches. We denote these minibatches by \( B_k \) where
\( k=1,\cdots,n/M \).
</p>
<h2 id="sgd-example">SGD example </h2>
<p>As an example, suppose we have \( 10 \) data points \( (\mathbf{x}_1,\cdots, \mathbf{x}_{10}) \) 
and we choose to have \( M=5 \) minibathces,
then each minibatch contains two data points. In particular we have
\( B_1 = (\mathbf{x}_1,\mathbf{x}_2), \cdots, B_5 =
(\mathbf{x}_9,\mathbf{x}_{10}) \). Note that if you choose \( M=1 \) you
have only a single batch with all data points and on the other extreme,
you may choose \( M=n \) resulting in a minibatch for each datapoint, i.e
\( B_k = \mathbf{x}_k \).
</p>

<p>The idea is now to approximate the gradient by replacing the sum over
all data points with a sum over the data points in one the minibatches
picked at random in each gradient descent step 
</p>
$$
\nabla_{\beta}
C(\mathbf{\beta}) = \sum_{i=1}^n \nabla_\beta c_i(\mathbf{x}_i,
\mathbf{\beta}) \rightarrow \sum_{i \in B_k}^n \nabla_\beta
c_i(\mathbf{x}_i, \mathbf{\beta}).
$$
<h2 id="the-gradient-step">The gradient step </h2>

<p>Thus a gradient descent step now looks like </p>
$$
\beta_{j+1} = \beta_j - \gamma_j \sum_{i \in B_k}^n \nabla_\beta c_i(\mathbf{x}_i,
\mathbf{\beta})
$$

<p>where \( k \) is picked at random with equal
probability from \( [1,n/M] \). An iteration over the number of
minibathces (n/M) is commonly referred to as an epoch. Thus it is
typical to choose a number of epochs and for each epoch iterate over
the number of minibatches, as exemplified in the code below.
</p>
<h2 id="simple-example-code">Simple example code </h2>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span> 

n = <span style="color: #B452CD">100</span> <span style="color: #228B22">#100 datapoints </span>
M = <span style="color: #B452CD">5</span>   <span style="color: #228B22">#size of each minibatch</span>
m = <span style="color: #658b00">int</span>(n/M) <span style="color: #228B22">#number of minibatches</span>
n_epochs = <span style="color: #B452CD">10</span> <span style="color: #228B22">#number of epochs</span>

j = <span style="color: #B452CD">0</span>
<span style="color: #8B008B; font-weight: bold">for</span> epoch <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #B452CD">1</span>,n_epochs+<span style="color: #B452CD">1</span>):
    <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(m):
        k = np.random.randint(m) <span style="color: #228B22">#Pick the k-th minibatch at random</span>
        <span style="color: #228B22">#Compute the gradient using the data in minibatch Bk</span>
        <span style="color: #228B22">#Compute new suggestion for </span>
        j += <span style="color: #B452CD">1</span>
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>Taking the gradient only on a subset of the data has two important
benefits. First, it introduces randomness which decreases the chance
that our opmization scheme gets stuck in a local minima. Second, if
the size of the minibatches are small relative to the number of
datapoints (\( M <  n \)), the computation of the gradient is much
cheaper since we sum over the datapoints in the \( k-th \) minibatch and not
all \( n \) datapoints.
</p>
<h2 id="when-do-we-stop">When do we stop? </h2>

<p>A natural question is when do we stop the search for a new minimum?
One possibility is to compute the full gradient after a given number
of epochs and check if the norm of the gradient is smaller than some
threshold and stop if true. However, the condition that the gradient
is zero is valid also for local minima, so this would only tell us
that we are close to a local/global minimum. However, we could also
evaluate the cost function at this point, store the result and
continue the search. If the test kicks in at a later stage we can
compare the values of the cost function and keep the \( \beta \) that
gave the lowest value.
</p>
<h2 id="slightly-different-approach">Slightly different approach </h2>

<p>Another approach is to let the step length \( \gamma_j \) depend on the
number of epochs in such a way that it becomes very small after a
reasonable time such that we do not move at all.
</p>

<p>As an example, let \( e = 0,1,2,3,\cdots \) denote the current epoch and let \( t_0, t_1 > 0 \) be two fixed numbers. Furthermore, let \( t = e \cdot m + i \) where \( m \) is the number of minibatches and \( i=0,\cdots,m-1 \). Then the function $$\gamma_j(t; t_0, t_1) = \frac{t_0}{t+t_1} $$ goes to zero as the number of epochs gets large. I.e. we start with a step length \( \gamma_j (0; t_0, t_1) = t_0/t_1 \) which decays in <em>time</em> \( t \).</p>

<p>In this way we can fix the number of epochs, compute \( \beta \) and
evaluate the cost function at the end. Repeating the computation will
give a different result since the scheme is random by design. Then we
pick the final \( \beta \) that gives the lowest value of the cost
function.
</p>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span> 

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">step_length</span>(t,t0,t1):
    <span style="color: #8B008B; font-weight: bold">return</span> t0/(t+t1)

n = <span style="color: #B452CD">100</span> <span style="color: #228B22">#100 datapoints </span>
M = <span style="color: #B452CD">5</span>   <span style="color: #228B22">#size of each minibatch</span>
m = <span style="color: #658b00">int</span>(n/M) <span style="color: #228B22">#number of minibatches</span>
n_epochs = <span style="color: #B452CD">500</span> <span style="color: #228B22">#number of epochs</span>
t0 = <span style="color: #B452CD">1.0</span>
t1 = <span style="color: #B452CD">10</span>

gamma_j = t0/t1
j = <span style="color: #B452CD">0</span>
<span style="color: #8B008B; font-weight: bold">for</span> epoch <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #B452CD">1</span>,n_epochs+<span style="color: #B452CD">1</span>):
    <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(m):
        k = np.random.randint(m) <span style="color: #228B22">#Pick the k-th minibatch at random</span>
        <span style="color: #228B22">#Compute the gradient using the data in minibatch Bk</span>
        <span style="color: #228B22">#Compute new suggestion for beta</span>
        t = epoch*m+i
        gamma_j = step_length(t,t0,t1)
        j += <span style="color: #B452CD">1</span>

<span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;gamma_j after %d epochs: %g&quot;</span> % (n_epochs,gamma_j))
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
<h2 id="program-for-stochastic-gradient">Program for stochastic gradient </h2>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #228B22"># Importing various packages</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">math</span> <span style="color: #8B008B; font-weight: bold">import</span> exp, sqrt
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">random</span> <span style="color: #8B008B; font-weight: bold">import</span> random, seed
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sklearn.linear_model</span> <span style="color: #8B008B; font-weight: bold">import</span> SGDRegressor

x = <span style="color: #B452CD">2</span>*np.random.rand(<span style="color: #B452CD">100</span>,<span style="color: #B452CD">1</span>)
y = <span style="color: #B452CD">4</span>+<span style="color: #B452CD">3</span>*x+np.random.randn(<span style="color: #B452CD">100</span>,<span style="color: #B452CD">1</span>)

xb = np.c_[np.ones((<span style="color: #B452CD">100</span>,<span style="color: #B452CD">1</span>)), x]
theta_linreg = np.linalg.inv(xb.T.dot(xb)).dot(xb.T).dot(y)
<span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;Own inversion&quot;</span>)
<span style="color: #658b00">print</span>(theta_linreg)
sgdreg = SGDRegressor(n_iter = <span style="color: #B452CD">50</span>, penalty=<span style="color: #8B008B; font-weight: bold">None</span>, eta0=<span style="color: #B452CD">0.1</span>)
sgdreg.fit(x,y.ravel())
<span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;sgdreg from scikit&quot;</span>)
<span style="color: #658b00">print</span>(sgdreg.intercept_, sgdreg.coef_)


theta = np.random.randn(<span style="color: #B452CD">2</span>,<span style="color: #B452CD">1</span>)

eta = <span style="color: #B452CD">0.1</span>
Niterations = <span style="color: #B452CD">1000</span>
m = <span style="color: #B452CD">100</span>

<span style="color: #8B008B; font-weight: bold">for</span> <span style="color: #658b00">iter</span> <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Niterations):
    gradients = <span style="color: #B452CD">2.0</span>/m*xb.T.dot(xb.dot(theta)-y)
    theta -= eta*gradients
<span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;theta frm own gd&quot;</span>)
<span style="color: #658b00">print</span>(theta)

xnew = np.array([[<span style="color: #B452CD">0</span>],[<span style="color: #B452CD">2</span>]])
xbnew = np.c_[np.ones((<span style="color: #B452CD">2</span>,<span style="color: #B452CD">1</span>)), xnew]
ypredict = xbnew.dot(theta)
ypredict2 = xbnew.dot(theta_linreg)


n_epochs = <span style="color: #B452CD">50</span>
t0, t1 = <span style="color: #B452CD">5</span>, <span style="color: #B452CD">50</span>
m = <span style="color: #B452CD">100</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">learning_schedule</span>(t):
    <span style="color: #8B008B; font-weight: bold">return</span> t0/(t+t1)

theta = np.random.randn(<span style="color: #B452CD">2</span>,<span style="color: #B452CD">1</span>)

<span style="color: #8B008B; font-weight: bold">for</span> epoch <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n_epochs):
    <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(m):
        random_index = np.random.randint(m)
        xi = xb[random_index:random_index+<span style="color: #B452CD">1</span>]
        yi = y[random_index:random_index+<span style="color: #B452CD">1</span>]
        gradients = <span style="color: #B452CD">2</span> * xi.T.dot(xi.dot(theta)-yi)
        eta = learning_schedule(epoch*m+i)
        theta = theta - eta*gradients
<span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;theta from own sdg&quot;</span>)
<span style="color: #658b00">print</span>(theta)


plt.plot(xnew, ypredict, <span style="color: #CD5555">&quot;r-&quot;</span>)
plt.plot(xnew, ypredict2, <span style="color: #CD5555">&quot;b-&quot;</span>)
plt.plot(x, y ,<span style="color: #CD5555">&#39;ro&#39;</span>)
plt.axis([<span style="color: #B452CD">0</span>,<span style="color: #B452CD">2.0</span>,<span style="color: #B452CD">0</span>, <span style="color: #B452CD">15.0</span>])
plt.xlabel(<span style="color: #CD5555">r&#39;$x$&#39;</span>)
plt.ylabel(<span style="color: #CD5555">r&#39;$y$&#39;</span>)
plt.title(<span style="color: #CD5555">r&#39;Random numbers &#39;</span>)
plt.show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
<h2 id="using-gradient-descent-methods-limitations">Using gradient descent methods, limitations </h2>

<ul>
<li> <b>Gradient descent (GD) finds local minima of our function</b>. Since the GD algorithm is deterministic, if it converges, it will converge to a local minimum of our energy function. Because in ML we are often dealing with extremely rugged landscapes with many local minima, this can lead to poor performance.</li>
<li> <b>GD is sensitive to initial conditions</b>. One consequence of the local nature of GD is that initial conditions matter. Depending on where one starts, one will end up at a different local minima. Therefore, it is very important to think about how one initializes the training process. This is true for GD as well as more complicated variants of GD.</li>
<li> <b>Gradients are computationally expensive to calculate for large datasets</b>. In many cases in statistics and ML, the energy function is a sum of terms, with one term for each data point. For example, in linear regression, \( E \propto \sum_{i=1}^n (y_i - \mathbf{w}^T\cdot\mathbf{x}_i)^2 \); for logistic regression, the square error is replaced by the cross entropy. To calculate the gradient we have to sum over <em>all</em> \( n \) data points. Doing this at every GD step becomes extremely computationally expensive. An ingenious solution to this, is to calculate the gradients using small subsets of the data called &quot;mini batches&quot;. This has the added benefit of introducing stochasticity into our algorithm.</li>
<li> <b>GD is very sensitive to choices of learning rates</b>. GD is extremely sensitive to the choice of learning rates. If the learning rate is very small, the training process take an extremely long time. For larger learning rates, GD can diverge and give poor results. Furthermore, depending on what the local landscape looks like, we have to modify the learning rates to ensure convergence. Ideally, we would <em>adaptively</em> choose the learning rates to match the landscape.</li>
<li> <b>GD treats all directions in parameter space uniformly.</b> Another major drawback of GD is that unlike Newton's method, the learning rate for GD is the same in all directions in parameter space. For this reason, the maximum learning rate is set by the behavior of the steepest direction and this can significantly slow down training. Ideally, we would like to take large steps in flat directions and small steps in steep directions. Since we are exploring rugged landscapes where curvatures change, this requires us to keep track of not only the gradient but second derivatives. The ideal scenario would be to calculate the Hessian but this proves to be too computationally expensive.</li> 
<li> GD can take exponential time to escape saddle points, even with random initialization. As we mentioned, GD is extremely sensitive to initial condition since it determines the particular local minimum GD would eventually reach. However, even with a good initialization scheme, through the introduction of randomness, GD can still take exponential time to escape saddle points.</li>
</ul>
<h2 id="codes-from-numerical-recipes">Codes from numerical recipes </h2>

<p>You can however use codes we have adapted from the text <a href="http://www.nr.com/" target="_blank">Numerical Recipes in C++</a>, see chapter 10.7.  
Here we present a program, which you also can find at the webpage of the course we use the functions <b>dfpmin</b> and <b>lnsrch</b>.  This is a variant of the Broyden et al algorithm discussed in the previous slide.
</p>

<ul>
<li> The program uses the harmonic oscillator in one dimensions as example.</li>
<li> The program does not use armadillo to handle vectors and matrices, but employs rather my own vector-matrix class. These auxiliary functions, and the main program <em>model.cpp</em> can all be found under the <a href="https://github.com/CompPhysics/ComputationalPhysics2/tree/gh-pages/doc/pub/cg/programs/c%2B%2B" target="_blank">program link here</a>.</li>
</ul>
<p>Below we show only excerpts from the main program. For the full program, see the above link.</p>
<h2 id="finding-the-minimum-of-the-harmonic-oscillator-model-in-one-dimension">Finding the minimum of the harmonic oscillator model in one dimension </h2>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;">//   Main function begins here
<span style="color: #658b00">int</span> main()
{
     <span style="color: #658b00">int</span> n, <span style="color: #658b00">iter</span>;
     double gtol, fret;
     double alpha;
     n = <span style="color: #B452CD">1</span>;
//   reserve space <span style="color: #8B008B">in</span> memory <span style="color: #8B008B; font-weight: bold">for</span> vectors containing the variational
//   parameters
     Vector g(n), p(n);
     cout &lt;&lt; <span style="color: #CD5555">&quot;Read in guess for alpha&quot;</span> &lt;&lt; endl;
     cin &gt;&gt; alpha;
     gtol = <span style="color: #B452CD">1.0e-5</span>;
//   now call dfmin <span style="color: #8B008B">and</span> compute the minimum
     p(<span style="color: #B452CD">0</span>) = alpha;
     dfpmin(p, n, gtol, &amp;<span style="color: #658b00">iter</span>, &amp;fret, Efunction, dEfunction);
     cout &lt;&lt; <span style="color: #CD5555">&quot;Value of energy minimum = &quot;</span> &lt;&lt; fret &lt;&lt; endl;
     cout &lt;&lt; <span style="color: #CD5555">&quot;Number of iterations = &quot;</span> &lt;&lt; <span style="color: #658b00">iter</span> &lt;&lt; endl;
     cout &lt;&lt; <span style="color: #CD5555">&quot;Value of alpha at minimum = &quot;</span> &lt;&lt; p(<span style="color: #B452CD">0</span>) &lt;&lt; endl;
      <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">0</span>;
}  // end of main program
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
<h2 id="functions-to-observe">Functions to observe </h2>

<p>The functions <b>Efunction</b> and <b>dEfunction</b> compute the expectation value of the energy and its derivative.
They use the the quasi-Newton method of <a href="https://www.springer.com/it/book/9780387303031" target="_blank">Broyden, Fletcher, Goldfarb, and Shanno (BFGS)</a>
It uses the first derivatives only. The BFGS algorithm has proven good performance even for non-smooth optimizations. 
These functions need to be changed when you want to your own derivatives.
</p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;">//  this function defines the expectation value of the local energy
double Efunction(Vector  &amp;x)
{
  double value = x(<span style="color: #B452CD">0</span>)*x(<span style="color: #B452CD">0</span>)*<span style="color: #B452CD">0.5</span>+<span style="color: #B452CD">1.0</span>/(<span style="color: #B452CD">8</span>*x(<span style="color: #B452CD">0</span>)*x(<span style="color: #B452CD">0</span>));
  <span style="color: #8B008B; font-weight: bold">return</span> value;
} // end of function to evaluate

//  this function defines the derivative of the energy 
void dEfunction(Vector &amp;x, Vector &amp;g)
{
  g(<span style="color: #B452CD">0</span>) = x(<span style="color: #B452CD">0</span>)-<span style="color: #B452CD">1.0</span>/(<span style="color: #B452CD">4</span>*x(<span style="color: #B452CD">0</span>)*x(<span style="color: #B452CD">0</span>)*x(<span style="color: #B452CD">0</span>));
} // end of function to evaluate
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>

<p>You need to change these functions in order to compute the local energy for your system. I used 1000
cycles per call to get a new value of \( \langle E_L[\alpha]\rangle \).
When I compute the local energy I also compute its derivative.
After roughly 10-20 iterations I got a converged result in terms of \( \alpha \).
</p>
<h1 id="resampling-techniques-bootstrap-and-blocking">Resampling Techniques, Bootstrap and Blocking </h1>
<h2 id="why-resampling-methods">Why resampling methods ? </h2>
<p> Statistical analysis</p>
<ul>
    <li> Our simulations can be treated as <em>computer experiments</em>. This is particularly the case for Monte Carlo methods</li>
    <li> The results can be analysed with the same statistical tools as we would use analysing experimental data.</li>
    <li> As in all experiments, we are looking for expectation values and an estimate of how accurate they are, i.e., possible sources for errors.</li>
</ul>
<h2 id="statistical-analysis">Statistical analysis </h2>

<ul>
    <li> As in other experiments, many numerical  experiments have two classes of errors:</li>
<ul>
      <li> Statistical errors</li>
      <li> Systematical errors</li>
</ul>
    <li> Statistical errors can be estimated using standard tools from statistics</li>
    <li> Systematical errors are method specific and must be treated differently from case to case.</li> 
</ul>
<h2 id="statistics-wrapping-up-from-last-week">Statistics, wrapping up from last week </h2>

<p>Let us analyze the problem by splitting up the correlation term into
partial sums of the form:
</p>
$$
f_d = \frac{1}{n-d}\sum_{k=1}^{n-d}(x_k - \bar x_n)(x_{k+d} - \bar x_n)
$$

<p>The correlation term of the error can now be rewritten in terms of
\( f_d \)
</p>
$$
\frac{2}{n}\sum_{k < l} (x_k - \bar x_n)(x_l - \bar x_n) =
2\sum_{d=1}^{n-1} f_d
$$

<p>The value of \( f_d \) reflects the correlation between measurements
separated by the distance \( d \) in the sample samples.  Notice that for
\( d=0 \), \( f \) is just the sample variance, \( \mathrm{var}(x) \). If we divide \( f_d \)
by \( \mathrm{var}(x) \), we arrive at the so called <em>autocorrelation function</em>
</p>
$$
\kappa_d = \frac{f_d}{\mathrm{var}(x)}
$$

<p>which gives us a useful measure of pairwise correlations
starting always at \( 1 \) for \( d=0 \).
</p>
<h2 id="statistics-final-expression">Statistics, final expression </h2>

<p>The sample error can now be
written in terms of the autocorrelation function:
</p>

$$
\begin{align}
\mathrm{err}_X^2 &=
\frac{1}{n}\mathrm{var}(x)+\frac{2}{n}\cdot\mathrm{var}(x)\sum_{d=1}^{n-1}
\frac{f_d}{\mathrm{var}(x)}\nonumber\\ &=&
\left(1+2\sum_{d=1}^{n-1}\kappa_d\right)\frac{1}{n}\mathrm{var}(x)\nonumber\\
&=\frac{\tau}{n}\cdot\mathrm{var}(x)
\label{_auto9}
\end{align}

$$

<p>and we see that \( \mathrm{err}_X \) can be expressed in terms the
uncorrelated sample variance times a correction factor \( \tau \) which
accounts for the correlation between measurements. We call this
correction factor the <em>autocorrelation time</em>:
</p>
$$
\begin{equation}
\tau = 1+2\sum_{d=1}^{n-1}\kappa_d
\label{eq:autocorrelation_time}
\end{equation}
$$
<h2 id="statistics-effective-number-of-correlations">Statistics, effective number of correlations </h2>

<p>For a correlation free experiment, \( \tau \)
equals 1.
</p>

<p>We can interpret a sequential
correlation as an effective reduction of the number of measurements by
a factor \( \tau \). The effective number of measurements becomes:
</p>
$$
n_\mathrm{eff} = \frac{n}{\tau}
$$

<p>To neglect the autocorrelation time \( \tau \) will always cause our
simple uncorrelated estimate of \( \mathrm{err}_X^2\approx \mathrm{var}(x)/n \) to
be less than the true sample error. The estimate of the error will be
too <em>good</em>. On the other hand, the calculation of the full
autocorrelation time poses an efficiency problem if the set of
measurements is very large.
</p>
<h2 id="can-we-understand-this-time-auto-correlation-function">Can we understand this? Time Auto-correlation Function </h2>

<p>The so-called time-displacement autocorrelation \( \phi(t) \) for a quantity \( \mathbf{M} \) is given by</p>
$$
\phi(t) = \int dt' \left[\mathbf{M}(t')-\langle \mathbf{M} \rangle\right]\left[\mathbf{M}(t'+t)-\langle \mathbf{M} \rangle\right],
$$

<p>which can be rewritten as </p>
$$
\phi(t) = \int dt' \left[\mathbf{M}(t')\mathbf{M}(t'+t)-\langle \mathbf{M} \rangle^2\right],
$$

<p>where \( \langle \mathbf{M} \rangle \) is the average value and
\( \mathbf{M}(t) \) its instantaneous value. We can discretize this function as follows, where we used our
set of computed values \( \mathbf{M}(t) \) for a set of discretized times (our Monte Carlo cycles corresponding to moving all electrons?)
</p>
$$
\phi(t)  = \frac{1}{t_{\mathrm{max}}-t}\sum_{t'=0}^{t_{\mathrm{max}}-t}\mathbf{M}(t')\mathbf{M}(t'+t)
-\frac{1}{t_{\mathrm{max}}-t}\sum_{t'=0}^{t_{\mathrm{max}}-t}\mathbf{M}(t')\times
\frac{1}{t_{\mathrm{max}}-t}\sum_{t'=0}^{t_{\mathrm{max}}-t}\mathbf{M}(t'+t).
\label{eq:phitf}
$$
<h2 id="time-auto-correlation-function">Time Auto-correlation Function </h2>

<p>One should be careful with times close to \( t_{\mathrm{max}} \), the upper limit of the sums 
becomes small and we end up integrating over a rather small time interval. This means that the statistical
error in \( \phi(t) \) due to the random nature of the fluctuations in \( \mathbf{M}(t) \) can become large.
</p>

<p>One should therefore choose \( t \ll t_{\mathrm{max}} \).</p>

<p>Note that the variable \( \mathbf{M} \) can be any expectation values of interest.</p>

<p>The time-correlation function gives a measure of the correlation between the various values of the variable 
at a time \( t' \) and a time \( t'+t \). If we multiply the values of \( \mathbf{M} \) at these two different times,
we will get a positive contribution if they are fluctuating in the same direction, or a negative value
if they fluctuate in the opposite direction. If we then integrate over time, or use the discretized version of, the time correlation function \( \phi(t) \) should take a non-zero value if the fluctuations are 
correlated, else it should gradually go to zero. For times a long way apart 
the different values of \( \mathbf{M} \)  are most likely 
uncorrelated and \( \phi(t) \) should be zero.
</p>
<h2 id="time-auto-correlation-function">Time Auto-correlation Function </h2>

<p>We can derive the correlation time by observing that our Metropolis algorithm is based on a random
walk in the space of all  possible spin configurations. 
Our probability 
distribution function \( \mathbf{\hat{w}}(t) \) after a given number of time steps \( t \) could be written as
</p>
$$
   \mathbf{\hat{w}}(t) = \mathbf{\hat{W}^t\hat{w}}(0),
$$

<p>with \( \mathbf{\hat{w}}(0) \) the distribution at \( t=0 \) and \( \mathbf{\hat{W}} \) representing the 
transition probability matrix. 
We can always expand \( \mathbf{\hat{w}}(0) \) in terms of the right eigenvectors of 
\( \mathbf{\hat{v}} \) of \( \mathbf{\hat{W}} \) as 
</p>
$$
    \mathbf{\hat{w}}(0)  = \sum_i\alpha_i\mathbf{\hat{v}}_i,
$$

<p>resulting in </p>
$$
   \mathbf{\hat{w}}(t) = \mathbf{\hat{W}}^t\mathbf{\hat{w}}(0)=\mathbf{\hat{W}}^t\sum_i\alpha_i\mathbf{\hat{v}}_i=
\sum_i\lambda_i^t\alpha_i\mathbf{\hat{v}}_i,
$$

<p>with \( \lambda_i \) the \( i^{\mathrm{th}} \) eigenvalue corresponding to  
the eigenvector \( \mathbf{\hat{v}}_i \). 
</p>
<h2 id="time-auto-correlation-function">Time Auto-correlation Function </h2>

<p>If we assume that \( \lambda_0 \) is the largest eigenvector we see that in the limit \( t\rightarrow \infty \),
\( \mathbf{\hat{w}}(t) \) becomes proportional to the corresponding eigenvector 
\( \mathbf{\hat{v}}_0 \). This is our steady state or final distribution. 
</p>

<p>We can relate this property to an observable like the mean energy.
With the probabilty \( \mathbf{\hat{w}}(t) \) (which in our case is the squared trial wave function) we
can write the expectation values as 
</p>
$$
 \langle \mathbf{M}(t) \rangle  = \sum_{\mu} \mathbf{\hat{w}}(t)_{\mu}\mathbf{M}_{\mu},
$$

<p>or as the scalar of a  vector product</p>
$$
 \langle \mathbf{M}(t) \rangle  = \mathbf{\hat{w}}(t)\mathbf{m},
$$

<p>with \( \mathbf{m} \) being the vector whose elements are the values of \( \mathbf{M}_{\mu} \) in its 
various microstates \( \mu \).
</p>
<h2 id="time-auto-correlation-function">Time Auto-correlation Function </h2>

<p>We rewrite this relation  as</p>
$$
 \langle \mathbf{M}(t) \rangle  = \mathbf{\hat{w}}(t)\mathbf{m}=\sum_i\lambda_i^t\alpha_i\mathbf{\hat{v}}_i\mathbf{m}_i.
$$

<p>If we define \( m_i=\mathbf{\hat{v}}_i\mathbf{m}_i \) as the expectation value of
\( \mathbf{M} \) in the \( i^{\mathrm{th}} \) eigenstate we can rewrite the last equation as
</p>
$$
 \langle \mathbf{M}(t) \rangle  = \sum_i\lambda_i^t\alpha_im_i.
$$

<p>Since we have that in the limit \( t\rightarrow \infty \) the mean value is dominated by the 
the largest eigenvalue \( \lambda_0 \), we can rewrite the last equation as
</p>
$$
 \langle \mathbf{M}(t) \rangle  = \langle \mathbf{M}(\infty) \rangle+\sum_{i\ne 0}\lambda_i^t\alpha_im_i.
$$

<p>We define the quantity</p>
$$
   \tau_i=-\frac{1}{log\lambda_i},
$$

<p>and rewrite the last expectation value as</p>
$$
 \langle \mathbf{M}(t) \rangle  = \langle \mathbf{M}(\infty) \rangle+\sum_{i\ne 0}\alpha_im_ie^{-t/\tau_i}.
\label{eq:finalmeanm}
$$
<h2 id="time-auto-correlation-function">Time Auto-correlation Function </h2>

<p>The quantities \( \tau_i \) are the correlation times for the system. They control also the auto-correlation function 
discussed above.  The longest correlation time is obviously given by the second largest
eigenvalue \( \tau_1 \), which normally defines the correlation time discussed above. For large times, this is the 
only correlation time that survives. If higher eigenvalues of the transition matrix are well separated from 
\( \lambda_1 \) and we simulate long enough,  \( \tau_1 \) may well define the correlation time. 
In other cases we may not be able to extract a reliable result for \( \tau_1 \). 
Coming back to the time correlation function \( \phi(t) \) we can present a more general definition in terms
of the mean magnetizations $ \langle \mathbf{M}(t) \rangle$. Recalling that the mean value is equal 
to $ \langle \mathbf{M}(\infty) \rangle$ we arrive at the expectation values
</p>
$$
\phi(t) =\langle \mathbf{M}(0)-\mathbf{M}(\infty)\rangle \langle \mathbf{M}(t)-\mathbf{M}(\infty)\rangle,
$$

<p>resulting in</p>
$$
\phi(t) =\sum_{i,j\ne 0}m_i\alpha_im_j\alpha_je^{-t/\tau_i},
$$

<p>which is appropriate for all times.</p>
<h2 id="correlation-time">Correlation Time </h2>

<p>If the correlation function decays exponentially</p>
$$ \phi (t) \sim \exp{(-t/\tau)}$$

<p>then the exponential correlation time can be computed as the average</p>
$$   \tau_{\mathrm{exp}}  =  -\langle  \frac{t}{log|\frac{\phi(t)}{\phi(0)}|} \rangle. $$

<p>If the decay is exponential, then</p>
$$  \int_0^{\infty} dt \phi(t)  = \int_0^{\infty} dt \phi(0)\exp{(-t/\tau)}  = \tau \phi(0),$$

<p>which  suggests another measure of correlation</p>
$$   \tau_{\mathrm{int}} = \sum_k \frac{\phi(k)}{\phi(0)}, $$

<p>called the integrated correlation time.</p>
<h2 id="resampling-methods-jackknife-and-bootstrap">Resampling methods: Jackknife and Bootstrap </h2>

<p>Two famous
resampling methods are the <b>independent bootstrap</b> and <b>the jackknife</b>. 
</p>

<p>The jackknife is a special case of the independent bootstrap. Still, the jackknife was made
popular prior to the independent bootstrap. And as the popularity of
the independent bootstrap soared, new variants, such as <b>the dependent bootstrap</b>.
</p>

<p>The Jackknife and independent bootstrap work for
independent, identically distributed random variables.
If these conditions are not
satisfied, the methods will fail.  Yet, it should be said that if the data are
independent, identically distributed, and we only want to estimate the
variance of \( \overline{X} \) (which often is the case), then there is no
need for bootstrapping. 
</p>
<h2 id="resampling-methods-jackknife">Resampling methods: Jackknife </h2>

<p>The Jackknife works by making many replicas of the estimator \( \widehat{\theta} \). 
The jackknife is a resampling method, we explained that this happens by scrambling the data in some way. When using the jackknife, this is done by systematically leaving out one observation from the vector of observed values \( \hat{x} = (x_1,x_2,\cdots,X_n) \). 
Let \( \hat{x}_i \) denote the vector
</p>
$$
\hat{x}_i = (x_1,x_2,\cdots,x_{i-1},x_{i+1},\cdots,x_n),
$$

<p>which equals the vector \( \hat{x} \) with the exception that observation
number \( i \) is left out. Using this notation, define
\( \widehat{\theta}_i \) to be the estimator
\( \widehat{\theta} \) computed using \( \vec{X}_i \). 
</p>
<h2 id="resampling-methods-jackknife-estimator">Resampling methods: Jackknife estimator </h2>

<p>To get an estimate for the bias and
standard error of \( \widehat{\theta} \), use the following
estimators for each component of \( \widehat{\theta} \)
</p>

$$
\widehat{\mathrm{Bias}}(\widehat \theta,\theta) = (n-1)\left( - \widehat{\theta} + \frac{1}{n}\sum_{i=1}^{n} \widehat \theta_i \right) \qquad \text{and} \qquad \widehat{\sigma}^2_{\widehat{\theta} } = \frac{n-1}{n}\sum_{i=1}^{n}( \widehat{\theta}_i - \frac{1}{n}\sum_{j=1}^{n}\widehat \theta_j )^2.
$$
<h2 id="jackknife-code-example">Jackknife code example </h2>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">import</span> *
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">numpy.random</span> <span style="color: #8B008B; font-weight: bold">import</span> randint, randn
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">time</span> <span style="color: #8B008B; font-weight: bold">import</span> time

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">jackknife</span>(data, stat):
    n = <span style="color: #658b00">len</span>(data);t = zeros(n); inds = arange(n); t0 = time()
    <span style="color: #228B22">## &#39;jackknifing&#39; by leaving out an observation for each i                                                                                                                      </span>
    <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(n):
        t[i] = stat(delete(data,i) )

    <span style="color: #228B22"># analysis                                                                                                                                                                     </span>
    <span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;Runtime: %g sec&quot;</span> % (time()-t0)); <span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;Jackknife Statistics :&quot;</span>)
    <span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;original           bias      std. error&quot;</span>)
    <span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;%8g %14g %15g&quot;</span> % (stat(data),(n-<span style="color: #B452CD">1</span>)*mean(t)/n, (n*var(t))**<span style="color: #B452CD">.5</span>))

    <span style="color: #8B008B; font-weight: bold">return</span> t


<span style="color: #228B22"># Returns mean of data samples                                                                                                                                                     </span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">stat</span>(data):
    <span style="color: #8B008B; font-weight: bold">return</span> mean(data)


mu, sigma = <span style="color: #B452CD">100</span>, <span style="color: #B452CD">15</span>
datapoints = <span style="color: #B452CD">10000</span>
x = mu + sigma*random.randn(datapoints)
<span style="color: #228B22"># jackknife returns the data sample                                                                                                                                                </span>
t = jackknife(x, stat)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
<h2 id="resampling-methods-bootstrap">Resampling methods: Bootstrap </h2>

<p>Bootstrapping is a nonparametric approach to statistical inference
that substitutes computation for more traditional distributional
assumptions and asymptotic results. Bootstrapping offers a number of
advantages: 
</p>
<ol>
<li> The bootstrap is quite general, although there are some cases in which it fails.</li>  
<li> Because it does not require distributional assumptions (such as normally distributed errors), the bootstrap can provide more accurate inferences when the data are not well behaved or when the sample size is small.</li>  
<li> It is possible to apply the bootstrap to statistics with sampling distributions that are difficult to derive, even asymptotically.</li> 
<li> It is relatively simple to apply the bootstrap to complex data-collection plans (such as stratified and clustered samples).</li>
</ol>
<h2 id="resampling-methods-bootstrap-background">Resampling methods: Bootstrap background </h2>

<p>Since \( \widehat{\theta} = \widehat{\theta}(\hat{X}) \) is a function of random variables,
\( \widehat{\theta} \) itself must be a random variable. Thus it has
a pdf, call this function \( p(\hat{t}) \). The aim of the bootstrap is to
estimate \( p(\hat{t}) \) by the relative frequency of
\( \widehat{\theta} \). You can think of this as using a histogram
in the place of \( p(\hat{t}) \). If the relative frequency closely
resembles \( p(\vec{t}) \), then using numerics, it is straight forward to
estimate all the interesting parameters of \( p(\hat{t}) \) using point
estimators.  
</p>
<h2 id="resampling-methods-more-bootstrap-background">Resampling methods: More Bootstrap background </h2>

<p>In the case that \( \widehat{\theta} \) has
more than one component, and the components are independent, we use the
same estimator on each component separately.  If the probability
density function of \( X_i \), \( p(x) \), had been known, then it would have
been straight forward to do this by: 
</p>
<ol>
<li> Drawing lots of numbers from \( p(x) \), suppose we call one such set of numbers \( (X_1^*, X_2^*, \cdots, X_n^*) \).</li> 
<li> Then using these numbers, we could compute a replica of \( \widehat{\theta} \) called \( \widehat{\theta}^* \).</li> 
</ol>
<p>By repeated use of (1) and (2), many
estimates of \( \widehat{\theta} \) could have been obtained. The
idea is to use the relative frequency of \( \widehat{\theta}^* \)
(think of a histogram) as an estimate of \( p(\hat{t}) \).
</p>
<h2 id="resampling-methods-bootstrap-approach">Resampling methods: Bootstrap approach </h2>

<p>But
unless there is enough information available about the process that
generated \( X_1,X_2,\cdots,X_n \), \( p(x) \) is in general
unknown. Therefore, <a href="https://projecteuclid.org/euclid.aos/1176344552" target="_blank">Efron in 1979</a>  asked the
question: What if we replace \( p(x) \) by the relative frequency
of the observation \( X_i \); if we draw observations in accordance with
the relative frequency of the observations, will we obtain the same
result in some asymptotic sense? The answer is yes.
</p>

<p>Instead of generating the histogram for the relative
frequency of the observation \( X_i \), just draw the values
\( (X_1^*,X_2^*,\cdots,X_n^*) \) with replacement from the vector
\( \hat{X} \). 
</p>
<h2 id="resampling-methods-bootstrap-steps">Resampling methods: Bootstrap steps </h2>

<p>The independent bootstrap works like this: </p>

<ol>
<li> Draw with replacement \( n \) numbers for the observed variables \( \hat{x} = (x_1,x_2,\cdots,x_n) \).</li> 
<li> Define a vector \( \hat{x}^* \) containing the values which were drawn from \( \hat{x} \).</li> 
<li> Using the vector \( \hat{x}^* \) compute \( \widehat{\theta}^* \) by evaluating \( \widehat \theta \) under the observations \( \hat{x}^* \).</li> 
<li> Repeat this process \( k \) times.</li> 
</ol>
<p>When you are done, you can draw a histogram of the relative frequency of \( \widehat \theta^* \). This is your estimate of the probability distribution \( p(t) \). Using this probability distribution you can estimate any statistics thereof. In principle you never draw the histogram of the relative frequency of \( \widehat{\theta}^* \). Instead you use the estimators corresponding to the statistic of interest. For example, if you are interested in estimating the variance of \( \widehat \theta \), apply the etsimator \( \widehat \sigma^2 \) to the values \( \widehat \theta ^* \).</p>
<h2 id="code-example-for-the-bootstrap-method">Code example for the Bootstrap method </h2>

<p>The following code starts with a Gaussian distribution with mean value \( \mu =100 \) and variance \( \sigma=15 \). We use this to generate the data used in the bootstrap analysis. The bootstrap analysis returns a data set after a given number of bootstrap operations (as many as we have data points). This data set consists of estimated mean values for each bootstrap operation. The histogram generated by the bootstrap method shows that the distribution for these mean values is also a Gaussian, centered around the mean value \( \mu=100 \) but with standard deviation \( \sigma/\sqrt{n} \), where \( n \) is the number of bootstrap samples (in this case the same as the number of original data points). The value of the standard deviation is what we expect from the central limit theorem. </p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;">%matplotlib inline

<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">import</span> *
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">numpy.random</span> <span style="color: #8B008B; font-weight: bold">import</span> randint, randn
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">time</span> <span style="color: #8B008B; font-weight: bold">import</span> time
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">scipy.stats</span> <span style="color: #8B008B; font-weight: bold">import</span> norm
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>

<span style="color: #228B22"># Returns mean of bootstrap samples                                                                                                                                                </span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">stat</span>(data):
    <span style="color: #8B008B; font-weight: bold">return</span> mean(data)

<span style="color: #228B22"># Bootstrap algorithm                                                                                                                                                              </span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">bootstrap</span>(data, statistic, R):
    t = zeros(R); n = <span style="color: #658b00">len</span>(data); inds = arange(n); t0 = time()

    <span style="color: #228B22"># non-parametric bootstrap                                                                                                                                                     </span>
    <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(R):
        t[i] = statistic(data[randint(<span style="color: #B452CD">0</span>,n,n)])

    <span style="color: #228B22"># analysis                                                                                                                                                                     </span>
    <span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;Runtime: %g sec&quot;</span> % (time()-t0)); <span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;Bootstrap Statistics :&quot;</span>)
    <span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;original           bias      std. error&quot;</span>)
    <span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;%8g %8g %14g %15g&quot;</span> % (statistic(data), std(data),\
                             mean(t), \
                             std(t)))
    <span style="color: #8B008B; font-weight: bold">return</span> t


mu, sigma = <span style="color: #B452CD">100</span>, <span style="color: #B452CD">15</span>
datapoints = <span style="color: #B452CD">10000</span>
x = mu + sigma*random.randn(datapoints)
<span style="color: #228B22"># bootstrap returns the data sample                                                                                                          t = bootstrap(x, stat, datapoints)</span>
<span style="color: #228B22"># the histogram of the bootstrapped  data  </span>
t = bootstrap(x, stat, datapoints)
<span style="color: #228B22"># the histogram of the bootstrapped  data                                            </span>
n, binsboot, patches = plt.hist(t, bins=<span style="color: #B452CD">50</span>, density=<span style="color: #CD5555">&#39;true&#39;</span>,histtype=<span style="color: #CD5555">&#39;bar&#39;</span>, color=<span style="color: #CD5555">&#39;red&#39;</span>, alpha=<span style="color: #B452CD">0.75</span>)

<span style="color: #228B22"># add a &#39;best fit&#39; line                                                                                                                                                          </span>
y = norm.pdf( binsboot, mean(t), std(t))
lt = plt.plot(binsboot, y, <span style="color: #CD5555">&#39;r--&#39;</span>, linewidth=<span style="color: #B452CD">1</span>)
plt.xlabel(<span style="color: #CD5555">&#39;Smarts&#39;</span>)
plt.ylabel(<span style="color: #CD5555">&#39;Probability&#39;</span>)
plt.axis([<span style="color: #B452CD">99.5</span>, <span style="color: #B452CD">100.6</span>, <span style="color: #B452CD">0</span>, <span style="color: #B452CD">3.0</span>])
plt.grid(<span style="color: #8B008B; font-weight: bold">True</span>)

plt.show()
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>
<h2 id="resampling-methods-blocking">Resampling methods: Blocking  </h2>

<p>The blocking method was made popular by <a href="https://aip.scitation.org/doi/10.1063/1.457480" target="_blank">Flyvbjerg and Pedersen (1989)</a>
and has become one of the standard ways to estimate
\( V(\widehat{\theta}) \) for exactly one \( \widehat{\theta} \), namely
\( \widehat{\theta} = \overline{X} \). 
</p>

<p>Assume \( n = 2^d \) for some integer \( d>1 \) and \( X_1,X_2,\cdots, X_n \) is a stationary time series to begin with. 
Moreover, assume that the time series is asymptotically uncorrelated. We switch to vector notation by arranging \( X_1,X_2,\cdots,X_n \) in an \( n \)-tuple. Define:
</p>
$$
\begin{align*}
\hat{X} = (X_1,X_2,\cdots,X_n).
\end{align*}
$$

<p>The strength of the blocking method is when the number of
observations, \( n \) is large. For large \( n \), the complexity of dependent
bootstrapping scales poorly, but the blocking method does not,
moreover, it becomes more accurate the larger \( n \) is.
</p>
<h2 id="blocking-transformations">Blocking Transformations </h2>
<p> We now define
blocking transformations. The idea is to take the mean of subsequent
pair of elements from \( \vec{X} \) and form a new vector
\( \vec{X}_1 \). Continuing in the same way by taking the mean of
subsequent pairs of elements of \( \vec{X}_1 \) we obtain \( \vec{X}_2 \), and
so on. 
Define \( \vec{X}_i \) recursively by:
</p>

$$
\begin{align} 
(\vec{X}_0)_k &\equiv (\vec{X})_k \nonumber \\
(\vec{X}_{i+1})_k &\equiv \frac{1}{2}\Big( (\vec{X}_i)_{2k-1} +
(\vec{X}_i)_{2k} \Big) \qquad \text{for all} \qquad 1 \leq i \leq d-1
\label{_auto10}
\end{align} 
$$

<p>The quantity \( \vec{X}_k \) is
subject to \( k \) <b>blocking transformations</b>.  We now have \( d \) vectors
\( \vec{X}_0, \vec{X}_1,\cdots,\vec X_{d-1} \) containing the subsequent
averages of observations. It turns out that if the components of
\( \vec{X} \) is a stationary time series, then the components of
\( \vec{X}_i \) is a stationary time series for all \( 0 \leq i \leq d-1 \)
</p>

<p>We can then compute the autocovariance, the variance, sample mean, and
number of observations for each \( i \). 
Let \( \gamma_i, \sigma_i^2,
\overline{X}_i \) denote the autocovariance, variance and average of the
elements of \( \vec{X}_i \) and let \( n_i \) be the number of elements of
\( \vec{X}_i \). It follows by induction that \( n_i = n/2^i \). 
</p>
<h2 id="blocking-transformations">Blocking Transformations </h2>

<p>Using the
definition of the blocking transformation and the distributive
property of the covariance, it is clear that since \( h =|i-j| \)
we can define
</p>
$$
\begin{align}
\gamma_{k+1}(h) &= cov\left( ({X}_{k+1})_{i}, ({X}_{k+1})_{j} \right) \nonumber \\
&=  \frac{1}{4}cov\left( ({X}_{k})_{2i-1} + ({X}_{k})_{2i}, ({X}_{k})_{2j-1} + ({X}_{k})_{2j} \right) \nonumber \\
&=  \frac{1}{2}\gamma_{k}(2h) + \frac{1}{2}\gamma_k(2h+1) \hspace{0.1cm} \mathrm{h = 0} 
\label{_auto11}\\
&=\frac{1}{4}\gamma_k(2h-1) + \frac{1}{2}\gamma_k(2h) + \frac{1}{4}\gamma_k(2h+1) \quad \mathrm{else}
\label{_auto12}
\end{align}
$$

<p>The quantity \( \hat{X} \) is asymptotic uncorrelated by assumption, \( \hat{X}_k \) is also asymptotic uncorrelated. Let's turn our attention to the variance of the sample mean \( V(\overline{X}) \). </p>
<h2 id="blocking-transformations-getting-there">Blocking Transformations, getting there </h2>
<p>We have</p>
$$
\begin{align}
V(\overline{X}_k) = \frac{\sigma_k^2}{n_k} + \underbrace{\frac{2}{n_k} \sum_{h=1}^{n_k-1}\left( 1 - \frac{h}{n_k} \right)\gamma_k(h)}_{\equiv e_k} = \frac{\sigma^2_k}{n_k} + e_k \quad \text{if} \quad \gamma_k(0) = \sigma_k^2. 
\label{_auto13}
\end{align}
$$

<p>The term \( e_k \) is called the <b>truncation error</b>: </p>
$$
\begin{equation}
e_k = \frac{2}{n_k} \sum_{h=1}^{n_k-1}\left( 1 - \frac{h}{n_k} \right)\gamma_k(h). 
\label{_auto14}
\end{equation}
$$

<p>We can show that \( V(\overline{X}_i) = V(\overline{X}_j) \) for all \( 0 \leq i \leq d-1 \) and \( 0 \leq j \leq d-1 \). </p>
<h2 id="blocking-transformations-final-expressions">Blocking Transformations, final expressions </h2>

<p>We can then wrap up</p>
$$
\begin{align}
n_{j+1} \overline{X}_{j+1}  &= \sum_{i=1}^{n_{j+1}} (\hat{X}_{j+1})_i =  \frac{1}{2}\sum_{i=1}^{n_{j}/2} (\hat{X}_{j})_{2i-1} + (\hat{X}_{j})_{2i} \nonumber \\
&= \frac{1}{2}\left[ (\hat{X}_j)_1 + (\hat{X}_j)_2 + \cdots + (\hat{X}_j)_{n_j} \right] = \underbrace{\frac{n_j}{2}}_{=n_{j+1}} \overline{X}_j = n_{j+1}\overline{X}_j. 
\label{_auto15}
\end{align}
$$

<p>By repeated use of this equation we get \( V(\overline{X}_i) = V(\overline{X}_0) = V(\overline{X}) \) for all \( 0 \leq i \leq d-1 \). This has the consequence that</p>
$$
\begin{align}
V(\overline{X}) = \frac{\sigma_k^2}{n_k} + e_k \qquad \text{for all} \qquad 0 \leq k \leq d-1. \label{eq:convergence}
\end{align}
$$

<p>Flyvbjerg and Petersen demonstrated that the sequence
\( \{e_k\}_{k=0}^{d-1} \) is decreasing, and conjecture that the term
\( e_k \) can be made as small as we would like by making \( k \) (and hence
\( d \)) sufficiently large. The sequence is decreasing (Master of Science thesis by Marius Jonsson, UiO 2018).
It means we can apply blocking transformations until
\( e_k \) is sufficiently small, and then estimate \( V(\overline{X}) \) by
\( \widehat{\sigma}^2_k/n_k \). 
</p>

<p>For an elegant solution and proof of the blocking method, see the recent article of <a href="https://journals.aps.org/pre/abstract/10.1103/PhysRevE.98.043304" target="_blank">Marius Jonsson (former MSc student of the Computational Physics group)</a>.</p>
<h1 id="boltzmann-machines">Boltzmann Machines </h1>

<p>Why use a generative model rather than the more well known discriminative deep neural networks (DNN)? </p>

<ul>
<li> Discriminitave methods have several limitations: They are mainly supervised learning methods, thus requiring labeled data. And there are tasks they cannot accomplish, like drawing new examples from an unknown probability distribution.</li>
<li> A generative model can learn to represent and sample from a probability distribution. The core idea is to learn a parametric model of the probability distribution from which the training data was drawn. As an example
<ol type="a"></li>
 <li> A model for images could learn to draw new examples of cats and dogs, given a training dataset of images of cats and dogs.</li>
 <li> Generate a sample of an ordered or disordered Ising model phase, having been given samples of such phases.</li>
 <li> Model the trial function for Monte Carlo calculations</li>
</ol>
<li> Both use gradient-descent based learning procedures for minimizing cost functions</li>
<li> Energy based models don't use backpropagation and automatic differentiation for computing gradients, instead turning to Markov Chain Monte Carlo methods.</li>
<li> DNNs often have several hidden layers. A restricted Boltzmann machine has only one hidden layer, however several RBMs can be stacked to make up Deep Belief Networks, of which they constitute the building blocks.</li>
</ul>
<p>History: The RBM was developed by amongst others Geoffrey Hinton, called by some the "Godfather of Deep Learning", working with the University of Toronto and Google.</p>

<p>A BM is what we would call an undirected probabilistic graphical model
with stochastic continuous or discrete units.
</p>

<p>It is interpreted as a stochastic recurrent neural network where the
state of each unit(neurons/nodes) depends on the units it is connected
to. The weights in the network represent thus the strength of the
interaction between various units/nodes.
</p>

<p>It turns into a Hopfield network if we choose deterministic rather
than stochastic units. In contrast to a Hopfield network, a BM is a
so-called generative model. It allows us to generate new samples from
the learned distribution.
</p>

<p>A standard BM network is divided into a set of observable and visible units \( \hat{x} \) and a set of unknown hidden units/nodes \( \hat{h} \).</p>

<p>Additionally there can be bias nodes for the hidden and visible layers. These biases are normally set to \( 1 \).</p>

<p>BMs are stackable, meaning they cwe can train a BM which serves as input to another BM. We can construct deep networks for learning complex PDFs. The layers can be trained one after another, a feature which makes them popular in deep learning</p>

<p>However, they are often hard to train. This leads to the introduction of so-called restricted BMs, or RBMS.
Here we take away all lateral connections between nodes in the visible layer as well as connections between nodes in the hidden layer. The network is illustrated in the figure below.
</p>
<h2 id="the-network">The network </h2>

<b>The network layers</b>:
<ol>
 <li> A function \( \mathbf{x} \) that represents the visible layer, a vector of \( M \) elements (nodes). This layer represents both what the RBM might be given as training input, and what we want it to be able to reconstruct. This might for example be the pixels of an image, the spin values of the Ising model, or coefficients representing speech.</li>
 <li> The function \( \mathbf{h} \) represents the hidden, or latent, layer. A vector of \( N \) elements (nodes). Also called "feature detectors".</li>
</ol>
<p>The goal of the hidden layer is to increase the model's expressive power. We encode complex interactions between visible variables by introducing additional, hidden variables that interact with visible degrees of freedom in a simple manner, yet still reproduce the complex correlations between visible degrees in the data once marginalized over (integrated out).</p>

<p>Examples of this trick being employed in physics: </p>
<ol>
 <li> The Hubbard-Stratonovich transformation</li>
 <li> The introduction of ghost fields in gauge theory</li>
 <li> Shadow wave functions in Quantum Monte Carlo simulations</li>
</ol>
<b>The network parameters, to be optimized/learned</b>:
<ol>
 <li> \( \mathbf{a} \) represents the visible bias, a vector of same length as \( \mathbf{x} \).</li>
 <li> \( \mathbf{b} \) represents the hidden bias, a vector of same lenght as \( \mathbf{h} \).</li>
 <li> \( W \) represents the interaction weights, a matrix of size \( M\times N \).</li>
</ol>
<h3 id="joint-distribution">Joint distribution </h3>
<p>The restricted Boltzmann machine is described by a Bolztmann distribution</p>
$$
\begin{align}
	P_{rbm}(\mathbf{x},\mathbf{h}) = \frac{1}{Z} e^{-\frac{1}{T_0}E(\mathbf{x},\mathbf{h})},
\label{_auto16}
\end{align}
$$

<p>where \( Z \) is the normalization constant or partition function, defined as </p>
$$
\begin{align}
	Z = \int \int e^{-\frac{1}{T_0}E(\mathbf{x},\mathbf{h})} d\mathbf{x} d\mathbf{h}.
\label{_auto17}
\end{align}
$$

<p>It is common to ignore \( T_0 \) by setting it to one. </p>
<h3 id="network-elements-the-energy-function">Network Elements, the energy function  </h3>

<p>The function \( E(\mathbf{x},\mathbf{h}) \) gives the <b>energy</b> of a
configuration (pair of vectors) \( (\mathbf{x}, \mathbf{h}) \). The lower
the energy of a configuration, the higher the probability of it. This
function also depends on the parameters \( \mathbf{a} \), \( \mathbf{b} \) and
\( W \). Thus, when we adjust them during the learning procedure, we are
adjusting the energy function to best fit our problem.
</p>
<h3 id="defining-different-types-of-rbms">Defining different types of RBMs </h3>

<p>There are different variants of RBMs, and the differences lie in the types of visible and hidden units we choose as well as in the implementation of the energy function \( E(\mathbf{x},\mathbf{h}) \). The connection between the nodes in the two layers is given by the weights \( w_{ij} \). </p>

<p> Binary-Binary RBM:</p>

<p>RBMs were first developed using binary units in both the visible and hidden layer. The corresponding energy function is defined as follows:</p>
$$
\begin{align}
	E(\mathbf{x}, \mathbf{h}) = - \sum_i^M x_i a_i- \sum_j^N b_j h_j - \sum_{i,j}^{M,N} x_i w_{ij} h_j,
\label{_auto18}
\end{align}
$$

<p>where the binary values taken on by the nodes are most commonly 0 and 1.</p>

<p> Gaussian-Binary RBM:</p>

<p>Another varient is the RBM where the visible units are Gaussian while the hidden units remain binary:</p>
$$
\begin{align}
	E(\mathbf{x}, \mathbf{h}) = \sum_i^M \frac{(x_i - a_i)^2}{2\sigma_i^2} - \sum_j^N b_j h_j - \sum_{i,j}^{M,N} \frac{x_i w_{ij} h_j}{\sigma_i^2}. 
\label{_auto19}
\end{align}
$$


<ol>
<li> RBMs are Useful when we model continuous data (i.e., we wish \( \mathbf{x} \) to be continuous)</li>
<li> Requires a smaller learning rate, since there's no upper bound to the value a component might take in the reconstruction</li>
</ol>
<p>Other types of units include:</p>
<ol>
<li> Softmax and multinomial units</li>
<li> Gaussian visible and hidden units</li>
<li> Binomial units</li>
<li> Rectified linear units</li>
</ol>
<h3 id="cost-function">Cost function </h3>

<p>When working with a training dataset, the most common training approach is maximizing the log-likelihood of the training data. The log likelihood characterizes the log-probability of generating the observed data using our generative model. Using this method our cost function is chosen as the negative log-likelihood. The learning then consists of trying to find parameters that maximize the probability of the dataset, and is known as Maximum Likelihood Estimation (MLE).
Denoting the parameters as \( \boldsymbol{\theta} = a_1,...,a_M,b_1,...,b_N,w_{11},...,w_{MN} \), the log-likelihood is given by
</p>
$$
\begin{align}
	\mathcal{L}(\{ \theta_i \}) &= \langle \text{log} P_\theta(\boldsymbol{x}) \rangle_{data} 
\label{_auto20}\\
	&= - \langle E(\boldsymbol{x}; \{ \theta_i\}) \rangle_{data} - \text{log} Z(\{ \theta_i\}),
\label{_auto21}
\end{align}
$$

<p>where we used that the normalization constant does not depend on the data, \( \langle \text{log} Z(\{ \theta_i\}) \rangle = \text{log} Z(\{ \theta_i\}) \)
Our cost function is the negative log-likelihood, \( \mathcal{C}(\{ \theta_i \}) = - \mathcal{L}(\{ \theta_i \}) \)
</p>
<h3 id="optimization-training">Optimization / Training </h3>

<p>The training procedure of choice often is Stochastic Gradient Descent (SGD). It consists of a series of iterations where we update the parameters according to the equation</p>
$$
\begin{align}
	\boldsymbol{\theta}_{k+1} = \boldsymbol{\theta}_k - \eta \nabla \mathcal{C} (\boldsymbol{\theta}_k)
\label{_auto22}
\end{align}
$$

<p>at each \( k \)-th iteration. There are a range of variants of the algorithm which aim at making the learning rate \( \eta \) more adaptive so the method might be more efficient while remaining stable.</p>

<p>We now need the gradient of the cost function in order to minimize it. We find that</p>
$$
\begin{align}
	\frac{\partial \mathcal{C}(\{ \theta_i\})}{\partial \theta_i}
	&= \langle \frac{\partial E(\boldsymbol{x}; \theta_i)}{\partial \theta_i} \rangle_{data}
	+ \frac{\partial \text{log} Z(\{ \theta_i\})}{\partial \theta_i} 
\label{_auto23}\\
	&= \langle O_i(\boldsymbol{x}) \rangle_{data} - \langle O_i(\boldsymbol{x}) \rangle_{model},
\label{_auto24}
\end{align}
$$

<p>where in order to simplify notation we defined the "operator"</p>
$$
\begin{align}
	O_i(\boldsymbol{x}) = \frac{\partial E(\boldsymbol{x}; \theta_i)}{\partial \theta_i}, 
\label{_auto25}
\end{align}
$$

<p>and used the statistical mechanics relationship between expectation values and the log-partition function:</p>
$$
\begin{align}
	\langle O_i(\boldsymbol{x}) \rangle_{model} = \text{Tr} P_\theta(\boldsymbol{x})O_i(\boldsymbol{x}) = - \frac{\partial \text{log} Z(\{ \theta_i\})}{\partial \theta_i}.
\label{_auto26}
\end{align}
$$

<p>The data-dependent term in the gradient is known as the positive phase
of the gradient, while the model-dependent term is known as the
negative phase of the gradient. The aim of the training is to lower
the energy of configurations that are near observed data points
(increasing their probability), and raising the energy of
configurations that are far from observed data points (decreasing
their probability).
</p>

<p>The gradient of the negative log-likelihood cost function of a Binary-Binary RBM is then</p>
$$
\begin{align}
	\frac{\partial \mathcal{C} (w_{ij}, a_i, b_j)}{\partial w_{ij}} =& \langle x_i h_j \rangle_{data} - \langle x_i h_j \rangle_{model} 
\label{_auto27}\\
	\frac{\partial \mathcal{C} (w_{ij}, a_i, b_j)}{\partial a_{ij}} =& \langle x_i \rangle_{data} - \langle x_i \rangle_{model} 
\label{_auto28}\\
	\frac{\partial \mathcal{C} (w_{ij}, a_i, b_j)}{\partial b_{ij}} =& \langle h_i \rangle_{data} - \langle h_i \rangle_{model}. 
\label{_auto29}\\
\label{_auto30}
\end{align}
$$

<p>To get the expectation values with respect to the <em>data</em>, we set the visible units to each of the observed samples in the training data, then update the hidden units according to the conditional probability found before. We then average over all samples in the training data to calculate expectation values with respect to the data. </p>
<h3 id="kullback-leibler-relative-entropy">Kullback-Leibler relative entropy </h3>

<p>When the goal of the training is to approximate a probability
distribution, as it is in generative modeling, another relevant
measure is the <b>Kullback-Leibler divergence</b>, also known as the
relative entropy or Shannon entropy. It is a non-symmetric measure of the
dissimilarity between two probability density functions \( p \) and
\( q \). If \( p \) is the unkown probability which we approximate with \( q \),
we can measure the difference by
</p>
$$
\begin{align}
	\text{KL}(p||q) = \int_{-\infty}^{\infty} p (\boldsymbol{x}) \log \frac{p(\boldsymbol{x})}{q(\boldsymbol{x})}  d\boldsymbol{x}.
\label{_auto31}
\end{align}
$$

<p>Thus, the Kullback-Leibler divergence between the distribution of the
training data \( f(\boldsymbol{x}) \) and the model distribution \( p(\boldsymbol{x}|
\boldsymbol{\theta}) \) is
</p>

$$
\begin{align}
	\text{KL} (f(\boldsymbol{x})|| p(\boldsymbol{x}| \boldsymbol{\theta})) =& \int_{-\infty}^{\infty}
	f (\boldsymbol{x}) \log \frac{f(\boldsymbol{x})}{p(\boldsymbol{x}| \boldsymbol{\theta})} d\boldsymbol{x} 
\label{_auto32}\\
	=& \int_{-\infty}^{\infty} f(\boldsymbol{x}) \log f(\boldsymbol{x}) d\boldsymbol{x} - \int_{-\infty}^{\infty} f(\boldsymbol{x}) \log
	p(\boldsymbol{x}| \boldsymbol{\theta}) d\boldsymbol{x} 
\label{_auto33}\\
	%=& \mathbb{E}_{f(\boldsymbol{x})} (\log f(\boldsymbol{x})) - \mathbb{E}_{f(\boldsymbol{x})} (\log p(\boldsymbol{x}| \boldsymbol{\theta}))
	=& \langle \log f(\boldsymbol{x}) \rangle_{f(\boldsymbol{x})} - \langle \log p(\boldsymbol{x}| \boldsymbol{\theta}) \rangle_{f(\boldsymbol{x})} 
\label{_auto34}\\
	=& \langle \log f(\boldsymbol{x}) \rangle_{data} + \langle E(\boldsymbol{x}) \rangle_{data} + \log Z 
\label{_auto35}\\
	=& \langle \log f(\boldsymbol{x}) \rangle_{data} + \mathcal{C}_{LL} .
\label{_auto36}
\end{align}
$$

<p>The first term is constant with respect to \( \boldsymbol{\theta} \) since \( f(\boldsymbol{x}) \) is independent of \( \boldsymbol{\theta} \). Thus the Kullback-Leibler Divergence is minimal when the second term is minimal. The second term is the log-likelihood cost function, hence minimizing the Kullback-Leibler divergence is equivalent to maximizing the log-likelihood.</p>

<p>To further understand generative models it is useful to study the
gradient of the cost function which is needed in order to minimize it
using methods like stochastic gradient descent. 
</p>

<p>The partition function is the generating function of
expectation values, in particular there are mathematical relationships
between expectation values and the log-partition function. In this
case we have
</p>
$$
\begin{align}
	\langle \frac{ \partial E(\boldsymbol{x}; \theta_i) } { \partial \theta_i} \rangle_{model}
	= \int p(\boldsymbol{x}| \boldsymbol{\theta}) \frac{ \partial E(\boldsymbol{x}; \theta_i) } { \partial \theta_i} d\boldsymbol{x} 
	= -\frac{\partial \log Z(\theta_i)}{ \partial  \theta_i} .
\label{_auto37}
\end{align}
$$

<p>Here \( \langle \cdot \rangle_{model} \) is the expectation value over the model probability distribution \( p(\boldsymbol{x}| \boldsymbol{\theta}) \).</p>
<h2 id="setting-up-for-gradient-descent-calculations">Setting up for gradient descent calculations </h2>

<p>Using the previous relationship we can express the gradient of the cost function as</p>

$$
\begin{align}
	\frac{\partial \mathcal{C}_{LL}}{\partial \theta_i}
	=& \langle \frac{ \partial E(\boldsymbol{x}; \theta_i) } { \partial \theta_i} \rangle_{data} + \frac{\partial \log Z(\theta_i)}{ \partial  \theta_i} 
\label{_auto38}\\
	=& \langle \frac{ \partial E(\boldsymbol{x}; \theta_i) } { \partial \theta_i} \rangle_{data} - \langle \frac{ \partial E(\boldsymbol{x}; \theta_i) } { \partial \theta_i} \rangle_{model} 
\label{_auto39}\\
	%=& \langle O_i(\boldsymbol{x}) \rangle_{data} - \langle O_i(\boldsymbol{x}) \rangle_{model}
\label{_auto40}
\end{align}
$$

<p>This expression shows that the gradient of the log-likelihood cost
function is a <b>difference of moments</b>, with one calculated from
the data and one calculated from the model. The data-dependent term is
called the <b>positive phase</b> and the model-dependent term is
called the <b>negative phase</b> of the gradient. We see now that
minimizing the cost function results in lowering the energy of
configurations \( \boldsymbol{x} \) near points in the training data and
increasing the energy of configurations not observed in the training
data. That means we increase the model's probability of configurations
similar to those in the training data.
</p>

<p>The gradient of the cost function also demonstrates why gradients of
unsupervised, generative models must be computed differently from for
those of for example FNNs. While the data-dependent expectation value
is easily calculated based on the samples \( \boldsymbol{x}_i \) in the training
data, we must sample from the model in order to generate samples from
which to caclulate the model-dependent term. We sample from the model
by using MCMC-based methods. We can not sample from the model directly
because the partition function \( Z \) is generally intractable.
</p>

<p>As in supervised machine learning problems, the goal is also here to
perform well on <b>unseen</b> data, that is to have good
generalization from the training data. The distribution \( f(x) \) we
approximate is not the <b>true</b> distribution we wish to estimate,
it is limited to the training data. Hence, in unsupervised training as
well it is important to prevent overfitting to the training data. Thus
it is common to add regularizers to the cost function in the same
manner as we discussed for say linear regression.
</p>
<h2 id="rbms-for-the-quantum-many-body-problem">RBMs for the quantum many body problem </h2>

<p>The idea of applying RBMs to quantum many body problems was presented by G. Carleo and M. Troyer, working with ETH Zurich and Microsoft Research.</p>

<p>Some of their motivation included</p>

<ul>
<li> The wave function \( \Psi \) is a monolithic mathematical quantity that contains all the information on a quantum state, be it a single particle or a complex molecule. In principle, an exponential amount of information is needed to fully encode a generic many-body quantum state.</li>
<li> There are still interesting open problems, including fundamental questions ranging from the dynamical properties of high-dimensional systems to the exact ground-state properties of strongly interacting fermions.</li>
<li> The difficulty lies in finding a general strategy to reduce the exponential complexity of the full many-body wave function down to its most essential features. That is
<ol type="a"></li>
<li> Dimensional reduction</li>
<li> Feature extraction</li>
</ol>
<li> Among the most successful techniques to attack these challenges, artifical neural networks play a prominent role.</li>
<li> Want to understand whether an artifical neural network may adapt to describe a quantum system.</li>
</ul>
<p>Carleo and Troyer applied the RBM to the quantum mechanical spin lattice systems of the Ising model and Heisenberg model, with encouraging results. Our goal is to test the method on systems of moving particles. For the spin lattice systems it was natural to use a binary-binary RBM, with the nodes taking values of 1 and -1. For moving particles, on the other hand, we want the visible nodes to be continuous, representing position coordinates. Thus, we start by choosing a Gaussian-binary RBM, where the visible nodes are continuous and hidden nodes take on values of 0 or 1. If eventually we would like the hidden nodes to be continuous as well the rectified linear units seem like the most relevant choice.</p>
<h2 id="representing-the-wave-function">Representing the wave function </h2>

<p>The wavefunction should be a probability amplitude depending on
 \( \boldsymbol{x} \). The RBM model is given by the joint distribution of
 \( \boldsymbol{x} \) and \( \boldsymbol{h} \)
</p>

$$
\begin{align}
        F_{rbm}(\mathbf{x},\mathbf{h}) = \frac{1}{Z} e^{-\frac{1}{T_0}E(\mathbf{x},\mathbf{h})}.
\label{_auto41}
\end{align}
$$

<p>To find the marginal distribution of \( \boldsymbol{x} \) we set:</p>

$$
\begin{align}
        F_{rbm}(\mathbf{x}) &= \sum_\mathbf{h} F_{rbm}(\mathbf{x}, \mathbf{h}) 
\label{_auto42}\\
                                &= \frac{1}{Z}\sum_\mathbf{h} e^{-E(\mathbf{x}, \mathbf{h})}.
\label{_auto43}
\end{align}
$$

<p>Now this is what we use to represent the wave function, calling it a neural-network quantum state (NQS)</p>
$$
\begin{align}
        \Psi (\mathbf{X}) &= F_{rbm}(\mathbf{x}) 
\label{_auto44}\\
        &= \frac{1}{Z}\sum_{\boldsymbol{h}} e^{-E(\mathbf{x}, \mathbf{h})} 
\label{_auto45}\\
        &= \frac{1}{Z} \sum_{\{h_j\}} e^{-\sum_i^M \frac{(x_i - a_i)^2}{2\sigma^2} + \sum_j^N b_j h_j + \sum_\
{i,j}^{M,N} \frac{x_i w_{ij} h_j}{\sigma^2}} 
\label{_auto46}\\
        &= \frac{1}{Z} e^{-\sum_i^M \frac{(x_i - a_i)^2}{2\sigma^2}} \prod_j^N (1 + e^{b_j + \sum_i^M \frac{x\
_i w_{ij}}{\sigma^2}}). 
\label{_auto47}\\
\label{_auto48}
\end{align}
$$
<h2 id="choose-the-cost-function">Choose the cost function </h2>

<p>Now we don't necessarily have training data (unless we generate it by using some other method). However, what we do have is the variational principle which allows us to obtain the ground state wave function by minimizing the expectation value of the energy of a trial wavefunction (corresponding to the untrained NQS). Similarly to the traditional variational Monte Carlo method then, it is the local energy we wish to minimize. The gradient to use for the stochastic gradient descent procedure is</p>

$$
\begin{align}
	C_i = \frac{\partial \langle E_L \rangle}{\partial \theta_i}
	= 2(\langle E_L \frac{1}{\Psi}\frac{\partial \Psi}{\partial \theta_i} \rangle - \langle E_L \rangle \langle \frac{1}{\Psi}\frac{\partial \Psi}{\partial \theta_i} \rangle ),
\label{_auto49}
\end{align}
$$

<p>where the local energy is given by</p>
$$
\begin{align}
	E_L = \frac{1}{\Psi} \hat{\mathbf{H}} \Psi.
\label{_auto50}
\end{align}
$$
<h3 id="mathematical-details">Mathematical details </h3>

<p>Because we are restricted to potential functions which are positive it
is convenient to express them as exponentials, so that
</p>

$$
\begin{align}
	\phi_C (\boldsymbol{x}_C) = e^{-E_C(\boldsymbol{x}_C)}
\label{_auto51}
\end{align}
$$

<p>where \( E(\boldsymbol{x}_C) \) is called an <em>energy function</em>, and the
exponential representation is the <em>Boltzmann distribution</em>. The
joint distribution is defined as the product of potentials.
</p>

<p>The joint distribution of the random variables is then</p>

$$
\begin{align}
	p(\boldsymbol{x}) =& \frac{1}{Z} \prod_C \phi_C (\boldsymbol{x}_C) \nonumber \\
	=& \frac{1}{Z} \prod_C e^{-E_C(\boldsymbol{x}_C)} \nonumber \\
	=& \frac{1}{Z} e^{-\sum_C E_C(\boldsymbol{x}_C)} \nonumber \\
	=& \frac{1}{Z} e^{-E(\boldsymbol{x})}.
\label{_auto52}
\end{align} 
$$

$$
\begin{align}
	p_{BM}(\boldsymbol{x}, \boldsymbol{h}) = \frac{1}{Z_{BM}} e^{-\frac{1}{T}E_{BM}(\boldsymbol{x}, \boldsymbol{h})} ,
\label{_auto53}
\end{align}
$$

<p>with the partition function </p>
$$
\begin{align}
	Z_{BM} = \int \int e^{-\frac{1}{T} E_{BM}(\tilde{\boldsymbol{x}}, \tilde{\boldsymbol{h}})} d\tilde{\boldsymbol{x}} d\tilde{\boldsymbol{h}} .
\label{_auto54}
\end{align}
$$

<p>\( T \) is a physics-inspired parameter named temperature and will be assumed to be 1 unless otherwise stated. The energy function of the Boltzmann machine determines the interactions between the nodes and is defined  </p>

$$
\begin{align}
	E_{BM}(\boldsymbol{x}, \boldsymbol{h}) =& - \sum_{i, k}^{M, K} a_i^k \alpha_i^k (x_i)
	- \sum_{j, l}^{N, L} b_j^l \beta_j^l (h_j) 
	- \sum_{i,j,k,l}^{M,N,K,L} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (h_j) \nonumber \\
	&- \sum_{i, m=i+1, k}^{M, M, K} \alpha_i^k (x_i) v_{im}^k \alpha_m^k (x_m)
	- \sum_{j,n=j+1,l}^{N,N,L} \beta_j^l (h_j) u_{jn}^l \beta_n^l (h_n).
\label{_auto55}
\end{align}
$$

<p>Here \( \alpha_i^k (x_i) \) and \( \beta_j^l (h_j) \) are one-dimensional
transfer functions or mappings from the given input value to the
desired feature value. They can be arbitrary functions of the input
variables and are independent of the parameterization (parameters
referring to weight and biases), meaning they are not affected by
training of the model. The indices \( k \) and \( l \) indicate that there can
be multiple transfer functions per variable.  Furthermore, \( a_i^k \) and
\( b_j^l \) are the visible and hidden bias. \( w_{ij}^{kl} \) are weights of
the \textbf{inter-layer} connection terms which connect visible and
hidden units. $ v_{im}^k$ and \( u_{jn}^l \) are weights of the
\textbf{intra-layer} connection terms which connect the visible units
to each other and the hidden units to each other, respectively.
</p>

<p>We remove the intra-layer connections by setting \( v_{im} \) and \( u_{jn} \)
to zero. The expression for the energy of the RBM is then
</p>

$$
\begin{align}
	E_{RBM}(\boldsymbol{x}, \boldsymbol{h}) = - \sum_{i, k}^{M, K} a_i^k \alpha_i^k (x_i)
	- \sum_{j, l}^{N, L} b_j^l \beta_j^l (h_j) 
	- \sum_{i,j,k,l}^{M,N,K,L} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (h_j). 
\label{_auto56}
\end{align}
$$

<p>resulting in </p>
$$
\begin{align}
	P_{RBM} (\boldsymbol{x}) =& \int P_{RBM} (\boldsymbol{x}, \tilde{\boldsymbol{h}})  d \tilde{\boldsymbol{h}} \nonumber \\
	=& \frac{1}{Z_{RBM}} \int e^{-E_{RBM} (\boldsymbol{x}, \tilde{\boldsymbol{h}}) } d\tilde{\boldsymbol{h}} \nonumber \\
	=& \frac{1}{Z_{RBM}} \int e^{\sum_{i, k} a_i^k \alpha_i^k (x_i)
	+ \sum_{j, l} b_j^l \beta_j^l (\tilde{h}_j) 
	+ \sum_{i,j,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (\tilde{h}_j)} 
	d\tilde{\boldsymbol{h}} \nonumber \\
	=& \frac{1}{Z_{RBM}} e^{\sum_{i, k} a_i^k \alpha_i^k (x_i)}
	\int \prod_j^N e^{\sum_l b_j^l \beta_j^l (\tilde{h}_j) 
	+ \sum_{i,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (\tilde{h}_j)} d\tilde{\boldsymbol{h}} \nonumber \\
	=& \frac{1}{Z_{RBM}} e^{\sum_{i, k} a_i^k \alpha_i^k (x_i)}
	\biggl( \int e^{\sum_l b_1^l \beta_1^l (\tilde{h}_1) + \sum_{i,k,l} \alpha_i^k (x_i) w_{i1}^{kl} \beta_1^l (\tilde{h}_1)} d \tilde{h}_1 \nonumber \\
	& \times \int e^{\sum_l b_2^l \beta_2^l (\tilde{h}_2) + \sum_{i,k,l} \alpha_i^k (x_i) w_{i2}^{kl} \beta_2^l (\tilde{h}_2)} d \tilde{h}_2 \nonumber \\
	& \times ... \nonumber \\
	& \times \int e^{\sum_l b_N^l \beta_N^l (\tilde{h}_N) + \sum_{i,k,l} \alpha_i^k (x_i) w_{iN}^{kl} \beta_N^l (\tilde{h}_N)} d \tilde{h}_N \biggr) \nonumber \\
	=& \frac{1}{Z_{RBM}} e^{\sum_{i, k} a_i^k \alpha_i^k (x_i)}
	\prod_j^N \int e^{\sum_l b_j^l \beta_j^l (\tilde{h}_j) + \sum_{i,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (\tilde{h}_j)}  d\tilde{h}_j
\label{_auto57}
\end{align}
$$

<p>Similarly</p>

$$
\begin{align}
	P_{RBM} (\boldsymbol{h}) =& \frac{1}{Z_{RBM}} \int e^{-E_{RBM} (\tilde{\boldsymbol{x}}, \boldsymbol{h})} d\tilde{\boldsymbol{x}} \nonumber \\
	=& \frac{1}{Z_{RBM}} e^{\sum_{j, l} b_j^l \beta_j^l (h_j)}
	\prod_i^M \int e^{\sum_k a_i^k \alpha_i^k (\tilde{x}_i)
	+ \sum_{j,k,l} \alpha_i^k (\tilde{x}_i) w_{ij}^{kl} \beta_j^l (h_j)} d\tilde{x}_i
\label{_auto58}
\end{align}
$$

<p>Using Bayes theorem</p>

$$
\begin{align}
	P_{RBM} (\boldsymbol{h}|\boldsymbol{x}) =& \frac{P_{RBM} (\boldsymbol{x}, \boldsymbol{h})}{P_{RBM} (\boldsymbol{x})} \nonumber \\
	=& \frac{\frac{1}{Z_{RBM}} e^{\sum_{i, k} a_i^k \alpha_i^k (x_i)
	+ \sum_{j, l} b_j^l \beta_j^l (h_j) 
	+ \sum_{i,j,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (h_j)}}
	{\frac{1}{Z_{RBM}} e^{\sum_{i, k} a_i^k \alpha_i^k (x_i)}
	\prod_j^N \int e^{\sum_l b_j^l \beta_j^l (\tilde{h}_j) + \sum_{i,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (\tilde{h}_j)}  d\tilde{h}_j} \nonumber \\
	=& \prod_j^N \frac{e^{\sum_l b_j^l \beta_j^l (h_j) + \sum_{i,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (h_j)} }
	{\int e^{\sum_l b_j^l \beta_j^l (\tilde{h}_j) + \sum_{i,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (\tilde{h}_j)}  d\tilde{h}_j}
\label{_auto59}
\end{align}
$$

<p>Similarly</p>

$$
\begin{align}
	P_{RBM} (\boldsymbol{x}|\boldsymbol{h}) =&  \frac{P_{RBM} (\boldsymbol{x}, \boldsymbol{h})}{P_{RBM} (\boldsymbol{h})} \nonumber \\
	=& \prod_i^M \frac{e^{\sum_k a_i^k \alpha_i^k (x_i)
	+ \sum_{j,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (h_j)}}
	{\int e^{\sum_k a_i^k \alpha_i^k (\tilde{x}_i)
	+ \sum_{j,k,l} \alpha_i^k (\tilde{x}_i) w_{ij}^{kl} \beta_j^l (h_j)} d\tilde{x}_i}
\label{_auto60}
\end{align}
$$

<p>The original RBM had binary visible and hidden nodes. They were
showned to be universal approximators of discrete distributions.
It was also shown that adding hidden units yields
strictly improved modelling power. The common choice of binary values
are 0 and 1. However, in some physics applications, -1 and 1 might be
a more natural choice. We will here use 0 and 1.
</p>

$$
\begin{align}
	E_{BB}(\boldsymbol{x}, \mathbf{h}) = - \sum_i^M x_i a_i- \sum_j^N b_j h_j - \sum_{i,j}^{M,N} x_i w_{ij} h_j.
\label{_auto61}
\end{align}
$$


$$
\begin{align}
	p_{BB}(\boldsymbol{x}, \boldsymbol{h}) =& \frac{1}{Z_{BB}} e^{\sum_i^M a_i x_i + \sum_j^N b_j h_j + \sum_{ij}^{M,N} x_i w_{ij} h_j} 
\label{_auto62}\\
	=& \frac{1}{Z_{BB}} e^{\boldsymbol{x}^T \boldsymbol{a} + \boldsymbol{b}^T \boldsymbol{h} + \boldsymbol{x}^T \boldsymbol{W} \boldsymbol{h}}
\label{_auto63}
\end{align}
$$

<p>with the partition function</p>

$$
\begin{align}
	Z_{BB} = \sum_{\boldsymbol{x}, \boldsymbol{h}} e^{\boldsymbol{x}^T \boldsymbol{a} + \boldsymbol{b}^T \boldsymbol{h} + \boldsymbol{x}^T \boldsymbol{W} \boldsymbol{h}} .
\label{_auto64}
\end{align}
$$
<h3 id="marginal-probability-density-functions">Marginal Probability Density Functions </h3>

<p>In order to find the probability of any configuration of the visible units we derive the marginal probability density function.</p>

$$
\begin{align}
	p_{BB} (\boldsymbol{x}) =& \sum_{\boldsymbol{h}} p_{BB} (\boldsymbol{x}, \boldsymbol{h}) 
\label{_auto65}\\
	=& \frac{1}{Z_{BB}} \sum_{\boldsymbol{h}} e^{\boldsymbol{x}^T \boldsymbol{a} + \boldsymbol{b}^T \boldsymbol{h} + \boldsymbol{x}^T \boldsymbol{W} \boldsymbol{h}} \nonumber \\
	=& \frac{1}{Z_{BB}} e^{\boldsymbol{x}^T \boldsymbol{a}} \sum_{\boldsymbol{h}} e^{\sum_j^N (b_j + \boldsymbol{x}^T \boldsymbol{w}_{\ast j})h_j} \nonumber \\
	=& \frac{1}{Z_{BB}} e^{\boldsymbol{x}^T \boldsymbol{a}} \sum_{\boldsymbol{h}} \prod_j^N e^{ (b_j + \boldsymbol{x}^T \boldsymbol{w}_{\ast j})h_j} \nonumber \\
	=& \frac{1}{Z_{BB}} e^{\boldsymbol{x}^T \boldsymbol{a}} \bigg ( \sum_{h_1} e^{(b_1 + \boldsymbol{x}^T \boldsymbol{w}_{\ast 1})h_1}
	\times \sum_{h_2} e^{(b_2 + \boldsymbol{x}^T \boldsymbol{w}_{\ast 2})h_2} \times \nonumber \\
	& ... \times \sum_{h_2} e^{(b_N + \boldsymbol{x}^T \boldsymbol{w}_{\ast N})h_N} \bigg ) \nonumber \\
	=& \frac{1}{Z_{BB}} e^{\boldsymbol{x}^T \boldsymbol{a}} \prod_j^N \sum_{h_j} e^{(b_j + \boldsymbol{x}^T \boldsymbol{w}_{\ast j}) h_j} \nonumber \\
	=& \frac{1}{Z_{BB}} e^{\boldsymbol{x}^T \boldsymbol{a}} \prod_j^N (1 + e^{b_j + \boldsymbol{x}^T \boldsymbol{w}_{\ast j}}) .
\label{_auto66}
\end{align}
$$

<p>A similar derivation yields the marginal probability of the hidden units</p>

$$
\begin{align}
	p_{BB} (\boldsymbol{h}) = \frac{1}{Z_{BB}} e^{\boldsymbol{b}^T \boldsymbol{h}} \prod_i^M (1 + e^{a_i + \boldsymbol{w}_{i\ast}^T \boldsymbol{h}}) .
\label{_auto67}
\end{align}
$$
<h3 id="conditional-probability-density-functions">Conditional Probability Density Functions </h3>

<p>We derive the probability of the hidden units given the visible units using Bayes' rule</p>

$$
\begin{align}
	p_{BB} (\boldsymbol{h}|\boldsymbol{x}) =& \frac{p_{BB} (\boldsymbol{x}, \boldsymbol{h})}{p_{BB} (\boldsymbol{x})} \nonumber \\
	=& \frac{ \frac{1}{Z_{BB}}  e^{\boldsymbol{x}^T \boldsymbol{a} + \boldsymbol{b}^T \boldsymbol{h} + \boldsymbol{x}^T \boldsymbol{W} \boldsymbol{h}} }
	        {\frac{1}{Z_{BB}} e^{\boldsymbol{x}^T \boldsymbol{a}} \prod_j^N (1 + e^{b_j + \boldsymbol{x}^T \boldsymbol{w}_{\ast j}})} \nonumber \\
	=& \frac{  e^{\boldsymbol{x}^T \boldsymbol{a}} e^{ \sum_j^N (b_j + \boldsymbol{x}^T \boldsymbol{w}_{\ast j} ) h_j} }
	        { e^{\boldsymbol{x}^T \boldsymbol{a}} \prod_j^N (1 + e^{b_j + \boldsymbol{x}^T \boldsymbol{w}_{\ast j}})} \nonumber \\
	=& \prod_j^N \frac{ e^{(b_j + \boldsymbol{x}^T \boldsymbol{w}_{\ast j} ) h_j}  }
	{1 + e^{b_j + \boldsymbol{x}^T \boldsymbol{w}_{\ast j}}} \nonumber \\
	=& \prod_j^N p_{BB} (h_j| \boldsymbol{x}) .
\label{_auto68}
\end{align}
$$

<p>From this we find the probability of a hidden unit being "on" or "off":</p>

$$
\begin{align}
	p_{BB} (h_j=1 | \boldsymbol{x}) =&   \frac{ e^{(b_j + \boldsymbol{x}^T \boldsymbol{w}_{\ast j} ) h_j}  }
	{1 + e^{b_j + \boldsymbol{x}^T \boldsymbol{w}_{\ast j}}} 
\label{_auto69}\\
	=&  \frac{ e^{(b_j + \boldsymbol{x}^T \boldsymbol{w}_{\ast j} )}  }
	{1 + e^{b_j + \boldsymbol{x}^T \boldsymbol{w}_{\ast j}}} 
\label{_auto70}\\
	=&  \frac{ 1 }{1 + e^{-(b_j + \boldsymbol{x}^T \boldsymbol{w}_{\ast j})} } ,
\label{_auto71}
\end{align}
$$

<p>and</p>

$$
\begin{align}
	p_{BB} (h_j=0 | \boldsymbol{x}) =\frac{ 1 }{1 + e^{b_j + \boldsymbol{x}^T \boldsymbol{w}_{\ast j}} } .
\label{_auto72}
\end{align}
$$

<p>Similarly we have that the conditional probability of the visible units given the hidden are</p>

$$
\begin{align}
	p_{BB} (\boldsymbol{x}|\boldsymbol{h}) =& \prod_i^M \frac{ e^{ (a_i + \boldsymbol{w}_{i\ast}^T \boldsymbol{h}) x_i} }{ 1 + e^{a_i + \boldsymbol{w}_{i\ast}^T \boldsymbol{h}} } 
\label{_auto73}\\
	&= \prod_i^M p_{BB} (x_i | \boldsymbol{h}) .
\label{_auto74}
\end{align}
$$


$$
\begin{align}
	p_{BB} (x_i=1 | \boldsymbol{h}) =& \frac{1}{1 + e^{-(a_i + \boldsymbol{w}_{i\ast}^T \boldsymbol{h} )}} 
\label{_auto75}\\
	p_{BB} (x_i=0 | \boldsymbol{h}) =& \frac{1}{1 + e^{a_i + \boldsymbol{w}_{i\ast}^T \boldsymbol{h} }} .
\label{_auto76}
\end{align}
$$
<h3 id="gaussian-binary-restricted-boltzmann-machines">Gaussian-Binary Restricted Boltzmann Machines </h3>

<p>Inserting into the expression for \( E_{RBM}(\boldsymbol{x},\boldsymbol{h}) \) in equation  results in the energy</p>

$$
\begin{align}
	E_{GB}(\boldsymbol{x}, \boldsymbol{h}) =& \sum_i^M \frac{(x_i - a_i)^2}{2\sigma_i^2}
	- \sum_j^N b_j h_j 
	-\sum_{ij}^{M,N} \frac{x_i w_{ij} h_j}{\sigma_i^2} \nonumber \\
	=& \vert\vert\frac{\boldsymbol{x} -\boldsymbol{a}}{2\boldsymbol{\sigma}}\vert\vert^2 - \boldsymbol{b}^T \boldsymbol{h} 
	- (\frac{\boldsymbol{x}}{\boldsymbol{\sigma}^2})^T \boldsymbol{W}\boldsymbol{h} . 
\label{_auto77}
\end{align}
$$
<h3 id="joint-probability-density-function">Joint Probability Density Function </h3>

$$
\begin{align}
	p_{GB} (\boldsymbol{x}, \boldsymbol{h}) =& \frac{1}{Z_{GB}} e^{-\vert\vert\frac{\boldsymbol{x} -\boldsymbol{a}}{2\boldsymbol{\sigma}}\vert\vert^2 + \boldsymbol{b}^T \boldsymbol{h} 
	+ (\frac{\boldsymbol{x}}{\boldsymbol{\sigma}^2})^T \boldsymbol{W}\boldsymbol{h}} \nonumber \\
	=& \frac{1}{Z_{GB}} e^{- \sum_i^M \frac{(x_i - a_i)^2}{2\sigma_i^2}
	+ \sum_j^N b_j h_j 
	+\sum_{ij}^{M,N} \frac{x_i w_{ij} h_j}{\sigma_i^2}} \nonumber \\
	=& \frac{1}{Z_{GB}} \prod_{ij}^{M,N} e^{-\frac{(x_i - a_i)^2}{2\sigma_i^2}
	+ b_j h_j 
	+\frac{x_i w_{ij} h_j}{\sigma_i^2}} ,
\label{_auto78}
\end{align}
$$

<p>with the partition function given by</p>

$$
\begin{align}
	Z_{GB} =& \int \sum_{\tilde{\boldsymbol{h}}}^{\tilde{\boldsymbol{H}}} e^{-\vert\vert\frac{\tilde{\boldsymbol{x}} -\boldsymbol{a}}{2\boldsymbol{\sigma}}\vert\vert^2 + \boldsymbol{b}^T \tilde{\boldsymbol{h}} 
	+ (\frac{\tilde{\boldsymbol{x}}}{\boldsymbol{\sigma}^2})^T \boldsymbol{W}\tilde{\boldsymbol{h}}} d\tilde{\boldsymbol{x}} .
\label{_auto79}
\end{align}
$$
<h3 id="marginal-probability-density-functions">Marginal Probability Density Functions </h3>

<p>We proceed to find the marginal probability densitites of the
Gaussian-binary RBM. We first marginalize over the binary hidden units
to find \( p_{GB} (\boldsymbol{x}) \)
</p>

$$
\begin{align}
	p_{GB} (\boldsymbol{x}) =& \sum_{\tilde{\boldsymbol{h}}}^{\tilde{\boldsymbol{H}}} p_{GB} (\boldsymbol{x}, \tilde{\boldsymbol{h}}) \nonumber \\
	=& \frac{1}{Z_{GB}} \sum_{\tilde{\boldsymbol{h}}}^{\tilde{\boldsymbol{H}}} 
	e^{-\vert\vert\frac{\boldsymbol{x} -\boldsymbol{a}}{2\boldsymbol{\sigma}}\vert\vert^2 + \boldsymbol{b}^T \tilde{\boldsymbol{h}} 
	+ (\frac{\boldsymbol{x}}{\boldsymbol{\sigma}^2})^T \boldsymbol{W}\tilde{\boldsymbol{h}}} \nonumber \\
	=& \frac{1}{Z_{GB}} e^{-\vert\vert\frac{\boldsymbol{x} -\boldsymbol{a}}{2\boldsymbol{\sigma}}\vert\vert^2}
	\prod_j^N (1 + e^{b_j + (\frac{\boldsymbol{x}}{\boldsymbol{\sigma}^2})^T \boldsymbol{w}_{\ast j}} ) .
\label{_auto80}
\end{align}
$$

<p>We next marginalize over the visible units. This is the first time we
marginalize over continuous values. We rewrite the exponential factor
dependent on \( \boldsymbol{x} \) as a Gaussian function before we integrate in
the last step.
</p>

$$
\begin{align}
	p_{GB} (\boldsymbol{h}) =& \int p_{GB} (\tilde{\boldsymbol{x}}, \boldsymbol{h}) d\tilde{\boldsymbol{x}} \nonumber \\
	=& \frac{1}{Z_{GB}} \int e^{-\vert\vert\frac{\tilde{\boldsymbol{x}} -\boldsymbol{a}}{2\boldsymbol{\sigma}}\vert\vert^2 + \boldsymbol{b}^T \boldsymbol{h} 
	+ (\frac{\tilde{\boldsymbol{x}}}{\boldsymbol{\sigma}^2})^T \boldsymbol{W}\boldsymbol{h}} d\tilde{\boldsymbol{x}} \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\boldsymbol{b}^T \boldsymbol{h} } \int \prod_i^M
	e^{- \frac{(\tilde{x}_i - a_i)^2}{2\sigma_i^2} + \frac{\tilde{x}_i \boldsymbol{w}_{i\ast}^T \boldsymbol{h}}{\sigma_i^2} } d\tilde{\boldsymbol{x}} \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\boldsymbol{b}^T \boldsymbol{h} }
	\biggl( \int e^{- \frac{(\tilde{x}_1 - a_1)^2}{2\sigma_1^2} + \frac{\tilde{x}_1 \boldsymbol{w}_{1\ast}^T \boldsymbol{h}}{\sigma_1^2} } d\tilde{x}_1 \nonumber \\
	& \times \int e^{- \frac{(\tilde{x}_2 - a_2)^2}{2\sigma_2^2} + \frac{\tilde{x}_2 \boldsymbol{w}_{2\ast}^T \boldsymbol{h}}{\sigma_2^2} } d\tilde{x}_2 \nonumber \\
	& \times ... \nonumber \\
	&\times \int e^{- \frac{(\tilde{x}_M - a_M)^2}{2\sigma_M^2} + \frac{\tilde{x}_M \boldsymbol{w}_{M\ast}^T \boldsymbol{h}}{\sigma_M^2} } d\tilde{x}_M \biggr) \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\boldsymbol{b}^T \boldsymbol{h}} \prod_i^M
	\int e^{- \frac{(\tilde{x}_i - a_i)^2 - 2\tilde{x}_i \boldsymbol{w}_{i\ast}^T \boldsymbol{h}}{2\sigma_i^2} } d\tilde{x}_i \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\boldsymbol{b}^T \boldsymbol{h}} \prod_i^M
	\int e^{- \frac{\tilde{x}_i^2 - 2\tilde{x}_i(a_i + \tilde{x}_i \boldsymbol{w}_{i\ast}^T \boldsymbol{h}) + a_i^2}{2\sigma_i^2} } d\tilde{x}_i \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\boldsymbol{b}^T \boldsymbol{h}} \prod_i^M
	\int e^{- \frac{\tilde{x}_i^2 - 2\tilde{x}_i(a_i + \boldsymbol{w}_{i\ast}^T \boldsymbol{h}) + (a_i + \boldsymbol{w}_{i\ast}^T \boldsymbol{h})^2 - (a_i + \boldsymbol{w}_{i\ast}^T \boldsymbol{h})^2 + a_i^2}{2\sigma_i^2} } d\tilde{x}_i \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\boldsymbol{b}^T \boldsymbol{h}} \prod_i^M
	\int e^{- \frac{(\tilde{x}_i - (a_i + \boldsymbol{w}_{i\ast}^T \boldsymbol{h}))^2 - a_i^2 -2a_i \boldsymbol{w}_{i\ast}^T \boldsymbol{h} - (\boldsymbol{w}_{i\ast}^T \boldsymbol{h})^2 + a_i^2}{2\sigma_i^2} } d\tilde{x}_i \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\boldsymbol{b}^T \boldsymbol{h}} \prod_i^M
	e^{\frac{2a_i \boldsymbol{w}_{i\ast}^T \boldsymbol{h} +(\boldsymbol{w}_{i\ast}^T \boldsymbol{h})^2 }{2\sigma_i^2}}
	\int e^{- \frac{(\tilde{x}_i - a_i - \boldsymbol{w}_{i\ast}^T \boldsymbol{h})^2}{2\sigma_i^2}}
	d\tilde{x}_i \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\boldsymbol{b}^T \boldsymbol{h}} \prod_i^M
	\sqrt{2\pi \sigma_i^2}
	e^{\frac{2a_i \boldsymbol{w}_{i\ast}^T \boldsymbol{h} +(\boldsymbol{w}_{i\ast}^T \boldsymbol{h})^2 }{2\sigma_i^2}} .
\label{_auto81}
\end{align}
$$
<h3 id="conditional-probability-density-functions">Conditional Probability Density Functions </h3>

<p>We finish by deriving the conditional probabilities.</p>
$$
\begin{align}
	p_{GB} (\boldsymbol{h}| \boldsymbol{x}) =& \frac{p_{GB} (\boldsymbol{x}, \boldsymbol{h})}{p_{GB} (\boldsymbol{x})} \nonumber \\
	=& \frac{\frac{1}{Z_{GB}} e^{-\vert\vert\frac{\boldsymbol{x} -\boldsymbol{a}}{2\boldsymbol{\sigma}}\vert\vert^2 + \boldsymbol{b}^T \boldsymbol{h} 
	+ (\frac{\boldsymbol{x}}{\boldsymbol{\sigma}^2})^T \boldsymbol{W}\boldsymbol{h}}}
	{\frac{1}{Z_{GB}} e^{-\vert\vert\frac{\boldsymbol{x} -\boldsymbol{a}}{2\boldsymbol{\sigma}}\vert\vert^2}
	\prod_j^N (1 + e^{b_j + (\frac{\boldsymbol{x}}{\boldsymbol{\sigma}^2})^T \boldsymbol{w}_{\ast j}} ) }
	\nonumber \\
	=& \prod_j^N \frac{e^{(b_j + (\frac{\boldsymbol{x}}{\boldsymbol{\sigma}^2})^T \boldsymbol{w}_{\ast j})h_j } }
	{1 + e^{b_j + (\frac{\boldsymbol{x}}{\boldsymbol{\sigma}^2})^T \boldsymbol{w}_{\ast j}}} \nonumber \\
	=& \prod_j^N p_{GB} (h_j|\boldsymbol{x}).
\label{_auto82}
\end{align}
$$

<p>The conditional probability of a binary hidden unit \( h_j \) being on or off again takes the form of a sigmoid function</p>

$$
\begin{align}
	p_{GB} (h_j =1 | \boldsymbol{x}) =& \frac{e^{b_j + (\frac{\boldsymbol{x}}{\boldsymbol{\sigma}^2})^T \boldsymbol{w}_{\ast j} } }
	{1 + e^{b_j + (\frac{\boldsymbol{x}}{\boldsymbol{\sigma}^2})^T \boldsymbol{w}_{\ast j}}} \nonumber \\
	=& \frac{1}{1 + e^{-b_j - (\frac{\boldsymbol{x}}{\boldsymbol{\sigma}^2})^T \boldsymbol{w}_{\ast j}}} 
\label{_auto83}\\
	p_{GB} (h_j =0 | \boldsymbol{x}) =&
	\frac{1}{1 + e^{b_j +(\frac{\boldsymbol{x}}{\boldsymbol{\sigma}^2})^T \boldsymbol{w}_{\ast j}}} .
\label{_auto84}
\end{align}
$$

<p>The conditional probability of the continuous \( \boldsymbol{x} \) now has another form, however.</p>

$$
\begin{align}
	p_{GB} (\boldsymbol{x}|\boldsymbol{h})
	=& \frac{p_{GB} (\boldsymbol{x}, \boldsymbol{h})}{p_{GB} (\boldsymbol{h})} \nonumber \\
	=& \frac{\frac{1}{Z_{GB}} e^{-\vert\vert\frac{\boldsymbol{x} -\boldsymbol{a}}{2\boldsymbol{\sigma}}\vert\vert^2 + \boldsymbol{b}^T \boldsymbol{h} 
	+ (\frac{\boldsymbol{x}}{\boldsymbol{\sigma}^2})^T \boldsymbol{W}\boldsymbol{h}}}
	{\frac{1}{Z_{GB}} e^{\boldsymbol{b}^T \boldsymbol{h}} \prod_i^M
	\sqrt{2\pi \sigma_i^2}
	e^{\frac{2a_i \boldsymbol{w}_{i\ast}^T \boldsymbol{h} +(\boldsymbol{w}_{i\ast}^T \boldsymbol{h})^2 }{2\sigma_i^2}}}
	\nonumber \\
	=& \prod_i^M \frac{1}{\sqrt{2\pi \sigma_i^2}}
	\frac{e^{- \frac{(x_i - a_i)^2}{2\sigma_i^2} + \frac{x_i \boldsymbol{w}_{i\ast}^T \boldsymbol{h}}{2\sigma_i^2} }}
	{e^{\frac{2a_i \boldsymbol{w}_{i\ast}^T \boldsymbol{h} +(\boldsymbol{w}_{i\ast}^T \boldsymbol{h})^2 }{2\sigma_i^2}}}
	\nonumber \\
	=& \prod_i^M \frac{1}{\sqrt{2\pi \sigma_i^2}}
	\frac{e^{-\frac{x_i^2 - 2a_i x_i + a_i^2 - 2x_i \boldsymbol{w}_{i\ast}^T\boldsymbol{h} }{2\sigma_i^2} } }
	{e^{\frac{2a_i \boldsymbol{w}_{i\ast}^T \boldsymbol{h} +(\boldsymbol{w}_{i\ast}^T \boldsymbol{h})^2 }{2\sigma_i^2}}}
	\nonumber \\
	=& \prod_i^M \frac{1}{\sqrt{2\pi \sigma_i^2}}
	e^{- \frac{x_i^2 - 2a_i x_i + a_i^2 - 2x_i \boldsymbol{w}_{i\ast}^T\boldsymbol{h}
	+ 2a_i \boldsymbol{w}_{i\ast}^T \boldsymbol{h} +(\boldsymbol{w}_{i\ast}^T \boldsymbol{h})^2}
	{2\sigma_i^2} }
	\nonumber \\
	=& \prod_i^M \frac{1}{\sqrt{2\pi \sigma_i^2}}
	e^{ - \frac{(x_i - b_i - \boldsymbol{w}_{i\ast}^T \boldsymbol{h})^2}{2\sigma_i^2}} \nonumber \\
	=& \prod_i^M \mathcal{N}
	(x_i | b_i + \boldsymbol{w}_{i\ast}^T \boldsymbol{h}, \sigma_i^2) 
\label{_auto85}\\
	\Rightarrow p_{GB} (x_i|\boldsymbol{h}) =& \mathcal{N}
	(x_i | b_i + \boldsymbol{w}_{i\ast}^T \boldsymbol{h}, \sigma_i^2) .
\label{_auto86}
\end{align}
$$

<p>The form of these conditional probabilities explains the name
"Gaussian" and the form of the Gaussian-binary energy function. We see
that the conditional probability of \( x_i \) given \( \boldsymbol{h} \) is a normal
distribution with mean \( b_i + \boldsymbol{w}_{i\ast}^T \boldsymbol{h} \) and variance
\( \sigma_i^2 \).
</p>
<h2 id="neural-quantum-states">Neural Quantum States </h2>

<p>The wavefunction should be a probability amplitude depending on \( \boldsymbol{x} \). The RBM model is given by the joint distribution of \( \boldsymbol{x} \) and \( \boldsymbol{h} \)</p>
$$
\begin{align}
	F_{rbm}(\boldsymbol{x},\mathbf{h}) = \frac{1}{Z} e^{-\frac{1}{T_0}E(\boldsymbol{x},\mathbf{h})}
\label{_auto87}
\end{align}
$$

<p>To find the marginal distribution of \( \boldsymbol{x} \) we set:</p>

$$
\begin{align}
	F_{rbm}(\mathbf{x}) &= \sum_\mathbf{h} F_{rbm}(\mathbf{x}, \mathbf{h}) 
\label{_auto88}\\
				&= \frac{1}{Z}\sum_\mathbf{h} e^{-E(\mathbf{x}, \mathbf{h})}
\label{_auto89}
\end{align}
$$

<p>Now this is what we use to represent the wave function, calling it a neural-network quantum state (NQS)</p>

$$
\begin{align}
	\Psi (\mathbf{X}) &= F_{rbm}(\mathbf{x}) 
\label{_auto90}\\
	&= \frac{1}{Z}\sum_{\boldsymbol{h}} e^{-E(\mathbf{x}, \mathbf{h})} 
\label{_auto91}\\
	&= \frac{1}{Z} \sum_{\{h_j\}} e^{-\sum_i^M \frac{(x_i - a_i)^2}{2\sigma^2} + \sum_j^N b_j h_j + \sum_{i,j}^{M,N} \frac{x_i w_{ij} h_j}{\sigma^2}} 
\label{_auto92}\\
	&= \frac{1}{Z} e^{-\sum_i^M \frac{(x_i - a_i)^2}{2\sigma^2}} \prod_j^N (1 + e^{b_j + \sum_i^M \frac{x_i w_{ij}}{\sigma^2}}) 
\label{_auto93}\\
\label{_auto94}
\end{align}
$$

<p>The above wavefunction is the most general one because it allows for
complex valued wavefunctions. However it fundamentally changes the
probabilistic foundation of the RBM, because what is usually a
probability in the RBM framework is now a an amplitude. This means
that a lot of the theoretical framework usually used to interpret the
model, i.e. graphical models, conditional probabilities, and Markov
random fields, breaks down. If we assume the wavefunction to be
postive definite, however, we can use the RBM to represent the squared
wavefunction, and thereby a probability. This also makes it possible
to sample from the model using Gibbs sampling, because we can obtain
the conditional probabilities.
</p>

$$
\begin{align}
	|\Psi (\mathbf{X})|^2 &= F_{rbm}(\mathbf{X}) 
\label{_auto95}\\
	\Rightarrow \Psi (\mathbf{X}) &= \sqrt{F_{rbm}(\mathbf{X})} 
\label{_auto96}\\
	&= \frac{1}{\sqrt{Z}}\sqrt{\sum_{\{h_j\}} e^{-E(\mathbf{X}, \mathbf{h})}} 
\label{_auto97}\\
	&= \frac{1}{\sqrt{Z}} \sqrt{\sum_{\{h_j\}} e^{-\sum_i^M \frac{(X_i - a_i)^2}{2\sigma^2} + \sum_j^N b_j h_j + \sum_{i,j}^{M,N} \frac{X_i w_{ij} h_j}{\sigma^2}} }
\label{_auto98}\\
	&= \frac{1}{\sqrt{Z}} e^{-\sum_i^M \frac{(X_i - a_i)^2}{4\sigma^2}} \sqrt{\sum_{\{h_j\}} \prod_j^N e^{b_j h_j + \sum_i^M \frac{X_i w_{ij} h_j}{\sigma^2}}} 
\label{_auto99}\\
	&= \frac{1}{\sqrt{Z}} e^{-\sum_i^M \frac{(X_i - a_i)^2}{4\sigma^2}} \sqrt{\prod_j^N \sum_{h_j}  e^{b_j h_j + \sum_i^M \frac{X_i w_{ij} h_j}{\sigma^2}}} 
\label{_auto100}\\
	&= \frac{1}{\sqrt{Z}} e^{-\sum_i^M \frac{(X_i - a_i)^2}{4\sigma^2}} \prod_j^N \sqrt{e^0 + e^{b_j + \sum_i^M \frac{X_i w_{ij}}{\sigma^2}}} 
\label{_auto101}\\
	&= \frac{1}{\sqrt{Z}} e^{-\sum_i^M \frac{(X_i - a_i)^2}{4\sigma^2}} \prod_j^N \sqrt{1 + e^{b_j + \sum_i^M \frac{X_i w_{ij}}{\sigma^2}}} 
\label{_auto102}\\
\label{_auto103}
\end{align}
$$
<h3 id="cost-function">Cost function </h3>

<p>This is where we deviate from what is common in machine
learning. Rather than defining a cost function based on some dataset,
our cost function is the energy of the quantum mechanical system. From
the variational principle we know that minizing this energy should
lead to the ground state wavefunction. As stated previously the local
energy is given by
</p>

$$
\begin{align}
	E_L = \frac{1}{\Psi} \hat{\mathbf{H}} \Psi,
\label{_auto104}
\end{align}
$$

<p>and the gradient is</p>

$$
\begin{align}
	G_i = \frac{\partial \langle E_L \rangle}{\partial \alpha_i}
	= 2(\langle E_L \frac{1}{\Psi}\frac{\partial \Psi}{\partial \alpha_i} \rangle - \langle E_L \rangle \langle \frac{1}{\Psi}\frac{\partial \Psi}{\partial \alpha_i} \rangle ),
\label{_auto105}
\end{align}
$$

<p>where \( \alpha_i = a_1,...,a_M,b_1,...,b_N,w_{11},...,w_{MN} \).</p>

<p>We use that \( \frac{1}{\Psi}\frac{\partial \Psi}{\partial \alpha_i} 
	= \frac{\partial \ln{\Psi}}{\partial \alpha_i} \),
and find
</p>

$$
\begin{align}
	\ln{\Psi({\mathbf{X}})} &= -\ln{Z} - \sum_m^M \frac{(X_m - a_m)^2}{2\sigma^2}
	+ \sum_n^N \ln({1 + e^{b_n + \sum_i^M \frac{X_i w_{in}}{\sigma^2}})}.
\label{_auto106}
\end{align}
$$

<p>This gives</p>

$$
\begin{align}
	\frac{\partial }{\partial a_m} \ln\Psi
	&= 	\frac{1}{\sigma^2} (X_m - a_m) 
\label{_auto107}\\
	\frac{\partial }{\partial b_n} \ln\Psi
	&=
	\frac{1}{e^{-b_n-\frac{1}{\sigma^2}\sum_i^M X_i w_{in}} + 1} 
\label{_auto108}\\
	\frac{\partial }{\partial w_{mn}} \ln\Psi
	&= \frac{X_m}{\sigma^2(e^{-b_n-\frac{1}{\sigma^2}\sum_i^M X_i w_{in}} + 1)}.
\label{_auto109}
\end{align}
$$

<p>If \( \Psi = \sqrt{F_{rbm}} \) we have</p>

$$
\begin{align}
	\ln{\Psi({\mathbf{X}})} &= -\frac{1}{2}\ln{Z} - \sum_m^M \frac{(X_m - a_m)^2}{4\sigma^2}
	+ \frac{1}{2}\sum_n^N \ln({1 + e^{b_n + \sum_i^M \frac{X_i w_{in}}{\sigma^2}})},
\label{_auto110}
\end{align}
$$

<p>which results in</p>

$$
\begin{align}
	\frac{\partial }{\partial a_m} \ln\Psi
	&= 	\frac{1}{2\sigma^2} (X_m - a_m) 
\label{_auto111}\\
	\frac{\partial }{\partial b_n} \ln\Psi
	&=
	\frac{1}{2(e^{-b_n-\frac{1}{\sigma^2}\sum_i^M X_i w_{in}} + 1)} 
\label{_auto112}\\
	\frac{\partial }{\partial w_{mn}} \ln\Psi
	&= \frac{X_m}{2\sigma^2(e^{-b_n-\frac{1}{\sigma^2}\sum_i^M X_i w_{in}} + 1)}.
\label{_auto113}
\end{align}
$$

<p>Let us assume again that our Hamiltonian is </p>
$$
\begin{align}
	\hat{\mathbf{H}} = \sum_p^P (-\frac{1}{2}\nabla_p^2 + \frac{1}{2}\omega^2 r_p^2 ) + \sum_{p < q} \frac{1}{r_{pq}},
\label{_auto114}
\end{align}
$$

<p>where the first summation term represents the standard harmonic
oscillator part and the latter the repulsive interaction between two
electrons. Natural units (\( \hbar=c=e=m_e=1 \)) are used, and \( P \) is the
number of particles. This gives us the following expression for the
local energy (\( D \) being the number of dimensions)
</p>

$$
\begin{align}
	E_L &= \frac{1}{\Psi} \mathbf{H} \Psi 
\label{_auto115}\\
	&= \frac{1}{\Psi} (\sum_p^P (-\frac{1}{2}\nabla_p^2 + \frac{1}{2}\omega^2 r_p^2 ) + \sum_{p < q} \frac{1}{r_{pq}}) \Psi 
\label{_auto116}\\
	&= -\frac{1}{2}\frac{1}{\Psi} \sum_p^P \nabla_p^2 \Psi 
	+ \frac{1}{2}\omega^2 \sum_p^P  r_p^2  + \sum_{p < q} \frac{1}{r_{pq}} 
\label{_auto117}\\
	&= -\frac{1}{2}\frac{1}{\Psi} \sum_p^P \sum_d^D \frac{\partial^2 \Psi}{\partial x_{pd}^2} + \frac{1}{2}\omega^2 \sum_p^P  r_p^2  + \sum_{p < q} \frac{1}{r_{pq}} 
\label{_auto118}\\
	&= \frac{1}{2} \sum_p^P \sum_d^D (-(\frac{\partial}{\partial x_{pd}} \ln\Psi)^2 -\frac{\partial^2}{\partial x_{pd}^2} \ln\Psi + \omega^2 x_{pd}^2)  + \sum_{p < q} \frac{1}{r_{pq}}. 
\label{_auto119}\\
\label{_auto120}
\end{align}
$$

<p>Letting each visible node in the Boltzmann machine 
represent one coordinate of one particle, we obtain
</p>

$$
\begin{align}
	E_L &=
	\frac{1}{2} \sum_m^M (-(\frac{\partial}{\partial v_m} \ln\Psi)^2 -\frac{\partial^2}{\partial v_m^2} \ln\Psi + \omega^2 v_m^2)  + \sum_{p < q} \frac{1}{r_{pq}},
\label{_auto121}
\end{align}
$$

<p>where we have that</p>

$$
\begin{align}
	\frac{\partial}{\partial x_m} \ln\Psi
	&= - \frac{1}{\sigma^2}(x_m - a_m) + \frac{1}{\sigma^2} \sum_n^N \frac{w_{mn}}{e^{-b_n - \frac{1}{\sigma^2}\sum_i^M x_i w_{in}} + 1} 
\label{_auto122}\\
	\frac{\partial^2}{\partial x_m^2} \ln\Psi
	&= - \frac{1}{\sigma^2} + \frac{1}{\sigma^4}\sum_n^N \omega_{mn}^2 \frac{e^{b_n + \frac{1}{\sigma^2}\sum_i^M x_i w_{in}}}{(e^{b_n + \frac{1}{\sigma^2}\sum_i^M x_i w_{in}} + 1)^2}.
\label{_auto123}
\end{align}
$$

<p>We now have all the expressions neeeded to calculate the gradient of
the expected local energy with respect to the RBM parameters
\( \frac{\partial \langle E_L \rangle}{\partial \alpha_i} \).
</p>

<p>If we use \( \Psi = \sqrt{F_{rbm}} \) we obtain</p>
$$
\begin{align}
	\frac{\partial}{\partial x_m} \ln\Psi
	&= - \frac{1}{2\sigma^2}(x_m - a_m) + \frac{1}{2\sigma^2} \sum_n^N
 	\frac{w_{mn}}{e^{-b_n-\frac{1}{\sigma^2}\sum_i^M x_i w_{in}} + 1}
	
\label{_auto124}\\
	\frac{\partial^2}{\partial x_m^2} \ln\Psi
	&= - \frac{1}{2\sigma^2} + \frac{1}{2\sigma^4}\sum_n^N \omega_{mn}^2 \frac{e^{b_n + \frac{1}{\sigma^2}\sum_i^M x_i w_{in}}}{(e^{b_n + \frac{1}{\sigma^2}\sum_i^M x_i w_{in}} + 1)^2}.
\label{_auto125}
\end{align}
$$

<p>The difference between this equation and the previous one is that we multiply by a factor \( 1/2 \).</p>
<h2 id="python-version-for-the-two-non-interacting-particles">Python version for the two non-interacting particles </h2>


<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="cell border-box-sizing code_cell rendered">
  <div class="input">
    <div class="inner_cell">
      <div class="input_area">
        <div class="highlight" style="background: #eeeedd">
  <pre style="line-height: 125%;"><span style="color: #228B22"># 2-electron VMC code for 2dim quantum dot with importance sampling</span>
<span style="color: #228B22"># Using gaussian rng for new positions and Metropolis- Hastings </span>
<span style="color: #228B22"># Added restricted boltzmann machine method for dealing with the wavefunction</span>
<span style="color: #228B22"># RBM code based heavily off of:</span>
<span style="color: #228B22"># https://github.com/CompPhysics/ComputationalPhysics2/tree/gh-pages/doc/Programs/BoltzmannMachines/MLcpp/src/Pycode/ob</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">math</span> <span style="color: #8B008B; font-weight: bold">import</span> exp, sqrt
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">random</span> <span style="color: #8B008B; font-weight: bold">import</span> random, seed, normalvariate
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">mpl_toolkits.mplot3d</span> <span style="color: #8B008B; font-weight: bold">import</span> Axes3D
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">matplotlib</span> <span style="color: #8B008B; font-weight: bold">import</span> cm
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">matplotlib.ticker</span> <span style="color: #8B008B; font-weight: bold">import</span> LinearLocator, FormatStrFormatter
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">sys</span>



<span style="color: #228B22"># Trial wave function for the 2-electron quantum dot in two dims</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">WaveFunction</span>(r,a,b,w):
    sigma=<span style="color: #B452CD">1.0</span>
    sig2 = sigma**<span style="color: #B452CD">2</span>
    Psi1 = <span style="color: #B452CD">0.0</span>
    Psi2 = <span style="color: #B452CD">1.0</span>
    Q = Qfac(r,b,w)
    
    <span style="color: #8B008B; font-weight: bold">for</span> iq <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberParticles):
        <span style="color: #8B008B; font-weight: bold">for</span> ix <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
            Psi1 += (r[iq,ix]-a[iq,ix])**<span style="color: #B452CD">2</span>
            
    <span style="color: #8B008B; font-weight: bold">for</span> ih <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberHidden):
        Psi2 *= (<span style="color: #B452CD">1.0</span> + np.exp(Q[ih]))
        
    Psi1 = np.exp(-Psi1/(<span style="color: #B452CD">2</span>*sig2))

    <span style="color: #8B008B; font-weight: bold">return</span> Psi1*Psi2

<span style="color: #228B22"># Local energy  for the 2-electron quantum dot in two dims, using analytical local energy</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">LocalEnergy</span>(r,a,b,w):
    sigma=<span style="color: #B452CD">1.0</span>
    sig2 = sigma**<span style="color: #B452CD">2</span>
    locenergy = <span style="color: #B452CD">0.0</span>
    
    Q = Qfac(r,b,w)

    <span style="color: #8B008B; font-weight: bold">for</span> iq <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberParticles):
        <span style="color: #8B008B; font-weight: bold">for</span> ix <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
            sum1 = <span style="color: #B452CD">0.0</span>
            sum2 = <span style="color: #B452CD">0.0</span>
            <span style="color: #8B008B; font-weight: bold">for</span> ih <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberHidden):
                sum1 += w[iq,ix,ih]/(<span style="color: #B452CD">1</span>+np.exp(-Q[ih]))
                sum2 += w[iq,ix,ih]**<span style="color: #B452CD">2</span> * np.exp(Q[ih]) / (<span style="color: #B452CD">1.0</span> + np.exp(Q[ih]))**<span style="color: #B452CD">2</span>
    
            dlnpsi1 = -(r[iq,ix] - a[iq,ix]) /sig2 + sum1/sig2
            dlnpsi2 = -<span style="color: #B452CD">1</span>/sig2 + sum2/sig2**<span style="color: #B452CD">2</span>
            locenergy += <span style="color: #B452CD">0.5</span>*(-dlnpsi1*dlnpsi1 - dlnpsi2 + r[iq,ix]**<span style="color: #B452CD">2</span>)
            
    <span style="color: #8B008B; font-weight: bold">if</span>(interaction==<span style="color: #8B008B; font-weight: bold">True</span>):
        <span style="color: #8B008B; font-weight: bold">for</span> iq1 <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberParticles):
            <span style="color: #8B008B; font-weight: bold">for</span> iq2 <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(iq1):
                distance = <span style="color: #B452CD">0.0</span>
                <span style="color: #8B008B; font-weight: bold">for</span> ix <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
                    distance += (r[iq1,ix] - r[iq2,ix])**<span style="color: #B452CD">2</span>
                    
                locenergy += <span style="color: #B452CD">1</span>/sqrt(distance)
                
    <span style="color: #8B008B; font-weight: bold">return</span> locenergy

<span style="color: #228B22"># Derivate of wave function ansatz as function of variational parameters</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">DerivativeWFansatz</span>(r,a,b,w):
    
    sigma=<span style="color: #B452CD">1.0</span>
    sig2 = sigma**<span style="color: #B452CD">2</span>
    
    Q = Qfac(r,b,w)
    
    WfDer = np.empty((<span style="color: #B452CD">3</span>,),dtype=<span style="color: #658b00">object</span>)
    WfDer = [np.copy(a),np.copy(b),np.copy(w)]
    
    WfDer[<span style="color: #B452CD">0</span>] = (r-a)/sig2
    WfDer[<span style="color: #B452CD">1</span>] = <span style="color: #B452CD">1</span> / (<span style="color: #B452CD">1</span> + np.exp(-Q))
    
    <span style="color: #8B008B; font-weight: bold">for</span> ih <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberHidden):
        WfDer[<span style="color: #B452CD">2</span>][:,:,ih] = w[:,:,ih] / (sig2*(<span style="color: #B452CD">1</span>+np.exp(-Q[ih])))
            
    <span style="color: #8B008B; font-weight: bold">return</span>  WfDer

<span style="color: #228B22"># Setting up the quantum force for the two-electron quantum dot, recall that it is a vector</span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">QuantumForce</span>(r,a,b,w):

    sigma=<span style="color: #B452CD">1.0</span>
    sig2 = sigma**<span style="color: #B452CD">2</span>
    
    qforce = np.zeros((NumberParticles,Dimension), np.double)
    sum1 = np.zeros((NumberParticles,Dimension), np.double)
    
    Q = Qfac(r,b,w)
    
    <span style="color: #8B008B; font-weight: bold">for</span> ih <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberHidden):
        sum1 += w[:,:,ih]/(<span style="color: #B452CD">1</span>+np.exp(-Q[ih]))
    
    qforce = <span style="color: #B452CD">2</span>*(-(r-a)/sig2 + sum1/sig2)
    
    <span style="color: #8B008B; font-weight: bold">return</span> qforce
    
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">Qfac</span>(r,b,w):
    Q = np.zeros((NumberHidden), np.double)
    temp = np.zeros((NumberHidden), np.double)
    
    <span style="color: #8B008B; font-weight: bold">for</span> ih <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberHidden):
        temp[ih] = (r*w[:,:,ih]).sum()
        
    Q = b + temp
    
    <span style="color: #8B008B; font-weight: bold">return</span> Q
    
<span style="color: #228B22"># Computing the derivative of the energy and the energy </span>
<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">EnergyMinimization</span>(a,b,w):

    NumberMCcycles= <span style="color: #B452CD">10000</span>
    <span style="color: #228B22"># Parameters in the Fokker-Planck simulation of the quantum force</span>
    D = <span style="color: #B452CD">0.5</span>
    TimeStep = <span style="color: #B452CD">0.05</span>
    <span style="color: #228B22"># positions</span>
    PositionOld = np.zeros((NumberParticles,Dimension), np.double)
    PositionNew = np.zeros((NumberParticles,Dimension), np.double)
    <span style="color: #228B22"># Quantum force</span>
    QuantumForceOld = np.zeros((NumberParticles,Dimension), np.double)
    QuantumForceNew = np.zeros((NumberParticles,Dimension), np.double)

    <span style="color: #228B22"># seed for rng generator </span>
    seed()
    energy = <span style="color: #B452CD">0.0</span>
    DeltaE = <span style="color: #B452CD">0.0</span>

    EnergyDer = np.empty((<span style="color: #B452CD">3</span>,),dtype=<span style="color: #658b00">object</span>)
    DeltaPsi = np.empty((<span style="color: #B452CD">3</span>,),dtype=<span style="color: #658b00">object</span>)
    DerivativePsiE = np.empty((<span style="color: #B452CD">3</span>,),dtype=<span style="color: #658b00">object</span>)
    EnergyDer = [np.copy(a),np.copy(b),np.copy(w)]
    DeltaPsi = [np.copy(a),np.copy(b),np.copy(w)]
    DerivativePsiE = [np.copy(a),np.copy(b),np.copy(w)]
    <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #B452CD">3</span>): EnergyDer[i].fill(<span style="color: #B452CD">0.0</span>)
    <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #B452CD">3</span>): DeltaPsi[i].fill(<span style="color: #B452CD">0.0</span>)
    <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #B452CD">3</span>): DerivativePsiE[i].fill(<span style="color: #B452CD">0.0</span>)

    
    <span style="color: #228B22">#Initial position</span>
    <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberParticles):
        <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
            PositionOld[i,j] = normalvariate(<span style="color: #B452CD">0.0</span>,<span style="color: #B452CD">1.0</span>)*sqrt(TimeStep)
    wfold = WaveFunction(PositionOld,a,b,w)
    QuantumForceOld = QuantumForce(PositionOld,a,b,w)

    <span style="color: #228B22">#Loop over MC MCcycles</span>
    <span style="color: #8B008B; font-weight: bold">for</span> MCcycle <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberMCcycles):
        <span style="color: #228B22">#Trial position moving one particle at the time</span>
        <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(NumberParticles):
            <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
                PositionNew[i,j] = PositionOld[i,j]+normalvariate(<span style="color: #B452CD">0.0</span>,<span style="color: #B452CD">1.0</span>)*sqrt(TimeStep)+\
                                       QuantumForceOld[i,j]*TimeStep*D
            wfnew = WaveFunction(PositionNew,a,b,w)
            QuantumForceNew = QuantumForce(PositionNew,a,b,w)
            
            GreensFunction = <span style="color: #B452CD">0.0</span>
            <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
                GreensFunction += <span style="color: #B452CD">0.5</span>*(QuantumForceOld[i,j]+QuantumForceNew[i,j])*\
                                      (D*TimeStep*<span style="color: #B452CD">0.5</span>*(QuantumForceOld[i,j]-QuantumForceNew[i,j])-\
                                      PositionNew[i,j]+PositionOld[i,j])
      
            GreensFunction = exp(GreensFunction)
            ProbabilityRatio = GreensFunction*wfnew**<span style="color: #B452CD">2</span>/wfold**<span style="color: #B452CD">2</span>
            <span style="color: #228B22">#Metropolis-Hastings test to see whether we accept the move</span>
            <span style="color: #8B008B; font-weight: bold">if</span> random() &lt;= ProbabilityRatio:
                <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(Dimension):
                    PositionOld[i,j] = PositionNew[i,j]
                    QuantumForceOld[i,j] = QuantumForceNew[i,j]
                wfold = wfnew
        <span style="color: #228B22">#print(&quot;wf new:        &quot;, wfnew)</span>
        <span style="color: #228B22">#print(&quot;force on 1 new:&quot;, QuantumForceNew[0,:])</span>
        <span style="color: #228B22">#print(&quot;pos of 1 new:  &quot;, PositionNew[0,:])</span>
        <span style="color: #228B22">#print(&quot;force on 2 new:&quot;, QuantumForceNew[1,:])</span>
        <span style="color: #228B22">#print(&quot;pos of 2 new:  &quot;, PositionNew[1,:])</span>
        DeltaE = LocalEnergy(PositionOld,a,b,w)
        DerPsi = DerivativeWFansatz(PositionOld,a,b,w)
        
        DeltaPsi[<span style="color: #B452CD">0</span>] += DerPsi[<span style="color: #B452CD">0</span>]
        DeltaPsi[<span style="color: #B452CD">1</span>] += DerPsi[<span style="color: #B452CD">1</span>]
        DeltaPsi[<span style="color: #B452CD">2</span>] += DerPsi[<span style="color: #B452CD">2</span>]
        
        energy += DeltaE

        DerivativePsiE[<span style="color: #B452CD">0</span>] += DerPsi[<span style="color: #B452CD">0</span>]*DeltaE
        DerivativePsiE[<span style="color: #B452CD">1</span>] += DerPsi[<span style="color: #B452CD">1</span>]*DeltaE
        DerivativePsiE[<span style="color: #B452CD">2</span>] += DerPsi[<span style="color: #B452CD">2</span>]*DeltaE
            
    <span style="color: #228B22"># We calculate mean values</span>
    energy /= NumberMCcycles
    DerivativePsiE[<span style="color: #B452CD">0</span>] /= NumberMCcycles
    DerivativePsiE[<span style="color: #B452CD">1</span>] /= NumberMCcycles
    DerivativePsiE[<span style="color: #B452CD">2</span>] /= NumberMCcycles
    DeltaPsi[<span style="color: #B452CD">0</span>] /= NumberMCcycles
    DeltaPsi[<span style="color: #B452CD">1</span>] /= NumberMCcycles
    DeltaPsi[<span style="color: #B452CD">2</span>] /= NumberMCcycles
    EnergyDer[<span style="color: #B452CD">0</span>]  = <span style="color: #B452CD">2</span>*(DerivativePsiE[<span style="color: #B452CD">0</span>]-DeltaPsi[<span style="color: #B452CD">0</span>]*energy)
    EnergyDer[<span style="color: #B452CD">1</span>]  = <span style="color: #B452CD">2</span>*(DerivativePsiE[<span style="color: #B452CD">1</span>]-DeltaPsi[<span style="color: #B452CD">1</span>]*energy)
    EnergyDer[<span style="color: #B452CD">2</span>]  = <span style="color: #B452CD">2</span>*(DerivativePsiE[<span style="color: #B452CD">2</span>]-DeltaPsi[<span style="color: #B452CD">2</span>]*energy)
    <span style="color: #8B008B; font-weight: bold">return</span> energy, EnergyDer


<span style="color: #228B22">#Here starts the main program with variable declarations</span>
NumberParticles = <span style="color: #B452CD">2</span>
Dimension = <span style="color: #B452CD">2</span>
NumberHidden = <span style="color: #B452CD">2</span>

interaction=<span style="color: #8B008B; font-weight: bold">False</span>

<span style="color: #228B22"># guess for parameters</span>
a=np.random.normal(loc=<span style="color: #B452CD">0.0</span>, scale=<span style="color: #B452CD">0.001</span>, size=(NumberParticles,Dimension))
b=np.random.normal(loc=<span style="color: #B452CD">0.0</span>, scale=<span style="color: #B452CD">0.001</span>, size=(NumberHidden))
w=np.random.normal(loc=<span style="color: #B452CD">0.0</span>, scale=<span style="color: #B452CD">0.001</span>, size=(NumberParticles,Dimension,NumberHidden))
<span style="color: #228B22"># Set up iteration using stochastic gradient method</span>
Energy = <span style="color: #B452CD">0</span>
EDerivative = np.empty((<span style="color: #B452CD">3</span>,),dtype=<span style="color: #658b00">object</span>)
EDerivative = [np.copy(a),np.copy(b),np.copy(w)]
<span style="color: #228B22"># Learning rate eta, max iterations, need to change to adaptive learning rate</span>
eta = <span style="color: #B452CD">0.001</span>
MaxIterations = <span style="color: #B452CD">50</span>
<span style="color: #658b00">iter</span> = <span style="color: #B452CD">0</span>
np.seterr(invalid=<span style="color: #CD5555">&#39;raise&#39;</span>)
Energies = np.zeros(MaxIterations)
EnergyDerivatives1 = np.zeros(MaxIterations)
EnergyDerivatives2 = np.zeros(MaxIterations)

<span style="color: #8B008B; font-weight: bold">while</span> <span style="color: #658b00">iter</span> &lt; MaxIterations:
    Energy, EDerivative = EnergyMinimization(a,b,w)
    agradient = EDerivative[<span style="color: #B452CD">0</span>]
    bgradient = EDerivative[<span style="color: #B452CD">1</span>]
    wgradient = EDerivative[<span style="color: #B452CD">2</span>]
    a -= eta*agradient
    b -= eta*bgradient 
    w -= eta*wgradient 
    Energies[<span style="color: #658b00">iter</span>] = Energy
    <span style="color: #658b00">print</span>(<span style="color: #CD5555">&quot;Energy:&quot;</span>,Energy)
    <span style="color: #228B22">#EnergyDerivatives1[iter] = EDerivative[0] </span>
    <span style="color: #228B22">#EnergyDerivatives2[iter] = EDerivative[1]</span>
    <span style="color: #228B22">#EnergyDerivatives3[iter] = EDerivative[2] </span>


    <span style="color: #658b00">iter</span> += <span style="color: #B452CD">1</span>

<span style="color: #228B22">#nice printout with Pandas</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">pandas</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">pd</span>
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">pandas</span> <span style="color: #8B008B; font-weight: bold">import</span> DataFrame
pd.set_option(<span style="color: #CD5555">&#39;max_columns&#39;</span>, <span style="color: #B452CD">6</span>)
data ={<span style="color: #CD5555">&#39;Energy&#39;</span>:Energies}<span style="color: #228B22">#,&#39;A Derivative&#39;:EnergyDerivatives1,&#39;B Derivative&#39;:EnergyDerivatives2,&#39;Weights Derivative&#39;:EnergyDerivatives3}</span>

frame = pd.DataFrame(data)
<span style="color: #658b00">print</span>(frame)
</pre>
</div>
      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output">
      <div class="output_area">
        <div class="output_subarea output_stream output_stdout output_text">          
        </div>
      </div>
    </div>
  </div>
</div>


<!-- ------------------- end of main content --------------- -->
</body>
</html>

