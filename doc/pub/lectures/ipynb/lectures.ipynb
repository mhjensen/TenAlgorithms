{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb098917",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html lectures.do.txt --no_mako -->\n",
    "<!-- dom:TITLE: Computational Physics  Lectures: Ten algorithms for quantum mechanical systems -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b34515",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Computational Physics  Lectures: Ten algorithms for quantum mechanical systems\n",
    "**Morten Hjorth-Jensen**, Department of Physics and Center for Computing in Science Education, University of Oslo, Oslo, Norway and Department of Physics and Astronomy and Facility for Rare Isotope Beams, East Lansing, Michigan State University, USA\n",
    "\n",
    "Date: **Oct 11, 2022**\n",
    "\n",
    "Copyright 1999-2022, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e97accd",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Overview\n",
    "In this contribution we outline central algorithms for studying\n",
    "quantum mechanical systems, with an emphasis on both computational and\n",
    "pedagogical aspects. Using simple systems that allow for analytical\n",
    "solutions, we show how one can move from linear algebra and eigenvalue\n",
    "algorithms using for example full configuration interaction theory, to\n",
    "stochastic methods like variational and Diffusion Monte Carlo\n",
    "approaches and finally, how we can use Monte Carlo methods together\n",
    "with deep learning algorithms. Along this journey we will present ten\n",
    "central algorithms which have changed considerably the way we study\n",
    "interacting many-particle systems.  These algorithms span from\n",
    "Householder’s famous transformation of matrices, via iterative\n",
    "eigenvalue solvers to neural networks and automatic differentiation\n",
    "for optimizing multidimensional functions. Codes and jupyter-notebooks\n",
    "are provided, allowing thereby the reader to experiment and practice\n",
    "the various methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb56db",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Algorithms\n",
    " * Linear algebra and eigenvalue problems\n",
    "\n",
    "a. Houselholder's transformation\n",
    "\n",
    "b. Jacobi/Givens rotations\n",
    "\n",
    "c. Iterative methods, Lanczsos' method\n",
    "\n",
    " * Monte Carlo methods\n",
    "\n",
    "a. Variational Monte Carlo\n",
    "\n",
    "b. Metropolis-Hastings algorithm\n",
    "\n",
    " * Optimization problems\n",
    "\n",
    "a. Gradient descent and steepest gradient descent\n",
    "\n",
    "b. Adaptive methods\n",
    "\n",
    " * Automatic differentiation\n",
    "\n",
    " * Deep learning\n",
    "\n",
    "a. Neural Networks\n",
    "\n",
    "b. Reduced Boltzmann machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cea602",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Eigenvalue problems, basic definitions\n",
    "\n",
    "Let us consider the matrix $\\mathbf{A}$ of dimension $n$. The eigenvalues of\n",
    "$\\mathbf{A}$ are defined through the matrix equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5b1874",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{A}\\mathbf{x}^{(\\nu)} = \\lambda^{(\\nu)}\\mathbf{x}^{(\\nu)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1797639",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\lambda^{(\\nu)}$ are the eigenvalues and $\\mathbf{x}^{(\\nu)}$ the\n",
    "corresponding eigenvectors.\n",
    "Unless otherwise stated, when we use the wording eigenvector we mean the\n",
    "right eigenvector. The left eigenvalue problem is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d657e51f",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{x}^{(\\nu)}_L\\mathbf{A} = \\lambda^{(\\nu)}\\mathbf{x}^{(\\nu)}_L\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e20c02",
   "metadata": {
    "editable": true
   },
   "source": [
    "The above right eigenvector problem is equivalent to a set of $n$ equations with $n$ unknowns\n",
    "$x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0720ef0f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Eigenvalue problems, basic definitions\n",
    "\n",
    "The eigenvalue problem can be rewritten as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62abac14",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\left( \\mathbf{A}-\\lambda^{(\\nu)} \\mathbf{I} \\right) \\mathbf{x}^{(\\nu)} = 0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f034dbb8",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $\\mathbf{I}$ being the unity matrix. This equation provides\n",
    "a solution to the problem if and only if the determinant\n",
    "is zero, namely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc844933",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\left| \\mathbf{A}-\\lambda^{(\\nu)}\\mathbf{I}\\right| = 0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ae75b5",
   "metadata": {
    "editable": true
   },
   "source": [
    "which in turn means that the determinant is a polynomial\n",
    "of degree $n$ in $\\lambda$ and in general we will have \n",
    "$n$ distinct zeros. \n",
    "\n",
    "The eigenvalues of a matrix \n",
    "$\\mathbf{A}\\in {\\mathbb{C}}^{n\\times n}$\n",
    "are thus the $n$ roots of its characteristic polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e454e9",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "P(\\lambda) = det(\\lambda\\mathbf{I}-\\mathbf{A}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196b4c3e",
   "metadata": {
    "editable": true
   },
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c312716",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "P(\\lambda)= \\prod_{i=1}^{n}\\left(\\lambda_i-\\lambda\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf86d908",
   "metadata": {
    "editable": true
   },
   "source": [
    "The set of these roots is called the spectrum and is denoted as\n",
    "$\\lambda(\\mathbf{A})$.\n",
    "If $\\lambda(\\mathbf{A})=\\left\\{\\lambda_1,\\lambda_2,\\dots ,\\lambda_n\\right\\}$ then we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f551012c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "det(\\mathbf{A})= \\lambda_1\\lambda_2\\dots\\lambda_n,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d714adfe",
   "metadata": {
    "editable": true
   },
   "source": [
    "and if we define the trace of $\\mathbf{A}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75bf8b8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "Tr(\\mathbf{A})=\\sum_{i=1}^n a_{ii}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ddf923",
   "metadata": {
    "editable": true
   },
   "source": [
    "then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c10456",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "Tr(\\mathbf{A})=\\lambda_1+\\lambda_2+\\dots+\\lambda_n.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b844cdb",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Abel-Ruffini Impossibility Theorem\n",
    "\n",
    "The *Abel-Ruffini* theorem (also known as Abel's impossibility theorem) \n",
    "states that there is no general solution in radicals to polynomial equations of degree five or higher.\n",
    "\n",
    "The content of this theorem is frequently misunderstood. It does not assert that higher-degree polynomial equations are unsolvable. \n",
    "In fact, if the polynomial has real or complex coefficients, and we allow complex solutions, then every polynomial equation has solutions; this is the fundamental theorem of algebra. Although these solutions cannot always be computed exactly with radicals, they can be computed to any desired degree of accuracy using numerical methods such as the Newton-Raphson method or Laguerre method, and in this way they are no different from solutions to polynomial equations of the second, third, or fourth degrees.\n",
    "\n",
    "The theorem only concerns the form that such a solution must take. The content of the theorem is \n",
    "that the solution of a higher-degree equation cannot in all cases be expressed in terms of the polynomial coefficients with a finite number of operations of addition, subtraction, multiplication, division and root extraction. Some polynomials of arbitrary degree, of which the simplest nontrivial example is the monomial equation $ax^n = b$, are always solvable with a radical.\n",
    "\n",
    "The *Abel-Ruffini* theorem says that there are some fifth-degree equations whose solution cannot be so expressed. \n",
    "The equation $x^5 - x + 1 = 0$ is an example. Some other fifth degree equations can be solved by radicals, \n",
    "for example $x^5 - x^4 - x + 1 = 0$. The precise criterion that distinguishes between those equations that can be solved \n",
    "by radicals and those that cannot was given by Galois and is now part of Galois theory: \n",
    "a polynomial equation can be solved by radicals if and only if its Galois group is a solvable group.\n",
    "\n",
    "Today, in the modern algebraic context, we say that second, third and fourth degree polynomial \n",
    "equations can always be solved by radicals because the symmetric groups $S_2, S_3$ and $S_4$ are solvable groups, \n",
    "whereas $S_n$ is not solvable for $n \\ge 5$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04a70b2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Eigenvalue problems, basic definitions\n",
    "\n",
    "In the present discussion we assume that our matrix is real and symmetric, that is \n",
    "$\\mathbf{A}\\in {\\mathbb{R}}^{n\\times n}$.\n",
    "The matrix $\\mathbf{A}$ has $n$ eigenvalues\n",
    "$\\lambda_1\\dots \\lambda_n$ (distinct or not). Let $\\mathbf{D}$ be the\n",
    "diagonal matrix with the eigenvalues on the diagonal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2722b965",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{D}=    \\left( \\begin{array}{ccccccc} \\lambda_1 & 0 & 0   & 0    & \\dots  &0     & 0 \\\\\n",
    "                                0 & \\lambda_2 & 0 & 0    & \\dots  &0     &0 \\\\\n",
    "                                0   & 0 & \\lambda_3 & 0  &0       &\\dots & 0\\\\\n",
    "                                \\dots  & \\dots & \\dots & \\dots  &\\dots      &\\dots & \\dots\\\\\n",
    "                                0   & \\dots & \\dots & \\dots  &\\dots       &\\lambda_{n-1} & \\\\\n",
    "                                0   & \\dots & \\dots & \\dots  &\\dots       &0 & \\lambda_n\n",
    "             \\end{array} \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab15c4e9",
   "metadata": {
    "editable": true
   },
   "source": [
    "If $\\mathbf{A}$ is real and symmetric then there exists a real orthogonal matrix $\\mathbf{S}$ such that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed7589",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{S}^T \\mathbf{A}\\mathbf{S}= \\mathrm{diag}(\\lambda_1,\\lambda_2,\\dots ,\\lambda_n),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adbfea2",
   "metadata": {
    "editable": true
   },
   "source": [
    "and for $j=1:n$ we have $\\mathbf{A}\\mathbf{S}(:,j) = \\lambda_j \\mathbf{S}(:,j)$.\n",
    "\n",
    "To obtain the eigenvalues of $\\mathbf{A}\\in {\\mathbb{R}}^{n\\times n}$,\n",
    "the strategy is to\n",
    "perform a series of similarity transformations on the original\n",
    "matrix $\\mathbf{A}$, in order to reduce it either into a  diagonal form as above\n",
    "or into a  tridiagonal form. \n",
    "\n",
    "We say that a matrix $\\mathbf{B}$ is a similarity\n",
    "transform  of  $\\mathbf{A}$ if"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c27489",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{B}= \\mathbf{S}^T \\mathbf{A}\\mathbf{S}, \\hspace{1cm} \\mathrm{where} \\hspace{1cm}  \\mathbf{S}^T\\mathbf{S}=\\mathbf{S}^{-1}\\mathbf{S} =\\mathbf{I}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347e9bb7",
   "metadata": {
    "editable": true
   },
   "source": [
    "The importance of a similarity transformation lies in the fact that\n",
    "the resulting matrix has the same\n",
    "eigenvalues, but the eigenvectors are in general different. \n",
    "\n",
    "To prove this we\n",
    "start with  the eigenvalue problem and a similarity transformed matrix $\\mathbf{B}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0b1db2",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{A}\\mathbf{x}=\\lambda\\mathbf{x} \\hspace{1cm} \\mathrm{and}\\hspace{1cm} \n",
    "    \\mathbf{B}= \\mathbf{S}^T \\mathbf{A}\\mathbf{S}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a15b89",
   "metadata": {
    "editable": true
   },
   "source": [
    "We multiply the first equation on the left by $\\mathbf{S}^T$ and insert\n",
    "$\\mathbf{S}^{T}\\mathbf{S} = \\mathbf{I}$ between $\\mathbf{A}$ and $\\mathbf{x}$. Then we get"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad00fd3a",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "   (\\mathbf{S}^T\\mathbf{A}\\mathbf{S})(\\mathbf{S}^T\\mathbf{x})=\\lambda\\mathbf{S}^T\\mathbf{x} ,\n",
    "\\label{_auto1} \\tag{1}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f6ef2",
   "metadata": {
    "editable": true
   },
   "source": [
    "which is the same as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5170b8a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{B} \\left ( \\mathbf{S}^T\\mathbf{x} \\right ) = \\lambda \\left (\\mathbf{S}^T\\mathbf{x}\\right ).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd43d6de",
   "metadata": {
    "editable": true
   },
   "source": [
    "The variable  $\\lambda$ is an eigenvalue of $\\mathbf{B}$ as well, but with\n",
    "eigenvector $\\mathbf{S}^T\\mathbf{x}$.\n",
    "\n",
    "The basic philosophy is to\n",
    " * Either apply subsequent similarity transformations (direct method) so that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaea326",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "   \\mathbf{S}_N^T\\dots \\mathbf{S}_1^T\\mathbf{A}\\mathbf{S}_1\\dots \\mathbf{S}_N=\\mathbf{D} ,\n",
    "\\label{_auto2} \\tag{2}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b60f20",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Or apply subsequent similarity transformations so that $\\mathbf{A}$ becomes tridiagonal (Householder) or upper/lower triangular (the *QR* method to be discussed later). \n",
    "\n",
    " * Thereafter, techniques for obtaining eigenvalues from tridiagonal matrices can be used.\n",
    "\n",
    " * Or use so-called power methods\n",
    "\n",
    " * Or use iterative methods (Krylov, Lanczos, Arnoldi). These methods are popular for huge matrix problems.\n",
    "\n",
    "One speaks normally of two main approaches to solving the eigenvalue problem.\n",
    " * The first is the formal method, involving determinants and the  characteristic polynomial. This proves how many eigenvalues  there are, and is the way most of you learned about how to solve the eigenvalue problem, but for matrices of dimensions greater than 2 or 3, it is rather impractical.\n",
    "\n",
    " * The other general approach is to use similarity or unitary tranformations  to reduce a matrix to diagonal form. This is normally done in two steps: first reduce to for example a *tridiagonal* form, and then to diagonal form. The main algorithms we will discuss in detail, Jacobi's and  Householder's  (so-called direct method) and Lanczos algorithms (an iterative method), follow this methodology. \n",
    "\n",
    "Direct or non-iterative methods  require for matrices of dimensionality $n\\times n$ typically $O(n^3)$ operations. These methods are normally called standard methods and are used for dimensionalities\n",
    "$n \\sim 10^5$ or smaller. A brief historical overview  \n",
    "\n",
    "<table class=\"dotable\" border=\"1\">\n",
    "<thead>\n",
    "<tr><th align=\"center\">Year</th> <th align=\"center\">    $n$     </th> <th align=\"center\">                 </th> </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td align=\"center\">   1950    </td> <td align=\"center\">   $n=20$          </td> <td align=\"center\">   (Wilkinson)          </td> </tr>\n",
    "<tr><td align=\"center\">   1965    </td> <td align=\"center\">   $n=200$         </td> <td align=\"center\">   (Forsythe et al.)    </td> </tr>\n",
    "<tr><td align=\"center\">   1980    </td> <td align=\"center\">   $n=2000$        </td> <td align=\"center\">   Linpack              </td> </tr>\n",
    "<tr><td align=\"center\">   1995    </td> <td align=\"center\">   $n=20000$       </td> <td align=\"center\">   Lapack               </td> </tr>\n",
    "<tr><td align=\"center\">   2017    </td> <td align=\"center\">   $n\\sim 10^5$    </td> <td align=\"center\">   Lapack               </td> </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "shows that in the course of 60 years the dimension that  direct diagonalization methods can handle  has increased by almost a factor of\n",
    "$10^4$. However, it pales beside the progress achieved by computer hardware, from flops to petaflops, a factor of almost $10^{15}$. We see clearly played out in history the $O(n^3)$ bottleneck  of direct matrix algorithms.\n",
    "\n",
    "Sloppily speaking, when  $n\\sim 10^4$ is cubed we have $O(10^{12})$ operations, which is smaller than the $10^{15}$ increase in flops.  \n",
    "\n",
    "If the matrix to diagonalize is large and sparse, direct methods simply become impractical, \n",
    "also because\n",
    "many of the direct methods tend to destroy sparsity. As a result large dense matrices may arise during the diagonalization procedure.  The idea behind iterative methods is to project the \n",
    "$n-$dimensional problem in smaller spaces, so-called Krylov subspaces. \n",
    "Given a matrix $\\mathbf{A}$ and a vector $\\mathbf{v}$, the associated Krylov sequences of vectors\n",
    "(and thereby subspaces) \n",
    "$\\mathbf{v}$, $\\mathbf{A}\\mathbf{v}$, $\\mathbf{A}^2\\mathbf{v}$, $\\mathbf{A}^3\\mathbf{v},\\dots$, represent\n",
    "successively larger Krylov subspaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f363f2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Discussion of Jacobi's method for eigenvalues\n",
    "\n",
    "Consider an  example of an ($n\\times n$) orthogonal transformation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598eb3be",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{S}=\n",
    " \\left( \n",
    "   \\begin{array}{cccccccc}\n",
    "   1  &    0  & \\dots &   0        &    0  & \\dots & 0 &   0       \\\\\n",
    "   0  &    1  & \\dots &   0        &    0  & \\dots & 0 &   0       \\\\\n",
    "\\dots & \\dots & \\dots & \\dots      & \\dots & \\dots & 0 & \\dots     \\\\ \n",
    "   0  &    0  & \\dots & \\cos\\theta  &    0  & \\dots & 0 & \\sin\\theta \\\\\n",
    "   0  &    0  & \\dots &   0        &    1  & \\dots & 0 &   0       \\\\\n",
    "\\dots & \\dots & \\dots & \\dots      & \\dots & \\dots & 1 & \\dots     \\\\\n",
    "   0  &    0  & \\dots &  -\\sin\\theta        &    0  & \\dots & 0 &   \\cos\\theta   \n",
    "   \\end{array}\n",
    " \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4171d7",
   "metadata": {
    "editable": true
   },
   "source": [
    "with property $\\mathbf{S^{T}} = \\mathbf{S^{-1}}$.\n",
    "It performs a plane rotation around an angle $\\theta$ in the Euclidean \n",
    "$n-$dimensional space. \n",
    "\n",
    "It means that its matrix elements that differ\n",
    "from zero are given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15bd602",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "s_{kk}= s_{ll}=\\cos\\theta, \n",
    "    s_{kl}=-s_{lk}= -\\sin\\theta, \n",
    "    s_{ii}=1\\hspace{0.5cm} i\\ne k \\hspace{0.5cm} i \\ne l,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2ab0d0",
   "metadata": {
    "editable": true
   },
   "source": [
    "A similarity transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1935d350",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{B}= \\mathbf{S}^T \\mathbf{A}\\mathbf{S},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1895ea2d",
   "metadata": {
    "editable": true
   },
   "source": [
    "results in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5edc0d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "b_{ik} =& a_{ik}\\cos\\theta - a_{il}\\sin\\theta , i \\ne k, i \\ne l \\\\\n",
    "b_{il} =& a_{il}\\cos\\theta + a_{ik}\\sin\\theta , i \\ne k, i \\ne l \\nonumber\\\\\n",
    "b_{kk} =& a_{kk}\\cos^2\\theta - 2a_{kl}\\cos\\theta \\sin\\theta +a_{ll}\\sin^2\\theta\\nonumber\\\\\n",
    "b_{ll} =& a_{ll}\\cos^2\\theta +2a_{kl}\\cos\\theta sin\\theta +a_{kk}\\sin^2\\theta\\nonumber\\\\\n",
    "b_{kl} =& (a_{kk}-a_{ll})\\cos\\theta \\sin\\theta +a_{kl}(\\cos^2\\theta-\\sin^2\\theta)\\nonumber \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1c1e7a",
   "metadata": {
    "editable": true
   },
   "source": [
    "The angle $\\theta$ is  arbitrary. The recipe is to choose  $\\theta$ so that all\n",
    "non-diagonal matrix elements $b_{kl}$ become zero.  \n",
    "\n",
    "The main idea is thus to reduce systematically the \n",
    "norm of the \n",
    "off-diagonal matrix elements  of a matrix  $\\mathbf{A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b068dfc9",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathrm{off}(\\mathbf{A}) = \\sqrt{\\sum_{i=1}^n\\sum_{j=1,j\\ne i}^n a_{ij}^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec17e87",
   "metadata": {
    "editable": true
   },
   "source": [
    "To demonstrate the algorithm, we consider the  simple $2\\times 2$  similarity transformation\n",
    "of the full matrix. The matrix is symmetric, we single out $ 1 \\le k < l \\le n$  and \n",
    "use the abbreviations $c=\\cos\\theta$ and $s=\\sin\\theta$ to obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4a1e48",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\left( \\begin{array}{cc} b_{kk} & 0 \\\\\n",
    "                          0 & b_{ll} \\\\\\end{array} \\right)  =  \\left( \\begin{array}{cc} c & -s \\\\\n",
    "                          s &c \\\\\\end{array} \\right)  \\left( \\begin{array}{cc} a_{kk} & a_{kl} \\\\\n",
    "                          a_{lk} &a_{ll} \\\\\\end{array} \\right) \\left( \\begin{array}{cc} c & s \\\\\n",
    "                          -s & c \\\\\\end{array} \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab38982",
   "metadata": {
    "editable": true
   },
   "source": [
    "We require that the non-diagonal matrix elements $b_{kl}=b_{lk}=0$, implying that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbdde6c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "a_{kl}(c^2-s^2)+(a_{kk}-a_{ll})cs = b_{kl} = 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64498e71",
   "metadata": {
    "editable": true
   },
   "source": [
    "If $a_{kl}=0$ one sees immediately that $\\cos\\theta = 1$ and $\\sin\\theta=0$.\n",
    "\n",
    "The Frobenius norm of an orthogonal transformation is always preserved. The Frobenius norm is defined\n",
    "as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df370e6",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathrm{norm}(\\mathbf{A})_F =  \\sqrt{\\sum_{i=1}^n\\sum_{j=1}^n |a_{ij}|^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4dab3f",
   "metadata": {
    "editable": true
   },
   "source": [
    "This means that for our $2\\times 2$ case  we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf31586",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "2a_{kl}^2+a_{kk}^2+a_{ll}^2 = b_{kk}^2+b_{ll}^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442dbecc",
   "metadata": {
    "editable": true
   },
   "source": [
    "which leads to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9813175e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathrm{off}(\\mathbf{B})^2 = \\mathrm{norm}(\\mathbf{B})_F^2-\\sum_{i=1}^nb_{ii}^2=\\mathrm{off}(\\mathbf{A})^2-2a_{kl}^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddff1917",
   "metadata": {
    "editable": true
   },
   "source": [
    "since"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fa50b5",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathrm{norm}(\\mathbf{B})_F^2-\\sum_{i=1}^nb_{ii}^2=\\mathrm{norm}(\\mathbf{A})_F^2-\\sum_{i=1}^na_{ii}^2+(a_{kk}^2+a_{ll}^2 -b_{kk}^2-b_{ll}^2).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fa85e7",
   "metadata": {
    "editable": true
   },
   "source": [
    "This results means that  the matrix $\\mathbf{A}$ moves closer to diagonal form  for each transformation.\n",
    "\n",
    "Defining the quantities $\\tan\\theta = t= s/c$ and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2453611",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\cot 2\\theta=\\tau = \\frac{a_{ll}-a_{kk}}{2a_{kl}},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e58eef",
   "metadata": {
    "editable": true
   },
   "source": [
    "we obtain the quadratic equation (using $\\cot 2\\theta=1/2(\\cot \\theta-\\tan\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822b5beb",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "t^2+2\\tau t-1= 0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea4fd69",
   "metadata": {
    "editable": true
   },
   "source": [
    "resulting in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aec8e5",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "t = -\\tau \\pm \\sqrt{1+\\tau^2},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764066c7",
   "metadata": {
    "editable": true
   },
   "source": [
    "and $c$ and $s$ are easily obtained via"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef0b98e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "c = \\frac{1}{\\sqrt{1+t^2}},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e376e63d",
   "metadata": {
    "editable": true
   },
   "source": [
    "and $s=tc$.  Convince yourself that we have $|\\theta| \\le \\pi/4$. This has the effect  \n",
    "of minimizing the difference between the matrices $\\mathbf{B}$ and $\\mathbf{A}$ since"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b17a4e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathrm{norm}(\\mathbf{B}-\\mathbf{A})_F^2=4(1-c)\\sum_{i=1,i\\ne k,l}^n(a_{ik}^2+a_{il}^2) +\\frac{2a_{kl}^2}{c^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0031585",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Choose a tolerance $\\epsilon$, making it a small number, typically $10^{-8}$ or smaller.\n",
    "\n",
    " * Setup a *while* test  where one compares the norm of the newly computed off-diagonal matrix elements  \\[ \\mathrm{off}(\\mathbf{A}) = \\sqrt{\\sum_{i=1}^n\\sum_{j=1,j\\ne i}^n a_{ij}^2}   >  \\epsilon. \\]\n",
    "\n",
    " * Now choose the matrix elements $a_{kl}$ so that we have those with largest value, that is $|a_{kl}|=\\mathrm{max}_{i\\ne j} |a_{ij}|$.\n",
    "\n",
    " * Compute thereafter $\\tau = (a_{ll}-a_{kk})/2a_{kl}$, $\\tan\\theta$, $\\cos\\theta$ and $\\sin\\theta$.\n",
    "\n",
    " * Compute thereafter the similarity transformation for this set of values $(k,l)$, obtaining the new matrix $\\mathbf{B}= \\mathbf{S}(k,l,\\theta)^T \\mathbf{A}\\mathbf{S}(k,l,\\theta)$.\n",
    "\n",
    " * Compute the new norm of the off-diagonal matrix elements and continue till you have satisfied $\\mathrm{off}(\\mathbf{B})  \\le  \\epsilon$\n",
    "\n",
    "The convergence rate of the Jacobi method is however poor, one needs typically $3n^2-5n^2$ rotations and each rotation \n",
    "requires $4n$ operations, resulting in a total of $12n^3-20n^3$ operations in order to zero out non-diagonal matrix elements.\n",
    "\n",
    "We specialize to a symmetric $3\\times 3 $ matrix $\\mathbf{A}$.\n",
    "We start the process as follows (assuming that $a_{23}=a_{32}$ is the largest non-diagonal)\n",
    "with $c=\\cos{\\theta}$ and $s=\\sin{\\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93e00f3",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{B} =\n",
    "      \\left( \\begin{array}{ccc} \n",
    "                1 & 0 & 0    \\\\\n",
    "                0 & c & -s     \\\\\n",
    "                0 & s & c\n",
    "             \\end{array} \\right)\\left( \\begin{array}{ccc} \n",
    "                a_{11} & a_{12} & a_{13}    \\\\\n",
    "                a_{21} & a_{22} & a_{23}     \\\\\n",
    "                a_{31} & a_{32} & a_{33}\n",
    "             \\end{array} \\right)\n",
    "              \\left( \\begin{array}{ccc} \n",
    "                1 & 0 & 0    \\\\\n",
    "                0 & c & s     \\\\\n",
    "                0 & -s & c\n",
    "             \\end{array} \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4635e5",
   "metadata": {
    "editable": true
   },
   "source": [
    "We will choose the angle $\\theta$ in order to have $a_{23}=a_{32}=0$.\n",
    "We get (symmetric matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9761d2",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{B} =\\left( \\begin{array}{ccc} \n",
    "                a_{11} & a_{12}c -a_{13}s& a_{12}s+a_{13}c    \\\\\n",
    "                a_{12}c -a_{13}s & a_{22}c^2+a_{33}s^2 -2a_{23}sc& (a_{22}-a_{33})sc +a_{23}(c^2-s^2)     \\\\\n",
    "                a_{12}s+a_{13}c & (a_{22}-a_{33})sc +a_{23}(c^2-s^2) & a_{22}s^2+a_{33}c^2 +2a_{23}sc\n",
    "             \\end{array} \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4527d2d2",
   "metadata": {
    "editable": true
   },
   "source": [
    "Note that $a_{11}$ is unchanged! As it should.\n",
    "\n",
    "We have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed180744",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{B} =\\left( \\begin{array}{ccc} \n",
    "                a_{11} & a_{12}c -a_{13}s& a_{12}s+a_{13}c    \\\\\n",
    "                a_{12}c -a_{13}s & a_{22}c^2+a_{33}s^2 -2a_{23}sc& (a_{22}-a_{33})sc +a_{23}(c^2-s^2)     \\\\\n",
    "                a_{12}s+a_{13}c & (a_{22}-a_{33})sc +a_{23}(c^2-s^2) & a_{22}s^2+a_{33}c^2 +2a_{23}sc\n",
    "             \\end{array} \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675099d7",
   "metadata": {
    "editable": true
   },
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e809a5",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "b_{11} =& a_{11} \\\\\n",
    "b_{12} =& a_{12}\\cos\\theta - a_{13}\\sin\\theta , 1 \\ne 2, 1 \\ne 3 \\\\\n",
    "b_{13} =& a_{13}\\cos\\theta + a_{12}\\sin\\theta , 1 \\ne 2, 1 \\ne 3 \\nonumber\\\\\n",
    "b_{22} =& a_{22}\\cos^2\\theta - 2a_{23}\\cos\\theta \\sin\\theta +a_{33}\\sin^2\\theta\\nonumber\\\\\n",
    "b_{33} =& a_{33}\\cos^2\\theta +2a_{23}\\cos\\theta \\sin\\theta +a_{22}\\sin^2\\theta\\nonumber\\\\\n",
    "b_{23} =& (a_{22}-a_{33})\\cos\\theta \\sin\\theta +a_{23}(\\cos^2\\theta-\\sin^2\\theta)\\nonumber \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3389c498",
   "metadata": {
    "editable": true
   },
   "source": [
    "We will fix the angle $\\theta$ so that $b_{23}=0$.\n",
    "\n",
    "We get then a new matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894466c8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{B} =\\left( \\begin{array}{ccc} \n",
    "                b_{11} & b_{12}& b_{13}    \\\\\n",
    "                b_{12}& b_{22}& 0    \\\\\n",
    "                b_{13}& 0& a_{33}\n",
    "             \\end{array} \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1af07f",
   "metadata": {
    "editable": true
   },
   "source": [
    "We repeat then assuming that $b_{12}$ is the largest non-diagonal matrix element and get a\n",
    "new matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfc4beb",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{C} =\n",
    "      \\left( \\begin{array}{ccc} \n",
    "                c & -s & 0    \\\\\n",
    "                s & c & 0     \\\\\n",
    "                0 & 0 & 1\n",
    "             \\end{array} \\right)\\left( \\begin{array}{ccc} \n",
    "                b_{11} & b_{12} & b_{13}    \\\\\n",
    "                b_{12} & b_{22} & 0     \\\\\n",
    "                b_{13} & 0 & b_{33}\n",
    "             \\end{array} \\right)\n",
    "              \\left( \\begin{array}{ccc} \n",
    "                c & s & 0    \\\\\n",
    "                -s & c & 0     \\\\\n",
    "                0 & 0 & 1\n",
    "             \\end{array} \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d6f96d",
   "metadata": {
    "editable": true
   },
   "source": [
    "We continue this process till all non-diagonal matrix elements are zero (ideally).\n",
    "You will notice that performing the above operations that the matrix element \n",
    "$b_{23}$ which was previous zero becomes different from zero.  This is one of the problems which slows\n",
    "down the jacobi procedure.\n",
    "\n",
    "The more general expression for the new matrix elements are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b57930a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "b_{ii} =& a_{ii}, i \\ne k, i \\ne l \\\\\n",
    "b_{ik} =& a_{ik}\\cos\\theta - a_{il}\\sin\\theta , i \\ne k, i \\ne l \\\\\n",
    "b_{il} =& a_{il}\\cos\\theta + a_{ik}\\sin\\theta , i \\ne k, i \\ne l \\nonumber\\\\\n",
    "b_{kk} =& a_{kk}\\cos^2\\theta - 2a_{kl}\\cos\\theta \\sin\\theta +a_{ll}\\sin^2\\theta\\nonumber\\\\\n",
    "b_{ll} =& a_{ll}\\cos^2\\theta +2a_{kl}\\cos\\theta \\sin\\theta +a_{kk}\\sin^2\\theta\\nonumber\\\\\n",
    "b_{kl} =& (a_{kk}-a_{ll})\\cos\\theta \\sin\\theta +a_{kl}(\\cos^2\\theta-\\sin^2\\theta)\\nonumber \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169e526c",
   "metadata": {
    "editable": true
   },
   "source": [
    "This is what we will need to code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b5a89b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Our system\n",
    "\n",
    "We are first interested in the solution of the radial part of Schroedinger's equation for one electron. This equation reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0694e06e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "-\\frac{\\hbar^2}{2 m} \\left ( \\frac{1}{r^2} \\frac{d}{dr} r^2\n",
    "  \\frac{d}{dr} - \\frac{l (l + 1)}{r^2} \\right )R(r) \n",
    "     + V(r) R(r) = E R(r).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eee621e",
   "metadata": {
    "editable": true
   },
   "source": [
    "In our case $V(r)$ is the harmonic oscillator potential $(1/2)kr^2$ with\n",
    "$k=m\\omega^2$ and $E$ is\n",
    "the energy of the harmonic oscillator in three dimensions.\n",
    "The oscillator frequency is $\\omega$ and the energies are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c72542",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "E_{nl}=  \\hbar \\omega \\left(2n+l+\\frac{3}{2}\\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd44bee",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $n=0,1,2,\\dots$ and $l=0,1,2,\\dots$.\n",
    "\n",
    "Since we have made a transformation to spherical coordinates it means that \n",
    "$r\\in [0,\\infty)$.  \n",
    "The quantum number\n",
    "$l$ is the orbital momentum of the electron.   Then we substitute $R(r) = (1/r) u(r)$ and obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c9d66a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "-\\frac{\\hbar^2}{2 m} \\frac{d^2}{dr^2} u(r) \n",
    "       + \\left ( V(r) + \\frac{l (l + 1)}{r^2}\\frac{\\hbar^2}{2 m}\n",
    "                                    \\right ) u(r)  = E u(r) .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4d8083",
   "metadata": {
    "editable": true
   },
   "source": [
    "The boundary conditions are $u(0)=0$ and $u(\\infty)=0$.\n",
    "\n",
    "We introduce a dimensionless variable $\\rho = (1/\\alpha) r$\n",
    "where $\\alpha$ is a constant with dimension length and get"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e220154",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "-\\frac{\\hbar^2}{2 m \\alpha^2} \\frac{d^2}{d\\rho^2} u(\\rho) \n",
    "       + \\left ( V(\\rho) + \\frac{l (l + 1)}{\\rho^2}\n",
    "         \\frac{\\hbar^2}{2 m\\alpha^2} \\right ) u(\\rho)  = E u(\\rho) .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3b434f",
   "metadata": {
    "editable": true
   },
   "source": [
    "For simplicity we set  $l=0$.\n",
    "Inserting $V(\\rho) = (1/2) k \\alpha^2\\rho^2$ we end up with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ba7c81",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "-\\frac{\\hbar^2}{2 m \\alpha^2} \\frac{d^2}{d\\rho^2} u(\\rho) \n",
    "       + \\frac{k}{2} \\alpha^2\\rho^2u(\\rho)  = E u(\\rho) .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797b27fb",
   "metadata": {
    "editable": true
   },
   "source": [
    "We multiply thereafter with $2m\\alpha^2/\\hbar^2$ on both sides and obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46550ea",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "-\\frac{d^2}{d\\rho^2} u(\\rho) \n",
    "       + \\frac{mk}{\\hbar^2} \\alpha^4\\rho^2u(\\rho)  = \\frac{2m\\alpha^2}{\\hbar^2}E u(\\rho) .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53bddf7",
   "metadata": {
    "editable": true
   },
   "source": [
    "We have thus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd1cce6",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "-\\frac{d^2}{d\\rho^2} u(\\rho) \n",
    "       + \\frac{mk}{\\hbar^2} \\alpha^4\\rho^2u(\\rho)  = \\frac{2m\\alpha^2}{\\hbar^2}E u(\\rho) .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b31c0a6",
   "metadata": {
    "editable": true
   },
   "source": [
    "The constant $\\alpha$ can now be fixed\n",
    "so that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5e94ad",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{mk}{\\hbar^2} \\alpha^4 = 1,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa8db8",
   "metadata": {
    "editable": true
   },
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35689a05",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\alpha = \\left(\\frac{\\hbar^2}{mk}\\right)^{1/4}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a61e6",
   "metadata": {
    "editable": true
   },
   "source": [
    "Defining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f77f229",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\lambda = \\frac{2m\\alpha^2}{\\hbar^2}E,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debf549d",
   "metadata": {
    "editable": true
   },
   "source": [
    "we can rewrite Schroedinger's equation as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1aaa94",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "-\\frac{d^2}{d\\rho^2} u(\\rho) + \\rho^2u(\\rho)  = \\lambda u(\\rho) .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df15efa",
   "metadata": {
    "editable": true
   },
   "source": [
    "This is the first equation to solve numerically. In three dimensions \n",
    "the eigenvalues for $l=0$ are \n",
    "$\\lambda_0=3,\\lambda_1=7,\\lambda_2=11,\\dots .$\n",
    "\n",
    "We use the by now standard\n",
    "expression for the second derivative of a function $u$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e6b94",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:diffoperation\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    u''=\\frac{u(\\rho+h) -2u(\\rho) +u(\\rho-h)}{h^2} +O(h^2),\n",
    "\\label{eq:diffoperation} \\tag{3}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa0c6b5",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $h$ is our step.\n",
    "Next we define minimum and maximum values for the variable $\\rho$,\n",
    "$\\rho_{\\mathrm{min}}=0$  and $\\rho_{\\mathrm{max}}$, respectively.\n",
    "You need to check your results for the energies against different values\n",
    "$\\rho_{\\mathrm{max}}$, since we cannot set\n",
    "$\\rho_{\\mathrm{max}}=\\infty$. \n",
    "\n",
    "With a given number of steps, $n_{\\mathrm{step}}$, we then \n",
    "define the step $h$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f61bf0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "h=\\frac{\\rho_{\\mathrm{max}}-\\rho_{\\mathrm{min}} }{n_{\\mathrm{step}}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceba890",
   "metadata": {
    "editable": true
   },
   "source": [
    "Define an arbitrary value of $\\rho$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4e44b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\rho_i= \\rho_{\\mathrm{min}} + ih \\hspace{1cm} i=0,1,2,\\dots , n_{\\mathrm{step}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8f0c07",
   "metadata": {
    "editable": true
   },
   "source": [
    "we can rewrite the Schr\\\"odinger equation for $\\rho_i$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560ba667",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "-\\frac{u(\\rho_i+h) -2u(\\rho_i) +u(\\rho_i-h)}{h^2}+\\rho_i^2u(\\rho_i)  = \\lambda u(\\rho_i),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b015f7ef",
   "metadata": {
    "editable": true
   },
   "source": [
    "or in  a more compact way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adac5db",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "-\\frac{u_{i+1} -2u_i +u_{i-1}}{h^2}+\\rho_i^2u_i=-\\frac{u_{i+1} -2u_i +u_{i-1} }{h^2}+V_iu_i  = \\lambda u_i,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc1f6c0",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $V_i=\\rho_i^2$ is the harmonic oscillator potential.\n",
    "\n",
    "Define first the diagonal matrix element"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17ed203",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "d_i=\\frac{2}{h^2}+V_i,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4784a00d",
   "metadata": {
    "editable": true
   },
   "source": [
    "and the non-diagonal matrix element"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861c850e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "e_i=-\\frac{1}{h^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1a6f56",
   "metadata": {
    "editable": true
   },
   "source": [
    "In this case the non-diagonal matrix elements are given by a mere constant. *All non-diagonal matrix elements are equal*.\n",
    "\n",
    "With these definitions the Schroedinger equation takes the following form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15218378",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "d_iu_i+e_{i-1}u_{i-1}+e_{i+1}u_{i+1}  = \\lambda u_i,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010036cd",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $u_i$ is unknown. We can write the \n",
    "latter equation as a matrix eigenvalue problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b9dd58",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:sematrix\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\left( \\begin{array}{ccccccc} d_1 & e_1 & 0   & 0    & \\dots  &0     & 0 \\\\\n",
    "                                e_1 & d_2 & e_2 & 0    & \\dots  &0     &0 \\\\\n",
    "                                0   & e_2 & d_3 & e_3  &0       &\\dots & 0\\\\\n",
    "                                \\dots  & \\dots & \\dots & \\dots  &\\dots      &\\dots & \\dots\\\\\n",
    "                                0   & \\dots & \\dots & \\dots  &\\dots       &d_{n_{\\mathrm{step}}-2} & e_{n_{\\mathrm{step}}-1}\\\\\n",
    "                                0   & \\dots & \\dots & \\dots  &\\dots       &e_{n_{\\mathrm{step}}-1} & d_{n_{\\mathrm{step}}-1}\n",
    "             \\end{array} \\right)      \\left( \\begin{array}{c} u_{1} \\\\\n",
    "                                                              u_{2} \\\\\n",
    "                                                              \\dots\\\\ \\dots\\\\ \\dots\\\\\n",
    "                                                              u_{n_{\\mathrm{step}}-1}\n",
    "             \\end{array} \\right)=\\lambda \\left( \\begin{array}{c} u_{1} \\\\\n",
    "                                                              u_{2} \\\\\n",
    "                                                              \\dots\\\\ \\dots\\\\ \\dots\\\\\n",
    "                                                              u_{n_{\\mathrm{step}}-1}\n",
    "             \\end{array} \\right) \n",
    "\\label{eq:sematrix} \\tag{4}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfd3d4e",
   "metadata": {
    "editable": true
   },
   "source": [
    "or if we wish to be more detailed, we can write the tridiagonal matrix as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5655c08",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:matrixse\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\left( \\begin{array}{ccccccc} \\frac{2}{h^2}+V_1 & -\\frac{1}{h^2} & 0   & 0    & \\dots  &0     & 0 \\\\\n",
    "                                -\\frac{1}{h^2} & \\frac{2}{h^2}+V_2 & -\\frac{1}{h^2} & 0    & \\dots  &0     &0 \\\\\n",
    "                                0   & -\\frac{1}{h^2} & \\frac{2}{h^2}+V_3 & -\\frac{1}{h^2}  &0       &\\dots & 0\\\\\n",
    "                                \\dots  & \\dots & \\dots & \\dots  &\\dots      &\\dots & \\dots\\\\\n",
    "                                0   & \\dots & \\dots & \\dots  &\\dots       &\\frac{2}{h^2}+V_{n_{\\mathrm{step}}-2} & -\\frac{1}{h^2}\\\\\n",
    "                                0   & \\dots & \\dots & \\dots  &\\dots       &-\\frac{1}{h^2} & \\frac{2}{h^2}+V_{n_{\\mathrm{step}}-1}\n",
    "             \\end{array} \\right)  \n",
    "\\label{eq:matrixse} \\tag{5} \n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7532e0",
   "metadata": {
    "editable": true
   },
   "source": [
    "Recall that the solutions are known via the boundary conditions at\n",
    "$i=n_{\\mathrm{step}}$ and at the other end point, that is for  $\\rho_0$.\n",
    "The solution is zero in both cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab54d987",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The Python code\n",
    "The code sets up the Hamiltonian matrix by defining the the minimun and maximum values of $r$ with a\n",
    "maximum value of integration points.  These are set in the initialization function. It plots the \n",
    "eigenfunctions of the three lowest eigenstates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd2fc26d",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#Program which solves the one-particle Schrodinger equation \n",
    "#for a potential specified in function\n",
    "#potential(). This example is for the harmonic oscillator in 3d\n",
    "\n",
    "from  matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "#Function for initialization of parameters\n",
    "def initialize():\n",
    "    RMin = 0.0\n",
    "    RMax = 10.0\n",
    "    lOrbital = 0\n",
    "    Dim = 400\n",
    "    return RMin, RMax, lOrbital, Dim\n",
    "# Here we set up the harmonic oscillator potential\n",
    "def potential(r):\n",
    "    return r*r\n",
    "\n",
    "#Get the boundary, orbital momentum and number of integration points\n",
    "RMin, RMax, lOrbital, Dim = initialize()\n",
    "\n",
    "#Initialize constants\n",
    "Step    = RMax/(Dim+1)\n",
    "DiagConst = 2.0 / (Step*Step)\n",
    "NondiagConst =  -1.0 / (Step*Step)\n",
    "OrbitalFactor = lOrbital * (lOrbital + 1.0)\n",
    "\n",
    "#Calculate array of potential values\n",
    "v = np.zeros(Dim)\n",
    "r = np.linspace(RMin,RMax,Dim)\n",
    "for i in range(Dim):\n",
    "    r[i] = RMin + (i+1) * Step;\n",
    "    v[i] = potential(r[i]) + OrbitalFactor/(r[i]*r[i]);\n",
    "\n",
    "#Setting up a tridiagonal matrix and finding eigenvectors and eigenvalues\n",
    "Hamiltonian = np.zeros((Dim,Dim))\n",
    "Hamiltonian[0,0] = DiagConst + v[0];\n",
    "Hamiltonian[0,1] = NondiagConst;\n",
    "for i in range(1,Dim-1):\n",
    "    Hamiltonian[i,i-1]  = NondiagConst;\n",
    "    Hamiltonian[i,i]    = DiagConst + v[i];\n",
    "    Hamiltonian[i,i+1]  = NondiagConst;\n",
    "Hamiltonian[Dim-1,Dim-2] = NondiagConst;\n",
    "Hamiltonian[Dim-1,Dim-1] = DiagConst + v[Dim-1];\n",
    "# diagonalize and obtain eigenvalues, not necessarily sorted\n",
    "EigValues, EigVectors = np.linalg.eig(Hamiltonian)\n",
    "# sort eigenvectors and eigenvalues\n",
    "permute = EigValues.argsort()\n",
    "EigValues = EigValues[permute]\n",
    "EigVectors = EigVectors[:,permute]\n",
    "# now plot the results for the three lowest lying eigenstates\n",
    "for i in range(3):\n",
    "    print (EigValues[i])\n",
    "FirstEigvector = EigVectors[:,0]\n",
    "SecondEigvector = EigVectors[:,1]\n",
    "ThirdEigvector = EigVectors[:,2]\n",
    "plt.plot(r, FirstEigvector**2 ,'b-',r, SecondEigvector**2 ,'g-',r, ThirdEigvector**2 ,'r-')\n",
    "plt.axis([0,4.6,0.0, 0.025])\n",
    "plt.xlabel(r'$r$')\n",
    "plt.ylabel(r'Radial probability $r^2|R(r)|^2$')\n",
    "plt.title(r'Radial probability distributions for three lowest-lying states')\n",
    "plt.savefig('eigenvector.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be32e18",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Two electrons that interact\n",
    "\n",
    "We are going to study two electrons in a harmonic oscillator well which\n",
    "also interact via a repulsive Coulomb interaction.\n",
    "Let us start with the single-electron equation written as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a893fb",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "-\\frac{\\hbar^2}{2 m} \\frac{d^2}{dr^2} u(r) \n",
    "       + \\frac{1}{2}k r^2u(r)  = E^{(1)} u(r),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca30f27",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $E^{(1)}$ stands for the energy with one electron only.\n",
    "For two electrons with no repulsive Coulomb interaction, we have the following \n",
    "Schroedinger equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59f0492",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\left(  -\\frac{\\hbar^2}{2 m} \\frac{d^2}{dr_1^2} -\\frac{\\hbar^2}{2 m} \\frac{d^2}{dr_2^2}+ \\frac{1}{2}k r_1^2+ \\frac{1}{2}k r_2^2\\right)u(r_1,r_2)  = E^{(2)} u(r_1,r_2) .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca915753",
   "metadata": {
    "editable": true
   },
   "source": [
    "Note that we deal with a two-electron wave function $u(r_1,r_2)$ and \n",
    "two-electron energy $E^{(2)}$.\n",
    "\n",
    "With no interaction this can be written out as the product of two\n",
    "single-electron wave functions, that is we have a solution on closed form.\n",
    "\n",
    "We introduce the relative coordinate $\\mathbf{r} = \\mathbf{r}_1-\\mathbf{r}_2$\n",
    "and the center-of-mass coordinate $\\mathbf{R} = 1/2(\\mathbf{r}_1+\\mathbf{r}_2)$.\n",
    "With these new coordinates, the radial Schroedinger equation reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9229d1b7",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\left(  -\\frac{\\hbar^2}{m} \\frac{d^2}{dr^2} -\\frac{\\hbar^2}{4 m} \\frac{d^2}{dR^2}+ \\frac{1}{4} k r^2+  kR^2\\right)u(r,R)  = E^{(2)} u(r,R).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b24108d",
   "metadata": {
    "editable": true
   },
   "source": [
    "The equations for $r$ and $R$ can be separated via the ansatz for the \n",
    "wave function $u(r,R) = \\psi(r)\\phi(R)$ and the energy is given by the sum\n",
    "of the relative energy $E_r$ and the center-of-mass energy $E_R$, that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1096e098",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "E^{(2)}=E_r+E_R.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d0c5e4",
   "metadata": {
    "editable": true
   },
   "source": [
    "We add then the repulsive Coulomb interaction between two electrons,\n",
    "namely a term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8e23d7",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "V(r_1,r_2) = \\frac{\\beta e^2}{|\\mathbf{r}_1-\\mathbf{r}_2|}=\\frac{\\beta e^2}{r},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db5a817",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $\\beta e^2=1.44$ eVnm.\n",
    "\n",
    "Adding this term, the $r$-dependent Schroedinger equation becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075db4ab",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\left(  -\\frac{\\hbar^2}{m} \\frac{d^2}{dr^2}+ \\frac{1}{4}k r^2+\\frac{\\beta e^2}{r}\\right)\\psi(r)  = E_r \\psi(r).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3526329",
   "metadata": {
    "editable": true
   },
   "source": [
    "This equation is similar to the one we had previously in parts (a) and (b) \n",
    "and we introduce\n",
    "again a dimensionless variable $\\rho = r/\\alpha$. Repeating the same\n",
    "steps, we arrive at"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cd6ea5",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "-\\frac{d^2}{d\\rho^2} \\psi(\\rho) \n",
    "       + \\frac{mk}{4\\hbar^2} \\alpha^4\\rho^2\\psi(\\rho)+\\frac{m\\alpha \\beta e^2}{\\rho\\hbar^2}\\psi(\\rho)  = \n",
    "\\frac{m\\alpha^2}{\\hbar^2}E_r \\psi(\\rho) .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6d41fc",
   "metadata": {
    "editable": true
   },
   "source": [
    "We want to manipulate this equation further to make it as similar to that in (a)\n",
    "as possible. We define a 'frequency'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1347e57c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\omega_r^2=\\frac{1}{4}\\frac{mk}{\\hbar^2} \\alpha^4,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69514b85",
   "metadata": {
    "editable": true
   },
   "source": [
    "and fix the constant $\\alpha$ by requiring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d318291c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{m\\alpha \\beta e^2}{\\hbar^2}=1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541b3266",
   "metadata": {
    "editable": true
   },
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d4bbfd",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\alpha = \\frac{\\hbar^2}{m\\beta e^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ff1b72",
   "metadata": {
    "editable": true
   },
   "source": [
    "Defining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a1ca6e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\lambda = \\frac{m\\alpha^2}{\\hbar^2}E,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdb2988",
   "metadata": {
    "editable": true
   },
   "source": [
    "we can rewrite Schroedinger's equation as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3583ce88",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "-\\frac{d^2}{d\\rho^2} \\psi(\\rho) + \\omega_r^2\\rho^2\\psi(\\rho) +\\frac{1}{\\rho}\\psi(\\rho) = \\lambda \\psi(\\rho).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9309cc",
   "metadata": {
    "editable": true
   },
   "source": [
    "We treat $\\omega_r$ as a parameter which reflects the strength of the oscillator potential.\n",
    "\n",
    "Here we will study the cases $\\omega_r = 0.01$, $\\omega_r = 0.5$, $\\omega_r =1$,\n",
    "and $\\omega_r = 5$   \n",
    "for the ground state only, that is the lowest-lying state.\n",
    "\n",
    "With no repulsive Coulomb interaction \n",
    "you should get a result which corresponds to \n",
    "the relative energy of a non-interacting system.   \n",
    "Make sure your results are \n",
    "stable as functions of $\\rho_{\\mathrm{max}}$ and the number of steps.\n",
    "\n",
    "We are only interested in the ground state with $l=0$. We omit the \n",
    "center-of-mass energy.\n",
    "\n",
    "For specific oscillator frequencies, the above equation has analytic answers,\n",
    "see the article by M. Taut, Phys. Rev. A 48, 3561 - 3566 (1993).\n",
    "The article can be retrieved from the following web address <http://prola.aps.org/abstract/PRA/v48/i5/p3561_1>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2dbbe8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Discussion of Householder's method for eigenvalues\n",
    "\n",
    "The drawbacks with Jacobi's method are rather obvious, with perhaps the most negative feature being the fact that we cannot tell * a priori* how many transformations are needed. Can we do better?  \n",
    "The answer to this is yes and is given by a clever algorithm outlined by Householder. It was ranked among the top ten algorithms in the previous century.  We will discuss this algorithm in more detail below.\n",
    "\n",
    "The first step  consists in finding\n",
    "an orthogonal  matrix $\\mathbf{S}$ which is the product of $(n-2)$ orthogonal matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b89a70a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{S}=\\mathbf{S}_1\\mathbf{S}_2\\dots\\mathbf{S}_{n-2},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee7a5f1",
   "metadata": {
    "editable": true
   },
   "source": [
    "each of which successively transforms one row and one column of $\\mathbf{A}$ into the \n",
    "required tridiagonal form. Only $n-2$ transformations are required, since the last two\n",
    "elements are already in tridiagonal form. \n",
    "\n",
    "In order to determine each $\\mathbf{S}_i$ let us\n",
    "see what happens after the first multiplication, namely,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01fa639",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{S}_1^T\\mathbf{A}\\mathbf{S}_1=    \\left( \\begin{array}{ccccccc} a_{11} & e_1 & 0   & 0    & \\dots  &0     & 0 \\\\\n",
    "                                e_1 & a'_{22} &a'_{23}  & \\dots    & \\dots  &\\dots &a'_{2n} \\\\\n",
    "                                0   & a'_{32} &a'_{33}  & \\dots    & \\dots  &\\dots &a'_{3n} \\\\\n",
    "                                0   & \\dots &\\dots & \\dots    & \\dots  &\\dots & \\\\\n",
    "                                0   & a'_{n2} &a'_{n3}  & \\dots    & \\dots  &\\dots &a'_{nn} \\\\\n",
    "             \\end{array} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8007b03d",
   "metadata": {
    "editable": true
   },
   "source": [
    "where the primed quantities represent a matrix $\\mathbf{A}'$ of dimension\n",
    "$n-1$ which will subsequentely be transformed by $\\mathbf{S}_2$.\n",
    "\n",
    "The factor  $e_1$ is a possibly non-vanishing element. The next\n",
    "transformation produced by $\\mathbf{S}_2$ has the same effect as  $\\mathbf{S}s$ but now on the submatirx $\\mathbf{A^{'}}$ only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae742d7b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\left (\\mathbf{S}_{1}\\mathbf{S}_{2} \\right )^{T} \\mathbf{A}\\mathbf{S}_{1} \\mathbf{S}_{2}\n",
    " = \\left( \\begin{array}{ccccccc} a_{11} & e_1 & 0   & 0    & \\dots  &0     & 0 \\\\\n",
    "                                e_1 & a'_{22} &e_2  & 0   & \\dots  &\\dots &0 \\\\\n",
    "                                0   & e_2 &a''_{33}  & \\dots    & \\dots  &\\dots &a''_{3n} \\\\\n",
    "                                0   & \\dots &\\dots & \\dots    & \\dots  &\\dots & \\\\\n",
    "                                0   & 0 &a''_{n3}  & \\dots    & \\dots  &\\dots &a''_{nn} \\\\\n",
    "             \\end{array} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71a1e00",
   "metadata": {
    "editable": true
   },
   "source": [
    "*Note that the effective size of the matrix on which we apply the transformation reduces for every new step. In the previous Jacobi method each similarity transformation is in principle performed on the full size of the original matrix*.\n",
    "\n",
    "After a series of such transformations, we end with a set of diagonal\n",
    "matrix elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487f6d31",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "a_{11}, a'_{22}, a''_{33}\\dots a^{n-1}_{nn},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62157a64",
   "metadata": {
    "editable": true
   },
   "source": [
    "and off-diagonal matrix elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbf3b7d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "e_1, e_2,e_3,  \\dots, e_{n-1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b451c2f",
   "metadata": {
    "editable": true
   },
   "source": [
    "The resulting matrix reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884f10ac",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{S}^{T} \\mathbf{A} \\mathbf{S} = \n",
    "    \\left( \\begin{array}{ccccccc} a_{11} & e_1 & 0   & 0    & \\dots  &0     & 0 \\\\\n",
    "                                e_1 & a'_{22} & e_2 & 0    & \\dots  &0     &0 \\\\\n",
    "                                0   & e_2 & a''_{33} & e_3  &0       &\\dots & 0\\\\\n",
    "                                \\dots  & \\dots & \\dots & \\dots  &\\dots      &\\dots & \\dots\\\\\n",
    "                                0   & \\dots & \\dots & \\dots  &\\dots       &a^{(n-1)}_{n-2} & e_{n-1}\\\\\n",
    "                                0   & \\dots & \\dots & \\dots  &\\dots       &e_{n-1} & a^{(n-1)}_{nn}\n",
    "             \\end{array} \\right) .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57a4b26",
   "metadata": {
    "editable": true
   },
   "source": [
    "It remains to find a recipe for determining the transformation $\\mathbf{S}_n$.\n",
    "We illustrate the method for $\\mathbf{S}_1$ which we assume takes the form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50928d10",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{S_{1}} = \\left( \\begin{array}{cc} 1 & \\mathbf{0^{T}} \\\\\n",
    "                              \\mathbf{0}& \\mathbf{P} \\end{array} \\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b068c4e2",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $\\mathbf{0^{T}}$ being a zero row vector, $\\mathbf{0^{T}} = \\{0,0,\\cdots\\}$\n",
    "of dimension $(n-1)$. The matrix $\\mathbf{P}$  is symmetric \n",
    "with dimension ($(n-1) \\times (n-1)$) satisfying\n",
    "$\\mathbf{P}^2=\\mathbf{I}$  and $\\mathbf{P}^T=\\mathbf{P}$. \n",
    "A possible choice which fullfils the latter two requirements is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e27eb",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{P}=\\mathbf{I}-2\\mathbf{u}\\mathbf{u}^T,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e9d9ee",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\mathbf{I}$ is the $(n-1)$ unity matrix and $\\mathbf{u}$ is an $n-1$\n",
    "column vector with norm $\\mathbf{u}^T\\mathbf{u}$ (inner product).\n",
    "\n",
    " Note that $\\mathbf{u}\\mathbf{u}^T$ is an outer product giving a\n",
    "matrix of dimension ($(n-1) \\times (n-1)$). \n",
    "Each matrix element of $\\mathbf{P}$ then reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f86cef",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "P_{ij}=\\delta_{ij}-2u_iu_j,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cd5997",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $i$ and $j$ range from $1$ to $n-1$. Applying the transformation  \n",
    "$\\mathbf{S}_1$ results in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f287c678",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{S}_1^T\\mathbf{A}\\mathbf{S}_1 =  \\left( \\begin{array}{cc} a_{11} & (\\mathbf{Pv})^T \\\\\n",
    "                              \\mathbf{Pv}& \\mathbf{A}' \\end{array} \\right) ,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e301f2",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\mathbf{v^{T}} = \\{a_{21}, a_{31},\\cdots, a_{n1}\\}$ and $\\mathbf{P}$s\n",
    "must satisfy ($\\mathbf{Pv})^{T} = \\{k, 0, 0,\\cdots \\}$. Then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ae9b7f",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:palpha\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\mathbf{Pv} = \\mathbf{v} -2\\mathbf{u}( \\mathbf{u}^T\\mathbf{v})= k \\mathbf{e},\n",
    "\\label{eq:palpha} \\tag{6}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57603c12",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $\\mathbf{e^{T}} = \\{1,0,0,\\dots 0\\}$.\n",
    "\n",
    "Solving the latter equation gives us $\\mathbf{u}$ and thus the needed transformation\n",
    "$\\mathbf{P}$. We do first however need to compute the scalar $k$ by taking the scalar\n",
    "product of the last equation with its transpose and using the fact that $\\mathbf{P}^2=\\mathbf{I}$.\n",
    "We get then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfabbfee",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "(\\mathbf{Pv})^T\\mathbf{Pv} = k^{2} = \\mathbf{v}^T\\mathbf{v}=\n",
    "   |v|^2 = \\sum_{i=2}^{n}a_{i1}^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c045c2ee",
   "metadata": {
    "editable": true
   },
   "source": [
    "which determines the constant $ k = \\pm v$.\n",
    "\n",
    " Now we can rewrite Eq. ([6](#eq:palpha))\n",
    "as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c09ca0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{v} - k\\mathbf{e} = 2\\mathbf{u}( \\mathbf{u}^T\\mathbf{v}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e67ff",
   "metadata": {
    "editable": true
   },
   "source": [
    "and taking the scalar product of this equation with itself and obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8075dd84",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:pmalpha\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    2( \\mathbf{u}^T\\mathbf{v})^2=(v^2\\pm a_{21}v),\n",
    "\\label{eq:pmalpha} \\tag{7}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db047d24",
   "metadata": {
    "editable": true
   },
   "source": [
    "which finally determines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af40f559",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{u}=\\frac{\\mathbf{v}-k\\mathbf{e}}{2( \\mathbf{u}^T\\mathbf{v})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82279b14",
   "metadata": {
    "editable": true
   },
   "source": [
    "In solving Eq. ([7](#eq:pmalpha)) great care has to be exercised so as to choose\n",
    "those values which make the right-hand largest in order to avoid loss of numerical\n",
    "precision. \n",
    "The above steps are then repeated for every transformations till we have a \n",
    "tridiagonal matrix suitable for obtaining the eigenvalues.  \n",
    "\n",
    "Our Householder transformation has given us a tridiagonal matrix. We discuss here how one can use\n",
    "Householder's iterative procedure to obtain the eigenvalues. \n",
    "Let us specialize to a $4\\times 4 $ matrix.\n",
    "The tridiagonal matrix takes the form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf61a19",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{A} =\n",
    "      \\left( \\begin{array}{cccc} \n",
    "                d_{1} & e_{1} & 0     &  0    \\\\\n",
    "                e_{1} & d_{2} & e_{2} &  0    \\\\\n",
    "                 0    & e_{2} & d_{3} & e_{3} \\\\\n",
    "                 0    &   0   & e_{3} & d_{4} \n",
    "             \\end{array} \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c1ee33",
   "metadata": {
    "editable": true
   },
   "source": [
    "As a first observation, if any of the elements $e_{i}$ are zero the\n",
    "matrix can be separated into smaller pieces before\n",
    "diagonalization. Specifically, if $e_{1} = 0$ then $d_{1}$ is an\n",
    "eigenvalue.\n",
    "\n",
    " Thus, let us introduce  a transformation $\\mathbf{S_{1}}$ which operates like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d6d677",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{S_{1}} =\n",
    "      \\left( \\begin{array}{cccc} \n",
    "                \\cos \\theta & 0 & 0 & \\sin \\theta\\\\\n",
    "                 0       & 0 & 0 &      0      \\\\\n",
    "                   0        & 0 & 0 &      0      \\\\\n",
    "               \\cos \\theta & 0 & 0 & \\cos \\theta \n",
    "             \\end{array} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dfb46f",
   "metadata": {
    "editable": true
   },
   "source": [
    "Then the similarity transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4db68cc",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{S_{1}^{T} A  S_{1}} = \\mathbf{A'} = \n",
    "      \\left( \\begin{array}{cccc}\n",
    "              d'_{1} & e'_{1} &   0    &   0   \\\\\n",
    "              e'_{1}  & d_{2}  & e_{2}  &   0   \\\\\n",
    "                0    & e_{2}  & d_{3}  & e'{3} \\\\\n",
    "                0    &   0    & e'_{3} & d'_{4}\n",
    "             \\end{array} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9345078",
   "metadata": {
    "editable": true
   },
   "source": [
    "produces a matrix where the primed elements in $\\mathbf{A'}$ have been\n",
    "changed by the transformation whereas the unprimed elements are unchanged.\n",
    "If we now choose $\\theta$ to\n",
    "give the element $a_{21}^{'} = e^{'}= 0$ then we have the first\n",
    "eigenvalue  $= a_{11}^{'} = d_{1}^{'}$.\n",
    "\n",
    "This procedure can be continued on the remaining three-dimensional\n",
    "submatrix for the next eigenvalue. Thus after few transformations    \n",
    "we have the wanted diagonal form.\n",
    "\n",
    "What we see here is just a special case of the more general procedure \n",
    "developed by Francis in two articles in 1961 and 1962.\n",
    "\n",
    "The algorithm is based on the so-called *QR* method (or just *QR*-algorithm). It follows from a theorem by Schur which states that any square matrix can be written out in terms of an orthogonal matrix $\\mathbf{Q}$ and an upper triangular matrix $\\mathbf{U}$. Historically *R* was used instead of \n",
    "*U* since the wording right triangular matrix was first used.\n",
    "The method is based on an iterative procedure similar to Jacobi's method, by a succession of\n",
    "planar rotations. For a tridiagonal matrix it is simple to carry out in principle, but complicated in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa4121a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Eigenvalues\n",
    "\n",
    "The eigenvalues of a  tridiagonal matrix can be obtained using the characteristic polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed98952",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "P(\\lambda) = det(\\lambda\\mathbf{I}-\\mathbf{A})= \\prod_{i=1}^{n}\\left(\\lambda_i-\\lambda\\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab679360",
   "metadata": {
    "editable": true
   },
   "source": [
    "which rewritten in matrix form reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9672f62",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "P(\\lambda)= \\left( \\begin{array}{ccccccc} d_1-\\lambda & e_1 & 0   & 0    & \\dots  &0     & 0 \\\\\n",
    "                                e_1 & d_2-\\lambda & e_2 & 0    & \\dots  &0     &0 \\\\\n",
    "                                0   & e_2 & d_3-\\lambda & e_3  &0       &\\dots & 0\\\\\n",
    "                                \\dots  & \\dots & \\dots & \\dots  &\\dots      &\\dots & \\dots\\\\\n",
    "                                0   & \\dots & \\dots & \\dots  &\\dots       &d_{N_{\\mathrm{step}}-2}-\\lambda & e_{N_{\\mathrm{step}}-1}\\\\\n",
    "                                0   & \\dots & \\dots & \\dots  &\\dots       &e_{N_{\\mathrm{step}}-1} & d_{N_{\\mathrm{step}}-1}-\\lambda\n",
    "             \\end{array} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d54eba",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can solve this equation in an iterative manner. \n",
    "We let $P_k(\\lambda)$ be the value of $k$ subdeterminant of the above matrix of dimension\n",
    "$n\\times n$. The polynomial $P_k(\\lambda)$ is clearly a polynomial of degree $k$.\n",
    "Starting with $P_1(\\lambda)$ we have $P_1(\\lambda)=d_1-\\lambda$. The next polynomial reads\n",
    "$P_2(\\lambda)=(d_2-\\lambda)P_1(\\lambda)-e_1^2$. By expanding the determinant for $P_k(\\lambda)$ \n",
    "in terms of the minors of the $n$th column we arrive at the recursion relation\n",
    "\\[ \n",
    "   P_k(\\lambda)=(d_k-\\lambda)P_{k-1}(\\lambda)-e_{k-1}^2P_{k-2}(\\lambda).\n",
    "\\]\n",
    "Together with the starting values $P_1(\\lambda)$ and $P_2(\\lambda)$ and good root searching methods\n",
    "we arrive at an efficient computational scheme for finding the roots of $P_n(\\lambda)$. \n",
    "However, for large matrices this algorithm is rather inefficient and time-consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01819afc",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Eigenvalues and Lanczos' method\n",
    "\n",
    "Basic features with a real symmetric matrix (and normally huge $n> 10^6$ and sparse) \n",
    "$\\hat{A}$ of dimension $n\\times n$:\n",
    "\n",
    "* Lanczos' algorithm generates a sequence of real tridiagonal matrices $T_k$ of dimension $k\\times k$ with $k\\le n$, with the property that the extremal eigenvalues of $T_k$ are progressively better estimates of $\\hat{A}$' extremal eigenvalues.* The method converges to the extremal eigenvalues.\n",
    "\n",
    "* The similarity transformation is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e56076",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{T}= \\hat{Q}^{T}\\hat{A}\\hat{Q},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc64c7b",
   "metadata": {
    "editable": true
   },
   "source": [
    "with the first vector $\\hat{Q}\\hat{e}_1=\\hat{q}_1$.\n",
    "\n",
    "We are going to solve iteratively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d50d93",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{T}= \\hat{Q}^{T}\\hat{A}\\hat{Q},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039cbd24",
   "metadata": {
    "editable": true
   },
   "source": [
    "with the first vector $\\hat{Q}\\hat{e}_1=\\hat{q}_1$.\n",
    "We can write out the matrix $\\hat{Q}$ in terms of its column vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd52556c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{Q}=\\left[\\hat{q}_1\\hat{q}_2\\dots\\hat{q}_n\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49d4eff",
   "metadata": {
    "editable": true
   },
   "source": [
    "The matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bddc7ec",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{T}= \\hat{Q}^{T}\\hat{A}\\hat{Q},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64943ad0",
   "metadata": {
    "editable": true
   },
   "source": [
    "can be written as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5008bb3",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{T} = \\left(\\begin{array}{cccccc}\n",
    "                           \\alpha_1& \\beta_1 & 0 &\\dots   & \\dots &0 \\\\\n",
    "                           \\beta_1 & \\alpha_2 & \\beta_2 &0 &\\dots &0 \\\\\n",
    "                           0& \\beta_2 & \\alpha_3 & \\beta_3 & \\dots &0 \\\\\n",
    "                           \\dots& \\dots   & \\dots &\\dots   &\\dots & 0 \\\\\n",
    "                           \\dots&   &  &\\beta_{n-2}  &\\alpha_{n-1}& \\beta_{n-1} \\\\\n",
    "                           0&  \\dots  &\\dots  &0   &\\beta_{n-1} & \\alpha_{n} \\\\\n",
    "                      \\end{array} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caf1ad5",
   "metadata": {
    "editable": true
   },
   "source": [
    "Using the fact that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48153faa",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{Q}\\hat{Q}^T=\\hat{I},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3e903d",
   "metadata": {
    "editable": true
   },
   "source": [
    "we can rewrite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f950d07",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{T}= \\hat{Q}^{T}\\hat{A}\\hat{Q},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf87d222",
   "metadata": {
    "editable": true
   },
   "source": [
    "as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3a63ab",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{Q}\\hat{T}= \\hat{A}\\hat{Q}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072a6044",
   "metadata": {
    "editable": true
   },
   "source": [
    "If we equate columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea2b4a0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{T} = \\left(\\begin{array}{cccccc}\n",
    "        \\alpha_1& \\beta_1 & 0 &\\dots   & \\dots &0 \\\\\n",
    "        \\beta_1 & \\alpha_2 & \\beta_2 &0 &\\dots &0 \\\\\n",
    "        0& \\beta_2 & \\alpha_3 & \\beta_3 & \\dots &0 \\\\\n",
    "        \\dots& \\dots   & \\dots &\\dots   &\\dots & 0 \\\\\n",
    "        \\dots&   &  &\\beta_{n-2}  &\\alpha_{n-1}& \\beta_{n-1} \\\\\n",
    "        0&  \\dots  &\\dots  &0   &\\beta_{n-1} & \\alpha_{n} \\\\\n",
    "        \\end{array} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c9b37b",
   "metadata": {
    "editable": true
   },
   "source": [
    "we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b30a89",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{A}\\hat{q}_k=\\beta_{k-1}\\hat{q}_{k-1}+\\alpha_k\\hat{q}_k+\\beta_k\\hat{q}_{k+1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420f4d64",
   "metadata": {
    "editable": true
   },
   "source": [
    "We have thus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389adfa6",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{A}\\hat{q}_k=\\beta_{k-1}\\hat{q}_{k-1}+\\alpha_k\\hat{q}_k+\\beta_k\\hat{q}_{k+1},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e054b920",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $\\beta_0\\hat{q}_0=0$ for $k=1:n-1$. Remember that the vectors $\\hat{q}_k$  are orthornormal and this implies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63e33a7",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\alpha_k=\\hat{q}_k^T\\hat{A}\\hat{q}_k,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1519e634",
   "metadata": {
    "editable": true
   },
   "source": [
    "and these vectors are called Lanczos vectors.\n",
    "\n",
    "We have thus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af3fd2d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{A}\\hat{q}_k=\\beta_{k-1}\\hat{q}_{k-1}+\\alpha_k\\hat{q}_k+\\beta_k\\hat{q}_{k+1},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c3fd1e",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $\\beta_0\\hat{q}_0=0$ for $k=1:n-1$ and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2913b1",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\alpha_k=\\hat{q}_k^T\\hat{A}\\hat{q}_k.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fa9535",
   "metadata": {
    "editable": true
   },
   "source": [
    "If"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e39dac",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{r}_k=(\\hat{A}-\\alpha_k\\hat{I})\\hat{q}_k-\\beta_{k-1}\\hat{q}_{k-1},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599707e7",
   "metadata": {
    "editable": true
   },
   "source": [
    "is non-zero, then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384be204",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{q}_{k+1}=\\hat{r}_{k}/\\beta_k,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed5d7c4",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $\\beta_k=\\pm ||\\hat{r}_{k}||_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a7e93",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Variational Monte Carlo methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63e336d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Quantum Monte Carlo Motivation\n",
    "\n",
    "We start with the variational principle.\n",
    "Given a hamiltonian $H$ and a trial wave function $\\Psi_T$, the variational principle states that the expectation value of $\\langle H \\rangle$, defined through"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb71e35",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "E[H]= \\langle H \\rangle =\n",
    "   \\frac{\\int d\\boldsymbol{R}\\Psi^{\\ast}_T(\\boldsymbol{R})H(\\boldsymbol{R})\\Psi_T(\\boldsymbol{R})}\n",
    "        {\\int d\\boldsymbol{R}\\Psi^{\\ast}_T(\\boldsymbol{R})\\Psi_T(\\boldsymbol{R})},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51bb0f9",
   "metadata": {
    "editable": true
   },
   "source": [
    "is an upper bound to the ground state energy $E_0$ of the hamiltonian $H$, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedb1c91",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "E_0 \\le \\langle H \\rangle .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60c05dc",
   "metadata": {
    "editable": true
   },
   "source": [
    "In general, the integrals involved in the calculation of various  expectation values  are multi-dimensional ones. Traditional integration methods such as the Gauss-Legendre will not be adequate for say the  computation of the energy of a many-body system.\n",
    "\n",
    "The trial wave function can be expanded in the eigenstates of the hamiltonian since they form a complete set, viz.,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da857b4",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\Psi_T(\\boldsymbol{R})=\\sum_i a_i\\Psi_i(\\boldsymbol{R}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74730e65",
   "metadata": {
    "editable": true
   },
   "source": [
    "and assuming the set of eigenfunctions to be normalized one obtains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07af2503",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\sum_{nm}a^*_ma_n \\int d\\boldsymbol{R}\\Psi^{\\ast}_m(\\boldsymbol{R})H(\\boldsymbol{R})\\Psi_n(\\boldsymbol{R})}\n",
    "        {\\sum_{nm}a^*_ma_n \\int d\\boldsymbol{R}\\Psi^{\\ast}_m(\\boldsymbol{R})\\Psi_n(\\boldsymbol{R})} =\\frac{\\sum_{n}a^2_n E_n}\n",
    "        {\\sum_{n}a^2_n} \\ge E_0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8f5e7b",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we used that $H(\\boldsymbol{R})\\Psi_n(\\boldsymbol{R})=E_n\\Psi_n(\\boldsymbol{R})$.\n",
    "In general, the integrals involved in the calculation of various  expectation\n",
    "values  are multi-dimensional ones. \n",
    "The variational principle yields the lowest state of a given symmetry.\n",
    "\n",
    "In most cases, a wave function has only small values in large parts of \n",
    "configuration space, and a straightforward procedure which uses\n",
    "homogenously distributed random points in configuration space \n",
    "will most likely lead to poor results. This may suggest that some kind\n",
    "of importance sampling combined with e.g., the Metropolis algorithm \n",
    "may be  a more efficient way of obtaining the ground state energy.\n",
    "The hope is then that those regions of configurations space where\n",
    "the wave function assumes appreciable values are sampled more \n",
    "efficiently. \n",
    "\n",
    "The tedious part in a VMC calculation is the search for the variational\n",
    "minimum. A good knowledge of the system is required in order to carry out\n",
    "reasonable VMC calculations. This is not always the case, \n",
    "and often VMC calculations \n",
    "serve rather as the starting\n",
    "point for so-called diffusion Monte Carlo calculations (DMC). DMC is a way of\n",
    "solving exactly the many-body Schroedinger equation by means of \n",
    "a stochastic procedure. A good guess on the binding energy\n",
    "and its wave function is however necessary. \n",
    "A carefully performed VMC calculation can aid in this context. \n",
    "\n",
    "The basic recipe in a VMC calculation consists of the following elements:\n",
    "\n",
    "* Construct first a trial wave function $\\psi_T(\\boldsymbol{R},\\boldsymbol{\\alpha})$,  for a many-body system consisting of $N$ particles located at positions  $\\boldsymbol{R}=(\\boldsymbol{R}_1,\\dots ,\\boldsymbol{R}_N)$. The trial wave function depends on $\\alpha$ variational parameters $\\boldsymbol{\\alpha}=(\\alpha_1,\\dots ,\\alpha_M)$.\n",
    "\n",
    "* Then we evaluate the expectation value of the hamiltonian $H$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c0f9f5",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "E[H]=\\langle H \\rangle =\n",
    "   \\frac{\\int d\\boldsymbol{R}\\Psi^{\\ast}_{T}(\\boldsymbol{R},\\boldsymbol{\\alpha})H(\\boldsymbol{R})\\Psi_{T}(\\boldsymbol{R},\\boldsymbol{\\alpha})}\n",
    "        {\\int d\\boldsymbol{R}\\Psi^{\\ast}_{T}(\\boldsymbol{R},\\boldsymbol{\\alpha})\\Psi_{T}(\\boldsymbol{R},\\boldsymbol{\\alpha})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d8db6a",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Thereafter we vary $\\alpha$ according to some minimization algorithm and return to the first step.\n",
    "\n",
    "With a trial wave function $\\psi_T(\\boldsymbol{R})$ we can in turn construct the quantum mechanical probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffc1dc0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "P(\\boldsymbol{R})= \\frac{\\left|\\psi_T(\\boldsymbol{R})\\right|^2}{\\int \\left|\\psi_T(\\boldsymbol{R})\\right|^2d\\boldsymbol{R}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ca2a8f",
   "metadata": {
    "editable": true
   },
   "source": [
    "This is our new probability distribution function  (PDF).\n",
    "The approximation to the expectation value of the Hamiltonian is now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c816410b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "E[H(\\boldsymbol{\\alpha})] = \n",
    "   \\frac{\\int d\\boldsymbol{R}\\Psi^{\\ast}_T(\\boldsymbol{R},\\boldsymbol{\\alpha})H(\\boldsymbol{R})\\Psi_T(\\boldsymbol{R},\\boldsymbol{\\alpha})}\n",
    "        {\\int d\\boldsymbol{R}\\Psi^{\\ast}_T(\\boldsymbol{R},\\boldsymbol{\\alpha})\\Psi_T(\\boldsymbol{R},\\boldsymbol{\\alpha})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47537ca2",
   "metadata": {
    "editable": true
   },
   "source": [
    "Define a new quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76666b67",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:locale1\"></div>\n",
    "\n",
    "$$\n",
    "E_L(\\boldsymbol{R},\\boldsymbol{\\alpha})=\\frac{1}{\\psi_T(\\boldsymbol{R},\\boldsymbol{\\alpha})}H\\psi_T(\\boldsymbol{R},\\boldsymbol{\\alpha}),\n",
    "\\label{eq:locale1} \\tag{8}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad5cc1d",
   "metadata": {
    "editable": true
   },
   "source": [
    "called the local energy, which, together with our trial PDF yields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3566c93",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:vmc1\"></div>\n",
    "\n",
    "$$\n",
    "E[H(\\boldsymbol{\\alpha})]=\\int P(\\boldsymbol{R})E_L(\\boldsymbol{R}) d\\boldsymbol{R}\\approx \\frac{1}{N}\\sum_{i=1}^NP(\\boldsymbol{R_i},\\boldsymbol{\\alpha})E_L(\\boldsymbol{R_i},\\boldsymbol{\\alpha})\n",
    "\\label{eq:vmc1} \\tag{9}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d523bf",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $N$ being the number of Monte Carlo samples.\n",
    "\n",
    "The Algorithm for performing a variational Monte Carlo calculations runs thus as this\n",
    "\n",
    "   * Initialisation: Fix the number of Monte Carlo steps. Choose an initial $\\boldsymbol{R}$ and variational parameters $\\alpha$ and calculate $\\left|\\psi_T^{\\alpha}(\\boldsymbol{R})\\right|^2$. \n",
    "\n",
    "   * Initialise the energy and the variance and start the Monte Carlo calculation.\n",
    "\n",
    "      * Calculate  a trial position  $\\boldsymbol{R}_p=\\boldsymbol{R}+r*step$ where $r$ is a random variable $r \\in [0,1]$.\n",
    "\n",
    "      * Metropolis algorithm to accept or reject this move  $w = P(\\boldsymbol{R}_p)/P(\\boldsymbol{R})$.\n",
    "\n",
    "      * If the step is accepted, then we set $\\boldsymbol{R}=\\boldsymbol{R}_p$. \n",
    "\n",
    "      * Update averages\n",
    "\n",
    "   * Finish and compute final averages.\n",
    "\n",
    "Observe that the jumping in space is governed by the variable *step*. This is Called brute-force sampling.\n",
    "Need importance sampling to get more relevant sampling, see lectures below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170c8333",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Quantum Monte Carlo: hydrogen atom\n",
    "\n",
    "The radial Schroedinger equation for the hydrogen atom can be\n",
    "written as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea9dbf9",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "-\\frac{\\hbar^2}{2m}\\frac{\\partial^2 u(r)}{\\partial r^2}-\n",
    "\\left(\\frac{ke^2}{r}-\\frac{\\hbar^2l(l+1)}{2mr^2}\\right)u(r)=Eu(r),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7cb889",
   "metadata": {
    "editable": true
   },
   "source": [
    "or with dimensionless variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128a7c80",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:hydrodimless1\"></div>\n",
    "\n",
    "$$\n",
    "-\\frac{1}{2}\\frac{\\partial^2 u(\\rho)}{\\partial \\rho^2}-\n",
    "\\frac{u(\\rho)}{\\rho}+\\frac{l(l+1)}{2\\rho^2}u(\\rho)-\\lambda u(\\rho)=0,\n",
    "\\label{eq:hydrodimless1} \\tag{10}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6562d4",
   "metadata": {
    "editable": true
   },
   "source": [
    "with the hamiltonian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ec90b3",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "H=-\\frac{1}{2}\\frac{\\partial^2 }{\\partial \\rho^2}-\n",
    "\\frac{1}{\\rho}+\\frac{l(l+1)}{2\\rho^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a2f699",
   "metadata": {
    "editable": true
   },
   "source": [
    "Use variational parameter $\\alpha$ in the trial\n",
    "wave function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38220301",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:trialhydrogen\"></div>\n",
    "\n",
    "$$\n",
    "u_T^{\\alpha}(\\rho)=\\alpha\\rho e^{-\\alpha\\rho}. \n",
    "\\label{eq:trialhydrogen} \\tag{11}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f128b1",
   "metadata": {
    "editable": true
   },
   "source": [
    "Inserting this wave function into the expression for the\n",
    "local energy $E_L$ gives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29d27fe",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "E_L(\\rho)=-\\frac{1}{\\rho}-\n",
    "              \\frac{\\alpha}{2}\\left(\\alpha-\\frac{2}{\\rho}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a09047",
   "metadata": {
    "editable": true
   },
   "source": [
    "A simple variational Monte Carlo calculation results in\n",
    "<table class=\"dotable\" border=\"1\">\n",
    "<thead>\n",
    "<tr><th align=\"center\">  $\\alpha$ </th> <th align=\"center\">$\\langle H \\rangle $</th> <th align=\"center\"> $\\sigma^2$</th> <th align=\"center\">$\\sigma/\\sqrt{N}$</th> </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td align=\"center\">   7.00000E-01    </td> <td align=\"center\">   -4.57759E-01            </td> <td align=\"center\">   4.51201E-02    </td> <td align=\"center\">   6.71715E-04          </td> </tr>\n",
    "<tr><td align=\"center\">   8.00000E-01    </td> <td align=\"center\">   -4.81461E-01            </td> <td align=\"center\">   3.05736E-02    </td> <td align=\"center\">   5.52934E-04          </td> </tr>\n",
    "<tr><td align=\"center\">   9.00000E-01    </td> <td align=\"center\">   -4.95899E-01            </td> <td align=\"center\">   8.20497E-03    </td> <td align=\"center\">   2.86443E-04          </td> </tr>\n",
    "<tr><td align=\"center\">   1.00000E-00    </td> <td align=\"center\">   -5.00000E-01            </td> <td align=\"center\">   0.00000E+00    </td> <td align=\"center\">   0.00000E+00          </td> </tr>\n",
    "<tr><td align=\"center\">   1.10000E+00    </td> <td align=\"center\">   -4.93738E-01            </td> <td align=\"center\">   1.16989E-02    </td> <td align=\"center\">   3.42036E-04          </td> </tr>\n",
    "<tr><td align=\"center\">   1.20000E+00    </td> <td align=\"center\">   -4.75563E-01            </td> <td align=\"center\">   8.85899E-02    </td> <td align=\"center\">   9.41222E-04          </td> </tr>\n",
    "<tr><td align=\"center\">   1.30000E+00    </td> <td align=\"center\">   -4.54341E-01            </td> <td align=\"center\">   1.45171E-01    </td> <td align=\"center\">   1.20487E-03          </td> </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "We note that at $\\alpha=1$ we obtain the exact\n",
    "result, and the variance is zero, as it should. The reason is that \n",
    "we then have the exact wave function, and the action of the hamiltionan\n",
    "on the wave function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60478ae1",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "H\\psi = \\mathrm{constant}\\times \\psi,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61197cf",
   "metadata": {
    "editable": true
   },
   "source": [
    "yields just a constant. The integral which defines various \n",
    "expectation values involving moments of the hamiltonian becomes then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5edc50",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\langle H^n \\rangle =\n",
    "   \\frac{\\int d\\boldsymbol{R}\\Psi^{\\ast}_T(\\boldsymbol{R})H^n(\\boldsymbol{R})\\Psi_T(\\boldsymbol{R})}\n",
    "        {\\int d\\boldsymbol{R}\\Psi^{\\ast}_T(\\boldsymbol{R})\\Psi_T(\\boldsymbol{R})}=\n",
    "\\mathrm{constant}\\times\\frac{\\int d\\boldsymbol{R}\\Psi^{\\ast}_T(\\boldsymbol{R})\\Psi_T(\\boldsymbol{R})}\n",
    "        {\\int d\\boldsymbol{R}\\Psi^{\\ast}_T(\\boldsymbol{R})\\Psi_T(\\boldsymbol{R})}=\\mathrm{constant}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4534ea",
   "metadata": {
    "editable": true
   },
   "source": [
    "**This gives an important information: the exact wave function leads to zero variance!**\n",
    "Variation is then performed by minimizing both the energy and the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631bd84e",
   "metadata": {
    "editable": true
   },
   "source": [
    "### A simple Python code that solves the two-boson or two-fermion case in two-dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b75c7b5",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Importing various packages\n",
    "from math import exp, sqrt\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import sys\n",
    "\n",
    "#Trial wave function for quantum dots in two dims\n",
    "def WaveFunction(r,alpha,beta):\n",
    "    r1 = r[0,0]**2 + r[0,1]**2\n",
    "    r2 = r[1,0]**2 + r[1,1]**2\n",
    "    r12 = sqrt((r[0,0]-r[1,0])**2 + (r[0,1]-r[1,1])**2)\n",
    "    deno = r12/(1+beta*r12)\n",
    "    return exp(-0.5*alpha*(r1+r2)+deno)\n",
    "\n",
    "#Local energy  for quantum dots in two dims, using analytical local energy\n",
    "def LocalEnergy(r,alpha,beta):\n",
    "    \n",
    "    r1 = (r[0,0]**2 + r[0,1]**2)\n",
    "    r2 = (r[1,0]**2 + r[1,1]**2)\n",
    "    r12 = sqrt((r[0,0]-r[1,0])**2 + (r[0,1]-r[1,1])**2)\n",
    "    deno = 1.0/(1+beta*r12)\n",
    "    deno2 = deno*deno\n",
    "    return 0.5*(1-alpha*alpha)*(r1 + r2) +2.0*alpha + 1.0/r12+deno2*(alpha*r12-deno2+2*beta*deno-1.0/r12)\n",
    "\n",
    "# The Monte Carlo sampling with the Metropolis algo\n",
    "def MonteCarloSampling():\n",
    "\n",
    "    NumberMCcycles= 100000\n",
    "    StepSize = 1.0\n",
    "    # positions\n",
    "    PositionOld = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    PositionNew = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    # seed for rng generator\n",
    "    seed()\n",
    "    # start variational parameter\n",
    "    alpha = 0.9\n",
    "    for ia in range(MaxVariations):\n",
    "        alpha += .025\n",
    "        AlphaValues[ia] = alpha\n",
    "        beta = 0.2 \n",
    "        for jb in range(MaxVariations):\n",
    "            beta += .01\n",
    "            BetaValues[jb] = beta\n",
    "            energy = energy2 = 0.0\n",
    "            DeltaE = 0.0\n",
    "            #Initial position\n",
    "            for i in range(NumberParticles):\n",
    "                for j in range(Dimension):\n",
    "                    PositionOld[i,j] = StepSize * (random() - .5)\n",
    "            wfold = WaveFunction(PositionOld,alpha,beta)\n",
    "\n",
    "            #Loop over MC MCcycles\n",
    "            for MCcycle in range(NumberMCcycles):\n",
    "                #Trial position\n",
    "                for i in range(NumberParticles):\n",
    "                    for j in range(Dimension):\n",
    "                        PositionNew[i,j] = PositionOld[i,j] + StepSize * (random() - .5)\n",
    "                wfnew = WaveFunction(PositionNew,alpha,beta)\n",
    "\n",
    "                #Metropolis test to see whether we accept the move\n",
    "                if random() < wfnew**2 / wfold**2:\n",
    "                   PositionOld = PositionNew.copy()\n",
    "                   wfold = wfnew\n",
    "                   DeltaE = LocalEnergy(PositionOld,alpha,beta)\n",
    "                energy += DeltaE\n",
    "                energy2 += DeltaE**2\n",
    "\n",
    "            #We calculate mean, variance and error ...\n",
    "            energy /= NumberMCcycles\n",
    "            energy2 /= NumberMCcycles\n",
    "            variance = energy2 - energy**2\n",
    "            error = sqrt(variance/NumberMCcycles)\n",
    "            Energies[ia,jb] = energy    \n",
    "    return Energies, AlphaValues, BetaValues\n",
    "\n",
    "\n",
    "#Here starts the main program with variable declarations\n",
    "NumberParticles = 2\n",
    "Dimension = 2\n",
    "MaxVariations = 10\n",
    "Energies = np.zeros((MaxVariations,MaxVariations))\n",
    "AlphaValues = np.zeros(MaxVariations)\n",
    "BetaValues = np.zeros(MaxVariations)\n",
    "(Energies, AlphaValues, BetaValues) = MonteCarloSampling()\n",
    "\n",
    "# Prepare for plots\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "# Plot the surface.\n",
    "X, Y = np.meshgrid(AlphaValues, BetaValues)\n",
    "surf = ax.plot_surface(X, Y, Energies,cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
    "# Customize the z axis.\n",
    "zmin = np.matrix(Energies).min()\n",
    "zmax = np.matrix(Energies).max()\n",
    "ax.set_zlim(zmin, zmax)\n",
    "ax.set_xlabel(r'$\\alpha$')\n",
    "ax.set_ylabel(r'$\\beta$')\n",
    "ax.set_zlabel(r'$\\langle E \\rangle$')\n",
    "ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "# Add a color bar which maps values to colors.\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f34fb7",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Quantum Monte Carlo: the helium atom\n",
    "\n",
    "The helium atom consists of two electrons and a nucleus with\n",
    "charge $Z=2$. \n",
    "The contribution  \n",
    "to the potential energy due to the attraction from the nucleus is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d81d7b1",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "-\\frac{2ke^2}{r_1}-\\frac{2ke^2}{r_2},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ddcc5c",
   "metadata": {
    "editable": true
   },
   "source": [
    "and if we add the repulsion arising from the two \n",
    "interacting electrons, we obtain the potential energy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f8536f",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "V(r_1, r_2)=-\\frac{2ke^2}{r_1}-\\frac{2ke^2}{r_2}+\n",
    "               \\frac{ke^2}{r_{12}},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ab986",
   "metadata": {
    "editable": true
   },
   "source": [
    "with the electrons separated at a distance \n",
    "$r_{12}=|\\boldsymbol{r}_1-\\boldsymbol{r}_2|$.\n",
    "\n",
    "The hamiltonian becomes then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2a24db",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{H}=-\\frac{\\hbar^2\\nabla_1^2}{2m}-\\frac{\\hbar^2\\nabla_2^2}{2m}\n",
    "          -\\frac{2ke^2}{r_1}-\\frac{2ke^2}{r_2}+\n",
    "               \\frac{ke^2}{r_{12}},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0323a4c",
   "metadata": {
    "editable": true
   },
   "source": [
    "and  Schroedingers equation reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40c9d96",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{H}\\psi=E\\psi.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c0d5f7",
   "metadata": {
    "editable": true
   },
   "source": [
    "All observables are evaluated with respect to the probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3051f28",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "P(\\boldsymbol{R})= \\frac{\\left|\\psi_T(\\boldsymbol{R})\\right|^2}{\\int \\left|\\psi_T(\\boldsymbol{R})\\right|^2d\\boldsymbol{R}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d987f88",
   "metadata": {
    "editable": true
   },
   "source": [
    "generated by the trial wave function.   \n",
    "The trial wave function must approximate an exact \n",
    "eigenstate in order that accurate results are to be obtained. \n",
    "\n",
    "Choice of trial wave function for Helium:\n",
    "Assume $r_1 \\rightarrow 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f29ce0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "E_L(\\boldsymbol{R})=\\frac{1}{\\psi_T(\\boldsymbol{R})}H\\psi_T(\\boldsymbol{R})=\n",
    "     \\frac{1}{\\psi_T(\\boldsymbol{R})}\\left(-\\frac{1}{2}\\nabla^2_1\n",
    "     -\\frac{Z}{r_1}\\right)\\psi_T(\\boldsymbol{R}) + \\mathrm{finite \\hspace{0.1cm}terms}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28e9cf3",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "E_L(R)=\n",
    "    \\frac{1}{\\mathbf{R}_T(r_1)}\\left(-\\frac{1}{2}\\frac{d^2}{dr_1^2}-\n",
    "     \\frac{1}{r_1}\\frac{d}{dr_1}\n",
    "     -\\frac{Z}{r_1}\\right)\\mathbf{R}_T(r_1) + \\mathrm{finite\\hspace{0.1cm} terms}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60042242",
   "metadata": {
    "editable": true
   },
   "source": [
    "For small values of $r_1$, the terms which dominate are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00dc458",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\lim_{r_1 \\rightarrow 0}E_L(R)=\n",
    "    \\frac{1}{\\mathbf{R}_T(r_1)}\\left(-\n",
    "     \\frac{1}{r_1}\\frac{d}{dr_1}\n",
    "     -\\frac{Z}{r_1}\\right)\\mathbf{R}_T(r_1),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe3e1bf",
   "metadata": {
    "editable": true
   },
   "source": [
    "since the second derivative does not diverge due to the finiteness of  $\\Psi$ at the origin.\n",
    "\n",
    "This results in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070c676a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{1}{\\mathbf{R}_T(r_1)}\\frac{d \\mathbf{R}_T(r_1)}{dr_1}=-Z,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e710ed",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e4a039",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{R}_T(r_1)\\propto e^{-Zr_1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a828905",
   "metadata": {
    "editable": true
   },
   "source": [
    "A similar condition applies to electron 2 as well. \n",
    "For orbital momenta $l > 0$ we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09413e8a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{1}{\\mathbf{R}_T(r)}\\frac{d \\mathbf{R}_T(r)}{dr}=-\\frac{Z}{l+1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057befad",
   "metadata": {
    "editable": true
   },
   "source": [
    "Similarly, studying the case $r_{12}\\rightarrow 0$ we can write \n",
    "a possible trial wave function as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbe0f81",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:wavehelium2\"></div>\n",
    "\n",
    "$$\n",
    "\\psi_T(\\boldsymbol{R})=e^{-\\alpha(r_1+r_2)}e^{\\beta r_{12}}.\n",
    "\\label{eq:wavehelium2} \\tag{12}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335a23f4",
   "metadata": {
    "editable": true
   },
   "source": [
    "The last equation can be generalized to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6975274c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\psi_T(\\boldsymbol{R})=\\phi(\\boldsymbol{r}_1)\\phi(\\boldsymbol{r}_2)\\dots\\phi(\\boldsymbol{r}_N)\n",
    "                   \\prod_{i < j}f(r_{ij}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e445ae",
   "metadata": {
    "editable": true
   },
   "source": [
    "for a system with $N$ electrons or particles. \n",
    "\n",
    "During the development of our code we need to make several checks. It is also very instructive to compute a closed form expression for the local energy. Since our wave function is rather simple  it is straightforward\n",
    "to find an analytic expressions.  Consider first the case of the simple helium function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88b866e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\Psi_T(\\boldsymbol{r}_1,\\boldsymbol{r}_2) = e^{-\\alpha(r_1+r_2)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd272ea",
   "metadata": {
    "editable": true
   },
   "source": [
    "The local energy is for this case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e16a4a7",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "E_{L1} = \\left(\\alpha-Z\\right)\\left(\\frac{1}{r_1}+\\frac{1}{r_2}\\right)+\\frac{1}{r_{12}}-\\alpha^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb51ced",
   "metadata": {
    "editable": true
   },
   "source": [
    "which gives an expectation value for the local energy given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f81d3a2",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\langle E_{L1} \\rangle = \\alpha^2-2\\alpha\\left(Z-\\frac{5}{16}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3168127a",
   "metadata": {
    "editable": true
   },
   "source": [
    "With closed form formulae we  can speed up the computation of the correlation. In our case\n",
    "we write it as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9c46c4",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\Psi_C= \\exp{\\left\\{\\sum_{i < j}\\frac{ar_{ij}}{1+\\beta r_{ij}}\\right\\}},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f93bcf",
   "metadata": {
    "editable": true
   },
   "source": [
    "which means that the gradient needed for the so-called quantum force and local energy \n",
    "can be calculated analytically.\n",
    "This will speed up your code since the computation of the correlation part and the Slater determinant are the most \n",
    "time consuming parts in your code.  \n",
    "\n",
    "We will refer to this correlation function as $\\Psi_C$ or the *linear Pade-Jastrow*.\n",
    "\n",
    "We can test this by computing the local energy for our helium wave function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c46bfa",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\psi_{T}(\\boldsymbol{r}_1,\\boldsymbol{r}_2) = \n",
    "   \\exp{\\left(-\\alpha(r_1+r_2)\\right)}\n",
    "   \\exp{\\left(\\frac{r_{12}}{2(1+\\beta r_{12})}\\right)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5c87b0",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $\\alpha$ and $\\beta$ as variational parameters.\n",
    "\n",
    "The local energy is for this case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ca71fa",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "E_{L2} = E_{L1}+\\frac{1}{2(1+\\beta r_{12})^2}\\left\\{\\frac{\\alpha(r_1+r_2)}{r_{12}}(1-\\frac{\\boldsymbol{r}_1\\boldsymbol{r}_2}{r_1r_2})-\\frac{1}{2(1+\\beta r_{12})^2}-\\frac{2}{r_{12}}+\\frac{2\\beta}{1+\\beta r_{12}}\\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545b0057",
   "metadata": {
    "editable": true
   },
   "source": [
    "It is very useful to test your code against these expressions. It means also that you don't need to\n",
    "compute a derivative numerically as discussed in the code example below. \n",
    "\n",
    "For the computation of various derivatives with different types of wave functions, you will find it useful to use python with symbolic python, that is sympy, see [online manual](http://docs.sympy.org/latest/index.html).  Using sympy allows you autogenerate both Latex code as well c++, python or Fortran codes. Here you will find some simple examples. We choose \n",
    "the $2s$ hydrogen-orbital  (not normalized) as an example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb6ceee",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\phi_{2s}(\\boldsymbol{r}) = (Zr - 2)\\exp{-(\\frac{1}{2}Zr)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effa6814",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $ r^2 = x^2 + y^2 + z^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "997ddf4f",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sympy import symbols, diff, exp, sqrt\n",
    "x, y, z, Z = symbols('x y z Z')\n",
    "r = sqrt(x*x + y*y + z*z)\n",
    "r\n",
    "phi = (Z*r - 2)*exp(-Z*r/2)\n",
    "phi\n",
    "diff(phi, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4571c1f0",
   "metadata": {
    "editable": true
   },
   "source": [
    "This doesn't look very nice, but sympy provides several functions that allow for improving and simplifying the output.\n",
    "\n",
    "We can improve our output by factorizing and substituting expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37025322",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sympy import symbols, diff, exp, sqrt, factor, Symbol, printing\n",
    "x, y, z, Z = symbols('x y z Z')\n",
    "r = sqrt(x*x + y*y + z*z)\n",
    "phi = (Z*r - 2)*exp(-Z*r/2)\n",
    "R = Symbol('r') #Creates a symbolic equivalent of r\n",
    "#print latex and c++ code\n",
    "print printing.latex(diff(phi, x).factor().subs(r, R))\n",
    "print printing.ccode(diff(phi, x).factor().subs(r, R))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16507f9",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can in turn look at second derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ace7d9c",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sympy import symbols, diff, exp, sqrt, factor, Symbol, printing\n",
    "x, y, z, Z = symbols('x y z Z')\n",
    "r = sqrt(x*x + y*y + z*z)\n",
    "phi = (Z*r - 2)*exp(-Z*r/2)\n",
    "R = Symbol('r') #Creates a symbolic equivalent of r\n",
    "(diff(diff(phi, x), x) + diff(diff(phi, y), y) + diff(diff(phi, z), z)).factor().subs(r, R)\n",
    "# Collect the Z values\n",
    "(diff(diff(phi, x), x) + diff(diff(phi, y), y) +diff(diff(phi, z), z)).factor().collect(Z).subs(r, R)\n",
    "# Factorize also the r**2 terms\n",
    "(diff(diff(phi, x), x) + diff(diff(phi, y), y) + diff(diff(phi, z), z)).factor().collect(Z).subs(r, R).subs(r**2, R**2).factor()\n",
    "print printing.ccode((diff(diff(phi, x), x) + diff(diff(phi, y), y) + diff(diff(phi, z), z)).factor().collect(Z).subs(r, R).subs(r**2, R**2).factor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0f8776",
   "metadata": {
    "editable": true
   },
   "source": [
    "With some practice this allows one to be able to check one's own calculation and translate automatically into code lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe086b6",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The Metropolis algorithm\n",
    "\n",
    "The Metropolis algorithm , see [the original article](http://scitation.aip.org/content/aip/journal/jcp/21/6/10.1063/1.1699114) was invented by Metropolis et. al\n",
    "and is often simply called the Metropolis algorithm.\n",
    "It is a method to sample a normalized probability\n",
    "distribution by a stochastic process. We define $\\mathbf{P}_i^{(n)}$ to\n",
    "be the probability for finding the system in the state $i$ at step $n$.\n",
    "The algorithm is then\n",
    "\n",
    "* Sample a possible new state $j$ with some probability $T_{i\\rightarrow j}$.\n",
    "\n",
    "* Accept the new state $j$ with probability $A_{i \\rightarrow j}$ and use it as the next sample. With probability $1-A_{i\\rightarrow j}$ the move is rejected and the original state $i$ is used again as a sample.\n",
    "\n",
    "We wish to derive the required properties of $T$ and $A$ such that\n",
    "$\\mathbf{P}_i^{(n\\rightarrow \\infty)} \\rightarrow p_i$ so that starting\n",
    "from any distribution, the method converges to the correct distribution.\n",
    "Note that the description here is for a discrete probability distribution.\n",
    "Replacing probabilities $p_i$ with expressions like $p(x_i)dx_i$ will\n",
    "take all of these over to the corresponding continuum expressions.\n",
    "\n",
    "The dynamical equation for $\\mathbf{P}_i^{(n)}$ can be written directly from\n",
    "the description above. The probability of being in the state $i$ at step $n$\n",
    "is given by the probability of being in any state $j$ at the previous step,\n",
    "and making an accepted transition to $i$ added to the probability of\n",
    "being in the state $i$, making a transition to any state $j$ and\n",
    "rejecting the move:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5de21e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{P}^{(n)}_i = \\sum_j \\left [\n",
    "\\mathbf{P}^{(n-1)}_jT_{j\\rightarrow i} A_{j\\rightarrow i} \n",
    "+\\mathbf{P}^{(n-1)}_iT_{i\\rightarrow j}\\left ( 1- A_{i\\rightarrow j} \\right)\n",
    "\\right ] \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f195d41",
   "metadata": {
    "editable": true
   },
   "source": [
    "Since the probability of making some transition must be 1,\n",
    "$\\sum_j T_{i\\rightarrow j} = 1$, and the above equation becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db31674",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{P}^{(n)}_i = \\mathbf{P}^{(n-1)}_i +\n",
    " \\sum_j \\left [\n",
    "\\mathbf{P}^{(n-1)}_jT_{j\\rightarrow i} A_{j\\rightarrow i} \n",
    "-\\mathbf{P}^{(n-1)}_iT_{i\\rightarrow j}A_{i\\rightarrow j}\n",
    "\\right ] \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfee0d43",
   "metadata": {
    "editable": true
   },
   "source": [
    "For large $n$ we require that $\\mathbf{P}^{(n\\rightarrow \\infty)}_i = p_i$,\n",
    "the desired probability distribution. Taking this limit, gives the\n",
    "balance requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ed447c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\sum_j \\left [\n",
    "p_jT_{j\\rightarrow i} A_{j\\rightarrow i}\n",
    "-p_iT_{i\\rightarrow j}A_{i\\rightarrow j}\n",
    "\\right ] = 0 \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5054687",
   "metadata": {
    "editable": true
   },
   "source": [
    "The balance requirement is very weak. Typically the much stronger detailed\n",
    "balance requirement is enforced, that is rather than the sum being\n",
    "set to zero, we set each term separately to zero and use this\n",
    "to determine the acceptance probabilities. Rearranging, the result is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c004a5ad",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{ A_{j\\rightarrow i}}{A_{i\\rightarrow j}}\n",
    "= \\frac{p_iT_{i\\rightarrow j}}{ p_jT_{j\\rightarrow i}} \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52527480",
   "metadata": {
    "editable": true
   },
   "source": [
    "The Metropolis choice is to maximize the $A$ values, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39addddd",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "A_{j \\rightarrow i} = \\min \\left ( 1,\n",
    "\\frac{p_iT_{i\\rightarrow j}}{ p_jT_{j\\rightarrow i}}\\right ).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a2aecb",
   "metadata": {
    "editable": true
   },
   "source": [
    "Other choices are possible, but they all correspond to multilplying\n",
    "$A_{i\\rightarrow j}$ and $A_{j\\rightarrow i}$ by the same constant\n",
    "smaller than unity.\\footnote{The penalty function method uses just such\n",
    "a factor to compensate for $p_i$ that are evaluated stochastically\n",
    "and are therefore noisy.}\n",
    "\n",
    "Having chosen the acceptance probabilities, we have guaranteed that\n",
    "if the  $\\mathbf{P}_i^{(n)}$ has equilibrated, that is if it is equal to $p_i$,\n",
    "it will remain equilibrated. Next we need to find the circumstances for\n",
    "convergence to equilibrium.\n",
    "\n",
    "The dynamical equation can be written as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c74fa8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{P}^{(n)}_i = \\sum_j M_{ij}\\mathbf{P}^{(n-1)}_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed07feab",
   "metadata": {
    "editable": true
   },
   "source": [
    "with the matrix $M$ given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af5e77",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "M_{ij} = \\delta_{ij}\\left [ 1 -\\sum_k T_{i\\rightarrow k} A_{i \\rightarrow k}\n",
    "\\right ] + T_{j\\rightarrow i} A_{j\\rightarrow i} \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae943e50",
   "metadata": {
    "editable": true
   },
   "source": [
    "Summing over $i$ shows that $\\sum_i M_{ij} = 1$, and since\n",
    "$\\sum_k T_{i\\rightarrow k} = 1$, and $A_{i \\rightarrow k} \\leq 1$, the\n",
    "elements of the matrix satisfy $M_{ij} \\geq 0$. The matrix $M$ is therefore\n",
    "a stochastic matrix.\n",
    "\n",
    "The Metropolis method is simply the power method for computing the\n",
    "right eigenvector of $M$ with the largest magnitude eigenvalue.\n",
    "By construction, the correct probability distribution is a right eigenvector\n",
    "with eigenvalue 1. Therefore, for the Metropolis method to converge\n",
    "to this result, we must show that $M$ has only one eigenvalue with this\n",
    "magnitude, and all other eigenvalues are smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b2bf2f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Importance sampling\n",
    "\n",
    "We need to replace the brute force\n",
    "Metropolis algorithm with a walk in coordinate space biased by the trial wave function.\n",
    "This approach is based on the Fokker-Planck equation and the Langevin equation for generating a trajectory in coordinate space.  The link between the Fokker-Planck equation and the Langevin equations are explained, only partly, in the slides below.\n",
    "An excellent reference on topics like Brownian motion, Markov chains, the Fokker-Planck equation and the Langevin equation is the text by  [Van Kampen](http://www.elsevier.com/books/stochastic-processes-in-physics-and-chemistry/van-kampen/978-0-444-52965-7)\n",
    "Here we will focus first on the implementation part first.\n",
    "\n",
    "For a diffusion process characterized by a time-dependent probability density $P(x,t)$ in one dimension the Fokker-Planck\n",
    "equation reads (for one particle /walker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0ff98c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial P}{\\partial t} = D\\frac{\\partial }{\\partial x}\\left(\\frac{\\partial }{\\partial x} -F\\right)P(x,t),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf09399",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $F$ is a drift term and $D$ is the diffusion coefficient. \n",
    "\n",
    "The new positions in coordinate space are given as the solutions of the Langevin equation using Euler's method, namely,\n",
    "we go from the Langevin equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660f8601",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial x(t)}{\\partial t} = DF(x(t)) +\\eta,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8730de57",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $\\eta$ a random variable,\n",
    "yielding a new position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceb8796",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "y = x+DF(x)\\Delta t +\\xi\\sqrt{\\Delta t},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806dd050",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\xi$ is gaussian random variable and $\\Delta t$ is a chosen time step. \n",
    "The quantity $D$ is, in atomic units, equal to $1/2$ and comes from the factor $1/2$ in the kinetic energy operator. Note that $\\Delta t$ is to be viewed as a parameter. Values of $\\Delta t \\in [0.001,0.01]$ yield in general rather stable values of the ground state energy.  \n",
    "\n",
    "The process of isotropic diffusion characterized by a time-dependent probability density $P(\\mathbf{x},t)$ obeys (as an approximation) the so-called Fokker-Planck equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5482e22f",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial P}{\\partial t} = \\sum_i D\\frac{\\partial }{\\partial \\mathbf{x_i}}\\left(\\frac{\\partial }{\\partial \\mathbf{x_i}} -\\mathbf{F_i}\\right)P(\\mathbf{x},t),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1e93c1",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\mathbf{F_i}$ is the $i^{th}$ component of the drift term (drift velocity) caused by an external potential, and $D$ is the diffusion coefficient. The convergence to a stationary probability density can be obtained by setting the left hand side to zero. The resulting equation will be satisfied if and only if all the terms of the sum are equal zero,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c9b404",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial^2 P}{\\partial {\\mathbf{x_i}^2}} = P\\frac{\\partial}{\\partial {\\mathbf{x_i}}}\\mathbf{F_i} + \\mathbf{F_i}\\frac{\\partial}{\\partial {\\mathbf{x_i}}}P.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ee7127",
   "metadata": {
    "editable": true
   },
   "source": [
    "The drift vector should be of the form $\\mathbf{F} = g(\\mathbf{x}) \\frac{\\partial P}{\\partial \\mathbf{x}}$. Then,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f7c3f8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial^2 P}{\\partial {\\mathbf{x_i}^2}} = P\\frac{\\partial g}{\\partial P}\\left( \\frac{\\partial P}{\\partial {\\mathbf{x}_i}}  \\right)^2 + P g \\frac{\\partial ^2 P}{\\partial {\\mathbf{x}_i^2}}  + g \\left( \\frac{\\partial P}{\\partial {\\mathbf{x}_i}}  \\right)^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99738b29",
   "metadata": {
    "editable": true
   },
   "source": [
    "The condition of stationary density means that the left hand side equals zero. In other words, the terms containing first and second derivatives have to cancel each other. It is possible only if $g = \\frac{1}{P}$, which yields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd2dae9",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{F} = 2\\frac{1}{\\Psi_T}\\nabla\\Psi_T,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf8033d",
   "metadata": {
    "editable": true
   },
   "source": [
    "which is known as the so-called *quantum force*. This term is responsible for pushing the walker towards regions of configuration space where the trial wave function is large, increasing the efficiency of the simulation in contrast to the Metropolis algorithm where the walker has the same probability of moving in every direction.\n",
    "\n",
    "The Fokker-Planck equation yields a (the solution to the equation) transition probability given by the Green's function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805afbad",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "G(y,x,\\Delta t) = \\frac{1}{(4\\pi D\\Delta t)^{3N/2}} \\exp{\\left(-(y-x-D\\Delta t F(x))^2/4D\\Delta t\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81450472",
   "metadata": {
    "editable": true
   },
   "source": [
    "which in turn means that our brute force Metropolis algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d7d506",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "A(y,x) = \\mathrm{min}(1,q(y,x))),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2585120",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $q(y,x) = |\\Psi_T(y)|^2/|\\Psi_T(x)|^2$ is now replaced by the [Metropolis-Hastings algorithm](http://scitation.aip.org/content/aip/journal/jcp/21/6/10.1063/1.1699114) as well as [Hasting's article](http://biomet.oxfordjournals.org/content/57/1/97.abstract),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc10dbe",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "q(y,x) = \\frac{G(x,y,\\Delta t)|\\Psi_T(y)|^2}{G(y,x,\\Delta t)|\\Psi_T(x)|^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6927e5b7",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Importance sampling, program elements\n",
    "\n",
    "The general derivative formula of the Jastrow factor is (the subscript $C$ stands for Correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07007b2e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{1}{\\Psi_C}\\frac{\\partial \\Psi_C}{\\partial x_k} =\n",
    "\\sum_{i=1}^{k-1}\\frac{\\partial g_{ik}}{\\partial x_k}\n",
    "+\n",
    "\\sum_{i=k+1}^{N}\\frac{\\partial g_{ki}}{\\partial x_k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48057450",
   "metadata": {
    "editable": true
   },
   "source": [
    "However, \n",
    "with our written in way which can be reused later as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a000e665",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\Psi_C=\\prod_{i< j}g(r_{ij})= \\exp{\\left\\{\\sum_{i<j}f(r_{ij})\\right\\}},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88007631",
   "metadata": {
    "editable": true
   },
   "source": [
    "the gradient needed for the quantum force and local energy is easy to compute.  \n",
    "The function $f(r_{ij})$ will depends on the system under study. In the equations below we will keep this general form.\n",
    "\n",
    "In the Metropolis/Hasting algorithm, the *acceptance ratio* determines the probability for a particle  to be accepted at a new position. The ratio of the trial wave functions evaluated at the new and current positions is given by ($OB$ for the onebody  part)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ad8eef",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "R \\equiv \\frac{\\Psi_{T}^{new}}{\\Psi_{T}^{old}} = \n",
    "\\frac{\\Psi_{OB}^{new}}{\\Psi_{OB}^{old}}\\frac{\\Psi_{C}^{new}}{\\Psi_{C}^{old}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cc6aeb",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here $\\Psi_{OB}$ is our onebody part (Slater determinant or product of boson single-particle states)  while $\\Psi_{C}$ is our correlation function, or Jastrow factor. \n",
    "We need to optimize the $\\nabla \\Psi_T / \\Psi_T$ ratio and the second derivative as well, that is\n",
    "the $\\mathbf{\\nabla}^2 \\Psi_T/\\Psi_T$ ratio. The first is needed when we compute the so-called quantum force in importance sampling.\n",
    "The second is needed when we compute the kinetic energy term of the local energy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d17a86",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\mathbf{\\mathbf{\\nabla}}  \\Psi}{\\Psi}  = \\frac{\\mathbf{\\nabla}  (\\Psi_{OB} \\, \\Psi_{C})}{\\Psi_{OB} \\, \\Psi_{C}}  =  \\frac{ \\Psi_C \\mathbf{\\nabla}  \\Psi_{OB} + \\Psi_{OB} \\mathbf{\\nabla}  \\Psi_{C}}{\\Psi_{OB} \\Psi_{C}} = \\frac{\\mathbf{\\nabla}  \\Psi_{OB}}{\\Psi_{OB}} + \\frac{\\mathbf{\\nabla}   \\Psi_C}{ \\Psi_C}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad79ce17",
   "metadata": {
    "editable": true
   },
   "source": [
    "The expectation value of the kinetic energy expressed in atomic units for electron $i$ is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84da35f",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\langle \\hat{K}_i \\rangle = -\\frac{1}{2}\\frac{\\langle\\Psi|\\mathbf{\\nabla}_{i}^2|\\Psi \\rangle}{\\langle\\Psi|\\Psi \\rangle},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fcf748",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{K}_i = -\\frac{1}{2}\\frac{\\mathbf{\\nabla}_{i}^{2} \\Psi}{\\Psi}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269ea322",
   "metadata": {
    "editable": true
   },
   "source": [
    "The second derivative which enters the definition of the local energy is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11001b1",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\mathbf{\\nabla}^2 \\Psi}{\\Psi}=\\frac{\\mathbf{\\nabla}^2 \\Psi_{OB}}{\\Psi_{OB}} + \\frac{\\mathbf{\\nabla}^2  \\Psi_C}{ \\Psi_C} + 2 \\frac{\\mathbf{\\nabla}  \\Psi_{OB}}{\\Psi_{OB}}\\cdot\\frac{\\mathbf{\\nabla}   \\Psi_C}{ \\Psi_C}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6034f174",
   "metadata": {
    "editable": true
   },
   "source": [
    "We discuss here how to calculate these quantities in an optimal way,\n",
    "\n",
    "We have defined the correlated function as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f8a0f2",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\Psi_C=\\prod_{i< j}g(r_{ij})=\\prod_{i< j}^Ng(r_{ij})= \\prod_{i=1}^N\\prod_{j=i+1}^Ng(r_{ij}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa48031",
   "metadata": {
    "editable": true
   },
   "source": [
    "with \n",
    "$r_{ij}=|\\mathbf{r}_i-\\mathbf{r}_j|=\\sqrt{(x_i-x_j)^2+(y_i-y_j)^2+(z_i-z_j)^2}$ in three dimensions or\n",
    "$r_{ij}=|\\mathbf{r}_i-\\mathbf{r}_j|=\\sqrt{(x_i-x_j)^2+(y_i-y_j)^2}$ if we work with two-dimensional systems.\n",
    "\n",
    "In our particular case we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1a41b7",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\Psi_C=\\prod_{i< j}g(r_{ij})=\\exp{\\left\\{\\sum_{i<j}f(r_{ij})\\right\\}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabac25e",
   "metadata": {
    "editable": true
   },
   "source": [
    "The total number of different relative distances $r_{ij}$ is $N(N-1)/2$. In a matrix storage format, the relative distances  form a strictly upper triangular matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cc889b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{r} \\equiv \\begin{pmatrix}\n",
    "  0 & r_{1,2} & r_{1,3} & \\cdots & r_{1,N} \\\\\n",
    "  \\vdots & 0       & r_{2,3} & \\cdots & r_{2,N} \\\\\n",
    "  \\vdots & \\vdots  & 0  & \\ddots & \\vdots  \\\\\n",
    "  \\vdots & \\vdots  & \\vdots  & \\ddots  & r_{N-1,N} \\\\\n",
    "  0 & 0  & 0  & \\cdots  & 0\n",
    " \\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e758b464",
   "metadata": {
    "editable": true
   },
   "source": [
    "This applies to  $\\mathbf{g} = \\mathbf{g}(r_{ij})$ as well. \n",
    "\n",
    "In our algorithm we will move one particle  at the time, say the $kth$-particle.  This sampling will be seen to be particularly efficient when we are going to compute a Slater determinant. \n",
    "\n",
    "We have that the ratio between Jastrow factors $R_C$ is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2452fd78",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "R_{C} = \\frac{\\Psi_{C}^\\mathrm{new}}{\\Psi_{C}^\\mathrm{cur}} =\n",
    "\\prod_{i=1}^{k-1}\\frac{g_{ik}^\\mathrm{new}}{g_{ik}^\\mathrm{cur}}\n",
    "\\prod_{i=k+1}^{N}\\frac{ g_{ki}^\\mathrm{new}} {g_{ki}^\\mathrm{cur}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe81541",
   "metadata": {
    "editable": true
   },
   "source": [
    "For the Pade-Jastrow form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e5856",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "R_{C} = \\frac{\\Psi_{C}^\\mathrm{new}}{\\Psi_{C}^\\mathrm{cur}} = \n",
    "\\frac{\\exp{U_{new}}}{\\exp{U_{cur}}} = \\exp{\\Delta U},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2b00dd",
   "metadata": {
    "editable": true
   },
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1a1c8b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\Delta U =\n",
    "\\sum_{i=1}^{k-1}\\big(f_{ik}^\\mathrm{new}-f_{ik}^\\mathrm{cur}\\big)\n",
    "+\n",
    "\\sum_{i=k+1}^{N}\\big(f_{ki}^\\mathrm{new}-f_{ki}^\\mathrm{cur}\\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f3c5c1",
   "metadata": {
    "editable": true
   },
   "source": [
    "One needs to develop a special algorithm \n",
    "that runs only through the elements of the upper triangular\n",
    "matrix $\\mathbf{g}$ and have $k$ as an index. \n",
    "\n",
    "The expression to be derived in the following is of interest when computing the quantum force and the kinetic energy. It has the form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf98b7ef",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\mathbf{\\nabla}_i\\Psi_C}{\\Psi_C} = \\frac{1}{\\Psi_C}\\frac{\\partial \\Psi_C}{\\partial x_i},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc865189",
   "metadata": {
    "editable": true
   },
   "source": [
    "for all dimensions and with $i$ running over all particles.\n",
    "\n",
    "For the first derivative only $N-1$ terms survive the ratio because the $g$-terms that are not differentiated cancel with their corresponding ones in the denominator. Then,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3a1a65",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{1}{\\Psi_C}\\frac{\\partial \\Psi_C}{\\partial x_k} =\n",
    "\\sum_{i=1}^{k-1}\\frac{1}{g_{ik}}\\frac{\\partial g_{ik}}{\\partial x_k}\n",
    "+\n",
    "\\sum_{i=k+1}^{N}\\frac{1}{g_{ki}}\\frac{\\partial g_{ki}}{\\partial x_k}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929feb7f",
   "metadata": {
    "editable": true
   },
   "source": [
    "An equivalent equation is obtained for the exponential form after replacing $g_{ij}$ by $\\exp(f_{ij})$, yielding:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57139d00",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{1}{\\Psi_C}\\frac{\\partial \\Psi_C}{\\partial x_k} =\n",
    "\\sum_{i=1}^{k-1}\\frac{\\partial g_{ik}}{\\partial x_k}\n",
    "+\n",
    "\\sum_{i=k+1}^{N}\\frac{\\partial g_{ki}}{\\partial x_k},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94d0d89",
   "metadata": {
    "editable": true
   },
   "source": [
    "with both expressions scaling as $\\mathcal{O}(N)$.\n",
    "\n",
    "Using the identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c65835b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial}{\\partial x_i}g_{ij} = -\\frac{\\partial}{\\partial x_j}g_{ij},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8fdff5",
   "metadata": {
    "editable": true
   },
   "source": [
    "we get expressions where all the derivatives acting on the particle  are represented by the *second* index of $g$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d02ca24",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{1}{\\Psi_C}\\frac{\\partial \\Psi_C}{\\partial x_k} =\n",
    "\\sum_{i=1}^{k-1}\\frac{1}{g_{ik}}\\frac{\\partial g_{ik}}{\\partial x_k}\n",
    "-\\sum_{i=k+1}^{N}\\frac{1}{g_{ki}}\\frac{\\partial g_{ki}}{\\partial x_i},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af27c50",
   "metadata": {
    "editable": true
   },
   "source": [
    "and for the exponential case:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72ad976",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{1}{\\Psi_C}\\frac{\\partial \\Psi_C}{\\partial x_k} =\n",
    "\\sum_{i=1}^{k-1}\\frac{\\partial g_{ik}}{\\partial x_k}\n",
    "-\\sum_{i=k+1}^{N}\\frac{\\partial g_{ki}}{\\partial x_i}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711df109",
   "metadata": {
    "editable": true
   },
   "source": [
    "For correlation forms depending only on the scalar distances $r_{ij}$ we can use the chain rule. Noting that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a509ed12",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial g_{ij}}{\\partial x_j} = \\frac{\\partial g_{ij}}{\\partial r_{ij}} \\frac{\\partial r_{ij}}{\\partial x_j} = \\frac{x_j - x_i}{r_{ij}} \\frac{\\partial g_{ij}}{\\partial r_{ij}},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d5afb8",
   "metadata": {
    "editable": true
   },
   "source": [
    "we arrive at"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09d1812",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{1}{\\Psi_C}\\frac{\\partial \\Psi_C}{\\partial x_k} = \n",
    "\\sum_{i=1}^{k-1}\\frac{1}{g_{ik}} \\frac{\\mathbf{r_{ik}}}{r_{ik}} \\frac{\\partial g_{ik}}{\\partial r_{ik}}\n",
    "-\\sum_{i=k+1}^{N}\\frac{1}{g_{ki}}\\frac{\\mathbf{r_{ki}}}{r_{ki}}\\frac{\\partial g_{ki}}{\\partial r_{ki}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770af1c0",
   "metadata": {
    "editable": true
   },
   "source": [
    "Note that for the Pade-Jastrow form we can set $g_{ij} \\equiv g(r_{ij}) = e^{f(r_{ij})} = e^{f_{ij}}$ and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0381f5",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial g_{ij}}{\\partial r_{ij}} = g_{ij} \\frac{\\partial f_{ij}}{\\partial r_{ij}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958b203e",
   "metadata": {
    "editable": true
   },
   "source": [
    "Therefore,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd53f023",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{1}{\\Psi_{C}}\\frac{\\partial \\Psi_{C}}{\\partial x_k} =\n",
    "\\sum_{i=1}^{k-1}\\frac{\\mathbf{r_{ik}}}{r_{ik}}\\frac{\\partial f_{ik}}{\\partial r_{ik}}\n",
    "-\\sum_{i=k+1}^{N}\\frac{\\mathbf{r_{ki}}}{r_{ki}}\\frac{\\partial f_{ki}}{\\partial r_{ki}},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee3e084",
   "metadata": {
    "editable": true
   },
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7502bd4",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{r}_{ij} = |\\mathbf{r}_j - \\mathbf{r}_i| = (x_j - x_i)\\mathbf{e}_1 + (y_j - y_i)\\mathbf{e}_2 + (z_j - z_i)\\mathbf{e}_3\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffcc830",
   "metadata": {
    "editable": true
   },
   "source": [
    "is the relative distance. \n",
    "\n",
    "The second derivative of the Jastrow factor divided by the Jastrow factor (the way it enters the kinetic energy) is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb63dac",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\left[\\frac{\\mathbf{\\nabla}^2 \\Psi_C}{\\Psi_C}\\right]_x =\\  \n",
    "2\\sum_{k=1}^{N}\n",
    "\\sum_{i=1}^{k-1}\\frac{\\partial^2 g_{ik}}{\\partial x_k^2}\\ +\\ \n",
    "\\sum_{k=1}^N\n",
    "\\left(\n",
    "\\sum_{i=1}^{k-1}\\frac{\\partial g_{ik}}{\\partial x_k} -\n",
    "\\sum_{i=k+1}^{N}\\frac{\\partial g_{ki}}{\\partial x_i}\n",
    "\\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd153832",
   "metadata": {
    "editable": true
   },
   "source": [
    "But we have a simple form for the function, namely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c602b3a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\Psi_{C}=\\prod_{i< j}\\exp{f(r_{ij})},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa22f171",
   "metadata": {
    "editable": true
   },
   "source": [
    "and it is easy to see that for particle  $k$\n",
    "we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db1d913",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\mathbf{\\nabla}^2_k \\Psi_C}{\\Psi_C }=\n",
    "\\sum_{ij\\ne k}\\frac{(\\mathbf{r}_k-\\mathbf{r}_i)(\\mathbf{r}_k-\\mathbf{r}_j)}{r_{ki}r_{kj}}f'(r_{ki})f'(r_{kj})+\n",
    "\\sum_{j\\ne k}\\left( f''(r_{kj})+\\frac{2}{r_{kj}}f'(r_{kj})\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5be4360",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Code example for two electrons in a quantum dots with importance sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5b8c08a",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 2-electron VMC code for 2dim quantum dot with importance sampling\n",
    "# Using gaussian rng for new positions and Metropolis- Hastings \n",
    "# No energy minimization\n",
    "from math import exp, sqrt\n",
    "from random import random, seed, normalvariate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "\n",
    "# Trial wave function for the 2-electron quantum dot in two dims\n",
    "def WaveFunction(r,alpha,beta):\n",
    "    r1 = r[0,0]**2 + r[0,1]**2\n",
    "    r2 = r[1,0]**2 + r[1,1]**2\n",
    "    r12 = sqrt((r[0,0]-r[1,0])**2 + (r[0,1]-r[1,1])**2)\n",
    "    deno = r12/(1+beta*r12)\n",
    "    return exp(-0.5*alpha*(r1+r2)+deno)\n",
    "\n",
    "# Local energy  for the 2-electron quantum dot in two dims, using analytical local energy\n",
    "def LocalEnergy(r,alpha,beta):\n",
    "    \n",
    "    r1 = (r[0,0]**2 + r[0,1]**2)\n",
    "    r2 = (r[1,0]**2 + r[1,1]**2)\n",
    "    r12 = sqrt((r[0,0]-r[1,0])**2 + (r[0,1]-r[1,1])**2)\n",
    "    deno = 1.0/(1+beta*r12)\n",
    "    deno2 = deno*deno\n",
    "    return 0.5*(1-alpha*alpha)*(r1 + r2) +2.0*alpha + 1.0/r12+deno2*(alpha*r12-deno2+2*beta*deno-1.0/r12)\n",
    "\n",
    "# Setting up the quantum force for the two-electron quantum dot, recall that it is a vector\n",
    "def QuantumForce(r,alpha,beta):\n",
    "\n",
    "    qforce = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    r12 = sqrt((r[0,0]-r[1,0])**2 + (r[0,1]-r[1,1])**2)\n",
    "    deno = 1.0/(1+beta*r12)\n",
    "    qforce[0,:] = -2*r[0,:]*alpha*(r[0,:]-r[1,:])*deno*deno/r12\n",
    "    qforce[1,:] = -2*r[1,:]*alpha*(r[1,:]-r[0,:])*deno*deno/r12\n",
    "    return qforce\n",
    "    \n",
    "# The Monte Carlo sampling with the Metropolis algo\n",
    "def MonteCarloSampling():\n",
    "\n",
    "    NumberMCcycles= 100000\n",
    "    # Parameters in the Fokker-Planck simulation of the quantum force\n",
    "    D = 0.5\n",
    "    TimeStep = 0.05\n",
    "    # positions\n",
    "    PositionOld = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    PositionNew = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    # Quantum force\n",
    "    QuantumForceOld = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    QuantumForceNew = np.zeros((NumberParticles,Dimension), np.double)\n",
    "\n",
    "    # seed for rng generator \n",
    "    seed()\n",
    "    # start variational parameter  loops, two parameters here\n",
    "    alpha = 0.9\n",
    "    for ia in range(MaxVariations):\n",
    "        alpha += .025\n",
    "        AlphaValues[ia] = alpha\n",
    "        beta = 0.2 \n",
    "        for jb in range(MaxVariations):\n",
    "            beta += .01\n",
    "            BetaValues[jb] = beta\n",
    "            energy = energy2 = 0.0\n",
    "            DeltaE = 0.0\n",
    "            #Initial position\n",
    "            for i in range(NumberParticles):\n",
    "                for j in range(Dimension):\n",
    "                    PositionOld[i,j] = normalvariate(0.0,1.0)*sqrt(TimeStep)\n",
    "            wfold = WaveFunction(PositionOld,alpha,beta)\n",
    "            QuantumForceOld = QuantumForce(PositionOld,alpha, beta)\n",
    "\n",
    "            #Loop over MC MCcycles\n",
    "            for MCcycle in range(NumberMCcycles):\n",
    "                #Trial position moving one particle at the time\n",
    "                for i in range(NumberParticles):\n",
    "                    for j in range(Dimension):\n",
    "                        PositionNew[i,j] = PositionOld[i,j]+normalvariate(0.0,1.0)*sqrt(TimeStep)+\\\n",
    "                                           QuantumForceOld[i,j]*TimeStep*D\n",
    "                    wfnew = WaveFunction(PositionNew,alpha,beta)\n",
    "                    QuantumForceNew = QuantumForce(PositionNew,alpha, beta)\n",
    "                    GreensFunction = 0.0\n",
    "                    for j in range(Dimension):\n",
    "                        GreensFunction += 0.5*(QuantumForceOld[i,j]+QuantumForceNew[i,j])*\\\n",
    "\t                              (D*TimeStep*0.5*(QuantumForceOld[i,j]-QuantumForceNew[i,j])-\\\n",
    "                                      PositionNew[i,j]+PositionOld[i,j])\n",
    "      \n",
    "                    GreensFunction = exp(GreensFunction)\n",
    "                    ProbabilityRatio = GreensFunction*wfnew**2/wfold**2\n",
    "                    #Metropolis-Hastings test to see whether we accept the move\n",
    "                    if random() <= ProbabilityRatio:\n",
    "                       for j in range(Dimension):\n",
    "                           PositionOld[i,j] = PositionNew[i,j]\n",
    "                           QuantumForceOld[i,j] = QuantumForceNew[i,j]\n",
    "                       wfold = wfnew\n",
    "                DeltaE = LocalEnergy(PositionOld,alpha,beta)\n",
    "                energy += DeltaE\n",
    "                energy2 += DeltaE**2\n",
    "            # We calculate mean, variance and error (no blocking applied)\n",
    "            energy /= NumberMCcycles\n",
    "            energy2 /= NumberMCcycles\n",
    "            variance = energy2 - energy**2\n",
    "            error = sqrt(variance/NumberMCcycles)\n",
    "            Energies[ia,jb] = energy    \n",
    "            outfile.write('%f %f %f %f %f\\n' %(alpha,beta,energy,variance,error))\n",
    "    return Energies, AlphaValues, BetaValues\n",
    "\n",
    "\n",
    "#Here starts the main program with variable declarations\n",
    "NumberParticles = 2\n",
    "Dimension = 2\n",
    "MaxVariations = 10\n",
    "Energies = np.zeros((MaxVariations,MaxVariations))\n",
    "AlphaValues = np.zeros(MaxVariations)\n",
    "BetaValues = np.zeros(MaxVariations)\n",
    "(Energies, AlphaValues, BetaValues) = MonteCarloSampling()\n",
    "outfile.close()\n",
    "# Prepare for plots\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "# Plot the surface.\n",
    "X, Y = np.meshgrid(AlphaValues, BetaValues)\n",
    "surf = ax.plot_surface(X, Y, Energies,cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
    "# Customize the z axis.\n",
    "zmin = np.matrix(Energies).min()\n",
    "zmax = np.matrix(Energies).max()\n",
    "ax.set_zlim(zmin, zmax)\n",
    "ax.set_xlabel(r'$\\alpha$')\n",
    "ax.set_ylabel(r'$\\beta$')\n",
    "ax.set_zlabel(r'$\\langle E \\rangle$')\n",
    "ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "# Add a color bar which maps values to colors.\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25270786",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Gradient Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99425d2d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Top-down start\n",
    "\n",
    "* We will start with a top-down view, with a simple harmonic oscillator problem in one dimension as case.\n",
    "\n",
    "* Thereafter we continue with implementing the simplest possible steepest descent approach to our two-electron problem with an electrostatic (Coulomb) interaction. Our code includes also importance sampling. The simple Python code here illustrates the basic elements which need to be included in our own code.\n",
    "\n",
    "* Then we move on to the mathematical description of various gradient methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8883bba8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Motivation\n",
    "Our aim with this part is to be able to\n",
    "* find an optimal value for the variational parameters using only some few Monte Carlo cycles\n",
    "\n",
    "* use these optimal values for the variational parameters to perform a large-scale Monte Carlo calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8345450f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Simple example and demonstration\n",
    "\n",
    "Let us illustrate what is needed in our calculations using a simple example, the harmonic oscillator in one dimension.\n",
    "For the harmonic oscillator in one-dimension we have a  trial wave function and probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3a45d1",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\psi_T(x;\\alpha) = \\exp{-(\\frac{1}{2}\\alpha^2x^2)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0f2eeb",
   "metadata": {
    "editable": true
   },
   "source": [
    "which results in a local energy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc18d96b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{1}{2}\\left(\\alpha^2+x^2(1-\\alpha^4)\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4e916d",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can compare our numerically calculated energies with the exact energy as function of $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f159fa5",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\overline{E}[\\alpha] = \\frac{1}{4}\\left(\\alpha^2+\\frac{1}{\\alpha^2}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98bfa33",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Simple example and demonstration\n",
    "The derivative of the energy with respect to $\\alpha$ gives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e85e765",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{d\\langle  E_L[\\alpha]\\rangle}{d\\alpha} = \\frac{1}{2}\\alpha-\\frac{1}{2\\alpha^3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fe0892",
   "metadata": {
    "editable": true
   },
   "source": [
    "and a second derivative which is always positive (meaning that we find a minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded57ad1",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{d^2\\langle  E_L[\\alpha]\\rangle}{d\\alpha^2} = \\frac{1}{2}+\\frac{3}{2\\alpha^4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6ca082",
   "metadata": {
    "editable": true
   },
   "source": [
    "The condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c308a7",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{d\\langle  E_L[\\alpha]\\rangle}{d\\alpha} = 0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50e6d02",
   "metadata": {
    "editable": true
   },
   "source": [
    "gives the optimal $\\alpha=1$, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add104c1",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 1: Find the local energy for the harmonic oscillator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37001a05",
   "metadata": {
    "editable": true
   },
   "source": [
    "**a)**\n",
    "Derive the local energy for the harmonic oscillator in one dimension and find its expectation value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dfe904",
   "metadata": {
    "editable": true
   },
   "source": [
    "**b)**\n",
    "Show also that the optimal value of optimal $\\alpha=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6196300d",
   "metadata": {
    "editable": true
   },
   "source": [
    "**c)**\n",
    "Repeat the above steps in two dimensions for $N$ bosons or electrons. What is the optimal value of $\\alpha$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0662f876",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Variance in the simple model\n",
    "We can also minimize the variance. In our simple model the variance is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc147d1e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\sigma^2[\\alpha]=\\frac{1}{4}\\left(1+(1-\\alpha^4)^2\\frac{3}{4\\alpha^4}\\right)-\\overline{E}^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de901d96",
   "metadata": {
    "editable": true
   },
   "source": [
    "which yields a second derivative which is always positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1794c64c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Computing the derivatives\n",
    "\n",
    "In general we end up computing the expectation value of the energy in terms \n",
    "of some parameters $\\alpha_0,\\alpha_1,\\dots,\\alpha_n$\n",
    "and we search for a minimum in this multi-variable parameter space.  \n",
    "This leads to an energy minimization problem *where we need the derivative of the energy as a function of the variational parameters*.\n",
    "\n",
    "In the above example this was easy and we were able to find the expression for the derivative by simple derivations. \n",
    "However, in our actual calculations the energy is represented by a multi-dimensional integral with several variational parameters.\n",
    "How can we can then obtain the derivatives of the energy with respect to the variational parameters without having \n",
    "to resort to expensive numerical derivations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e471b5",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Expressions for finding the derivatives of the local energy\n",
    "\n",
    "To find the derivatives of the local energy expectation value as function of the variational parameters, we can use the chain rule and the hermiticity of the Hamiltonian.  \n",
    "\n",
    "Let us define"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f6e6d6",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\bar{E}_{\\alpha}=\\frac{d\\langle  E_L[\\alpha]\\rangle}{d\\alpha}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a85dde",
   "metadata": {
    "editable": true
   },
   "source": [
    "as the derivative of the energy with respect to the variational parameter $\\alpha$ (we limit ourselves to one parameter only).\n",
    "In the above example this was easy and we obtain a simple expression for the derivative.\n",
    "We define also the derivative of the trial function (skipping the subindex $T$) as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c34823c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\bar{\\psi}_{\\alpha}=\\frac{d\\psi[\\alpha]\\rangle}{d\\alpha}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a313e9cd",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Derivatives of the local energy\n",
    "The elements of the gradient of the local energy are then (using the chain rule and the hermiticity of the Hamiltonian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cbf1b8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\bar{E}_{\\alpha} = 2\\left( \\langle \\frac{\\bar{\\psi}_{\\alpha}}{\\psi[\\alpha]}E_L[\\alpha]\\rangle -\\langle \\frac{\\bar{\\psi}_{\\alpha}}{\\psi[\\alpha]}\\rangle\\langle E_L[\\alpha] \\rangle\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff3d1da",
   "metadata": {
    "editable": true
   },
   "source": [
    "From a computational point of view it means that you need to compute the expectation values of"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f501b12",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\langle \\frac{\\bar{\\psi}_{\\alpha}}{\\psi[\\alpha]}E_L[\\alpha]\\rangle,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8f331d",
   "metadata": {
    "editable": true
   },
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd33d64",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\langle \\frac{\\bar{\\psi}_{\\alpha}}{\\psi[\\alpha]}\\rangle\\langle E_L[\\alpha]\\rangle\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcbb3c5",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 2: General expression for the derivative of the energy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c263829c",
   "metadata": {
    "editable": true
   },
   "source": [
    "**a)**\n",
    "Show that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0430e51",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\bar{E}_{\\alpha} = 2\\left( \\langle \\frac{\\bar{\\psi}_{\\alpha}}{\\psi[\\alpha]}E_L[\\alpha]\\rangle -\\langle \\frac{\\bar{\\psi}_{\\alpha}}{\\psi[\\alpha]}\\rangle\\langle E_L[\\alpha] \\rangle\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fb3967",
   "metadata": {
    "editable": true
   },
   "source": [
    "**b)**\n",
    "Find the corresponding expression for the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639fd064",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Python program for 2-electrons in 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96e3b2b7",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 2-electron VMC code for 2dim quantum dot with importance sampling\n",
    "# Using gaussian rng for new positions and Metropolis- Hastings \n",
    "# Added energy minimization with gradient descent using fixed step size\n",
    "# To do: replace with optimization codes from scipy and/or use stochastic gradient descent\n",
    "from math import exp, sqrt\n",
    "from random import random, seed, normalvariate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "# Trial wave function for the 2-electron quantum dot in two dims\n",
    "def WaveFunction(r,alpha,beta):\n",
    "    r1 = r[0,0]**2 + r[0,1]**2\n",
    "    r2 = r[1,0]**2 + r[1,1]**2\n",
    "    r12 = sqrt((r[0,0]-r[1,0])**2 + (r[0,1]-r[1,1])**2)\n",
    "    deno = r12/(1+beta*r12)\n",
    "    return exp(-0.5*alpha*(r1+r2)+deno)\n",
    "\n",
    "# Local energy  for the 2-electron quantum dot in two dims, using analytical local energy\n",
    "def LocalEnergy(r,alpha,beta):\n",
    "    \n",
    "    r1 = (r[0,0]**2 + r[0,1]**2)\n",
    "    r2 = (r[1,0]**2 + r[1,1]**2)\n",
    "    r12 = sqrt((r[0,0]-r[1,0])**2 + (r[0,1]-r[1,1])**2)\n",
    "    deno = 1.0/(1+beta*r12)\n",
    "    deno2 = deno*deno\n",
    "    return 0.5*(1-alpha*alpha)*(r1 + r2) +2.0*alpha + 1.0/r12+deno2*(alpha*r12-deno2+2*beta*deno-1.0/r12)\n",
    "\n",
    "# Derivate of wave function ansatz as function of variational parameters\n",
    "def DerivativeWFansatz(r,alpha,beta):\n",
    "    \n",
    "    WfDer  = np.zeros((2), np.double)\n",
    "    r1 = (r[0,0]**2 + r[0,1]**2)\n",
    "    r2 = (r[1,0]**2 + r[1,1]**2)\n",
    "    r12 = sqrt((r[0,0]-r[1,0])**2 + (r[0,1]-r[1,1])**2)\n",
    "    deno = 1.0/(1+beta*r12)\n",
    "    deno2 = deno*deno\n",
    "    WfDer[0] = -0.5*(r1+r2)\n",
    "    WfDer[1] = -r12*r12*deno2\n",
    "    return  WfDer\n",
    "\n",
    "# Setting up the quantum force for the two-electron quantum dot, recall that it is a vector\n",
    "def QuantumForce(r,alpha,beta):\n",
    "\n",
    "    qforce = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    r12 = sqrt((r[0,0]-r[1,0])**2 + (r[0,1]-r[1,1])**2)\n",
    "    deno = 1.0/(1+beta*r12)\n",
    "    qforce[0,:] = -2*r[0,:]*alpha*(r[0,:]-r[1,:])*deno*deno/r12\n",
    "    qforce[1,:] = -2*r[1,:]*alpha*(r[1,:]-r[0,:])*deno*deno/r12\n",
    "    return qforce\n",
    "    \n",
    "\n",
    "# Computing the derivative of the energy and the energy \n",
    "def EnergyMinimization(alpha, beta):\n",
    "\n",
    "    NumberMCcycles= 10000\n",
    "    # Parameters in the Fokker-Planck simulation of the quantum force\n",
    "    D = 0.5\n",
    "    TimeStep = 0.05\n",
    "    # positions\n",
    "    PositionOld = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    PositionNew = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    # Quantum force\n",
    "    QuantumForceOld = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    QuantumForceNew = np.zeros((NumberParticles,Dimension), np.double)\n",
    "\n",
    "    # seed for rng generator \n",
    "    seed()\n",
    "    energy = 0.0\n",
    "    DeltaE = 0.0\n",
    "    EnergyDer = np.zeros((2), np.double)\n",
    "    DeltaPsi = np.zeros((2), np.double)\n",
    "    DerivativePsiE = np.zeros((2), np.double)\n",
    "    #Initial position\n",
    "    for i in range(NumberParticles):\n",
    "        for j in range(Dimension):\n",
    "            PositionOld[i,j] = normalvariate(0.0,1.0)*sqrt(TimeStep)\n",
    "    wfold = WaveFunction(PositionOld,alpha,beta)\n",
    "    QuantumForceOld = QuantumForce(PositionOld,alpha, beta)\n",
    "\n",
    "    #Loop over MC MCcycles\n",
    "    for MCcycle in range(NumberMCcycles):\n",
    "        #Trial position moving one particle at the time\n",
    "        for i in range(NumberParticles):\n",
    "            for j in range(Dimension):\n",
    "                PositionNew[i,j] = PositionOld[i,j]+normalvariate(0.0,1.0)*sqrt(TimeStep)+\\\n",
    "                                       QuantumForceOld[i,j]*TimeStep*D\n",
    "            wfnew = WaveFunction(PositionNew,alpha,beta)\n",
    "            QuantumForceNew = QuantumForce(PositionNew,alpha, beta)\n",
    "            GreensFunction = 0.0\n",
    "            for j in range(Dimension):\n",
    "                GreensFunction += 0.5*(QuantumForceOld[i,j]+QuantumForceNew[i,j])*\\\n",
    "\t                              (D*TimeStep*0.5*(QuantumForceOld[i,j]-QuantumForceNew[i,j])-\\\n",
    "                                      PositionNew[i,j]+PositionOld[i,j])\n",
    "      \n",
    "            GreensFunction = exp(GreensFunction)\n",
    "            ProbabilityRatio = GreensFunction*wfnew**2/wfold**2\n",
    "            #Metropolis-Hastings test to see whether we accept the move\n",
    "            if random() <= ProbabilityRatio:\n",
    "                for j in range(Dimension):\n",
    "                    PositionOld[i,j] = PositionNew[i,j]\n",
    "                    QuantumForceOld[i,j] = QuantumForceNew[i,j]\n",
    "                wfold = wfnew\n",
    "        DeltaE = LocalEnergy(PositionOld,alpha,beta)\n",
    "        DerPsi = DerivativeWFansatz(PositionOld,alpha,beta)\n",
    "        DeltaPsi += DerPsi\n",
    "        energy += DeltaE\n",
    "        DerivativePsiE += DerPsi*DeltaE\n",
    "            \n",
    "    # We calculate mean values\n",
    "    energy /= NumberMCcycles\n",
    "    DerivativePsiE /= NumberMCcycles\n",
    "    DeltaPsi /= NumberMCcycles\n",
    "    EnergyDer  = 2*(DerivativePsiE-DeltaPsi*energy)\n",
    "    return energy, EnergyDer\n",
    "\n",
    "\n",
    "#Here starts the main program with variable declarations\n",
    "NumberParticles = 2\n",
    "Dimension = 2\n",
    "# guess for variational parameters\n",
    "alpha = 0.9\n",
    "beta = 0.2\n",
    "# Set up iteration using gradient descent method\n",
    "Energy = 0\n",
    "EDerivative = np.zeros((2), np.double)\n",
    "eta = 0.01\n",
    "Niterations = 50\n",
    "# \n",
    "for iter in range(Niterations):\n",
    "    Energy, EDerivative = EnergyMinimization(alpha,beta)\n",
    "    alphagradient = EDerivative[0]\n",
    "    betagradient = EDerivative[1]\n",
    "    alpha -= eta*alphagradient\n",
    "    beta -= eta*betagradient \n",
    "\n",
    "print(alpha, beta)\n",
    "print(Energy, EDerivative[0], EDerivative[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e963de60",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Using Broyden's algorithm in scipy\n",
    "The following function uses the above described BFGS algorithm. Here we have defined a function which calculates the energy and a function which computes the first derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c570525",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 2-electron VMC code for 2dim quantum dot with importance sampling\n",
    "# Using gaussian rng for new positions and Metropolis- Hastings \n",
    "# Added energy minimization using the BFGS algorithm, see p. 136 of https://www.springer.com/it/book/9780387303031\n",
    "from math import exp, sqrt\n",
    "from random import random, seed, normalvariate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from scipy.optimize import minimize\n",
    "import sys\n",
    "\n",
    "\n",
    "# Trial wave function for the 2-electron quantum dot in two dims\n",
    "def WaveFunction(r,alpha,beta):\n",
    "    r1 = r[0,0]**2 + r[0,1]**2\n",
    "    r2 = r[1,0]**2 + r[1,1]**2\n",
    "    r12 = sqrt((r[0,0]-r[1,0])**2 + (r[0,1]-r[1,1])**2)\n",
    "    deno = r12/(1+beta*r12)\n",
    "    return exp(-0.5*alpha*(r1+r2)+deno)\n",
    "\n",
    "# Local energy  for the 2-electron quantum dot in two dims, using analytical local energy\n",
    "def LocalEnergy(r,alpha,beta):\n",
    "    \n",
    "    r1 = (r[0,0]**2 + r[0,1]**2)\n",
    "    r2 = (r[1,0]**2 + r[1,1]**2)\n",
    "    r12 = sqrt((r[0,0]-r[1,0])**2 + (r[0,1]-r[1,1])**2)\n",
    "    deno = 1.0/(1+beta*r12)\n",
    "    deno2 = deno*deno\n",
    "    return 0.5*(1-alpha*alpha)*(r1 + r2) +2.0*alpha + 1.0/r12+deno2*(alpha*r12-deno2+2*beta*deno-1.0/r12)\n",
    "\n",
    "# Derivate of wave function ansatz as function of variational parameters\n",
    "def DerivativeWFansatz(r,alpha,beta):\n",
    "    \n",
    "    WfDer  = np.zeros((2), np.double)\n",
    "    r1 = (r[0,0]**2 + r[0,1]**2)\n",
    "    r2 = (r[1,0]**2 + r[1,1]**2)\n",
    "    r12 = sqrt((r[0,0]-r[1,0])**2 + (r[0,1]-r[1,1])**2)\n",
    "    deno = 1.0/(1+beta*r12)\n",
    "    deno2 = deno*deno\n",
    "    WfDer[0] = -0.5*(r1+r2)\n",
    "    WfDer[1] = -r12*r12*deno2\n",
    "    return  WfDer\n",
    "\n",
    "# Setting up the quantum force for the two-electron quantum dot, recall that it is a vector\n",
    "def QuantumForce(r,alpha,beta):\n",
    "\n",
    "    qforce = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    r12 = sqrt((r[0,0]-r[1,0])**2 + (r[0,1]-r[1,1])**2)\n",
    "    deno = 1.0/(1+beta*r12)\n",
    "    qforce[0,:] = -2*r[0,:]*alpha*(r[0,:]-r[1,:])*deno*deno/r12\n",
    "    qforce[1,:] = -2*r[1,:]*alpha*(r[1,:]-r[0,:])*deno*deno/r12\n",
    "    return qforce\n",
    "    \n",
    "\n",
    "# Computing the derivative of the energy and the energy \n",
    "def EnergyDerivative(x0):\n",
    "\n",
    "    \n",
    "    # Parameters in the Fokker-Planck simulation of the quantum force\n",
    "    D = 0.5\n",
    "    TimeStep = 0.05\n",
    "    NumberMCcycles= 10000\n",
    "    # positions\n",
    "    PositionOld = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    PositionNew = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    # Quantum force\n",
    "    QuantumForceOld = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    QuantumForceNew = np.zeros((NumberParticles,Dimension), np.double)\n",
    "\n",
    "    energy = 0.0\n",
    "    DeltaE = 0.0\n",
    "    alpha = x0[0]\n",
    "    beta = x0[1]\n",
    "    EnergyDer = 0.0\n",
    "    DeltaPsi = 0.0\n",
    "    DerivativePsiE = 0.0 \n",
    "    #Initial position\n",
    "    for i in range(NumberParticles):\n",
    "        for j in range(Dimension):\n",
    "            PositionOld[i,j] = normalvariate(0.0,1.0)*sqrt(TimeStep)\n",
    "    wfold = WaveFunction(PositionOld,alpha,beta)\n",
    "    QuantumForceOld = QuantumForce(PositionOld,alpha, beta)\n",
    "\n",
    "    #Loop over MC MCcycles\n",
    "    for MCcycle in range(NumberMCcycles):\n",
    "        #Trial position moving one particle at the time\n",
    "        for i in range(NumberParticles):\n",
    "            for j in range(Dimension):\n",
    "                PositionNew[i,j] = PositionOld[i,j]+normalvariate(0.0,1.0)*sqrt(TimeStep)+\\\n",
    "                                       QuantumForceOld[i,j]*TimeStep*D\n",
    "            wfnew = WaveFunction(PositionNew,alpha,beta)\n",
    "            QuantumForceNew = QuantumForce(PositionNew,alpha, beta)\n",
    "            GreensFunction = 0.0\n",
    "            for j in range(Dimension):\n",
    "                GreensFunction += 0.5*(QuantumForceOld[i,j]+QuantumForceNew[i,j])*\\\n",
    "\t                              (D*TimeStep*0.5*(QuantumForceOld[i,j]-QuantumForceNew[i,j])-\\\n",
    "                                      PositionNew[i,j]+PositionOld[i,j])\n",
    "      \n",
    "            GreensFunction = exp(GreensFunction)\n",
    "            ProbabilityRatio = GreensFunction*wfnew**2/wfold**2\n",
    "            #Metropolis-Hastings test to see whether we accept the move\n",
    "            if random() <= ProbabilityRatio:\n",
    "                for j in range(Dimension):\n",
    "                    PositionOld[i,j] = PositionNew[i,j]\n",
    "                    QuantumForceOld[i,j] = QuantumForceNew[i,j]\n",
    "                wfold = wfnew\n",
    "        DeltaE = LocalEnergy(PositionOld,alpha,beta)\n",
    "        DerPsi = DerivativeWFansatz(PositionOld,alpha,beta)\n",
    "        DeltaPsi += DerPsi\n",
    "        energy += DeltaE\n",
    "        DerivativePsiE += DerPsi*DeltaE\n",
    "            \n",
    "    # We calculate mean values\n",
    "    energy /= NumberMCcycles\n",
    "    DerivativePsiE /= NumberMCcycles\n",
    "    DeltaPsi /= NumberMCcycles\n",
    "    EnergyDer  = 2*(DerivativePsiE-DeltaPsi*energy)\n",
    "    return EnergyDer\n",
    "\n",
    "\n",
    "# Computing the expectation value of the local energy \n",
    "def Energy(x0):\n",
    "    # Parameters in the Fokker-Planck simulation of the quantum force\n",
    "    D = 0.5\n",
    "    TimeStep = 0.05\n",
    "    # positions\n",
    "    PositionOld = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    PositionNew = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    # Quantum force\n",
    "    QuantumForceOld = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    QuantumForceNew = np.zeros((NumberParticles,Dimension), np.double)\n",
    "\n",
    "    energy = 0.0\n",
    "    DeltaE = 0.0\n",
    "    alpha = x0[0]\n",
    "    beta = x0[1]\n",
    "    NumberMCcycles= 10000\n",
    "    #Initial position\n",
    "    for i in range(NumberParticles):\n",
    "        for j in range(Dimension):\n",
    "            PositionOld[i,j] = normalvariate(0.0,1.0)*sqrt(TimeStep)\n",
    "    wfold = WaveFunction(PositionOld,alpha,beta)\n",
    "    QuantumForceOld = QuantumForce(PositionOld,alpha, beta)\n",
    "\n",
    "    #Loop over MC MCcycles\n",
    "    for MCcycle in range(NumberMCcycles):\n",
    "        #Trial position moving one particle at the time\n",
    "        for i in range(NumberParticles):\n",
    "            for j in range(Dimension):\n",
    "                PositionNew[i,j] = PositionOld[i,j]+normalvariate(0.0,1.0)*sqrt(TimeStep)+\\\n",
    "                                       QuantumForceOld[i,j]*TimeStep*D\n",
    "            wfnew = WaveFunction(PositionNew,alpha,beta)\n",
    "            QuantumForceNew = QuantumForce(PositionNew,alpha, beta)\n",
    "            GreensFunction = 0.0\n",
    "            for j in range(Dimension):\n",
    "                GreensFunction += 0.5*(QuantumForceOld[i,j]+QuantumForceNew[i,j])*\\\n",
    "\t                              (D*TimeStep*0.5*(QuantumForceOld[i,j]-QuantumForceNew[i,j])-\\\n",
    "                                      PositionNew[i,j]+PositionOld[i,j])\n",
    "      \n",
    "            GreensFunction = exp(GreensFunction)\n",
    "            ProbabilityRatio = GreensFunction*wfnew**2/wfold**2\n",
    "            #Metropolis-Hastings test to see whether we accept the move\n",
    "            if random() <= ProbabilityRatio:\n",
    "                for j in range(Dimension):\n",
    "                    PositionOld[i,j] = PositionNew[i,j]\n",
    "                    QuantumForceOld[i,j] = QuantumForceNew[i,j]\n",
    "                wfold = wfnew\n",
    "        DeltaE = LocalEnergy(PositionOld,alpha,beta)\n",
    "        energy += DeltaE\n",
    "            \n",
    "    # We calculate mean values\n",
    "    energy /= NumberMCcycles\n",
    "    return energy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Here starts the main program with variable declarations\n",
    "NumberParticles = 2\n",
    "Dimension = 2\n",
    "# seed for rng generator \n",
    "seed()\n",
    "# guess for variational parameters\n",
    "x0 = np.array([0.9,0.2])\n",
    "# Using Broydens method\n",
    "res = minimize(Energy, x0, method='BFGS', jac=EnergyDerivative, options={'gtol': 1e-4,'disp': True})\n",
    "print(res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049f8c5b",
   "metadata": {
    "editable": true
   },
   "source": [
    "Note that the **minimize** function returns the finale values for the variable $\\alpha=x0[0]$ and $\\beta=x0[1]$ in the array $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb291125",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Brief reminder on Newton-Raphson's method\n",
    "\n",
    "Let us quickly remind ourselves how we derive the above method.\n",
    "\n",
    "Perhaps the most celebrated of all one-dimensional root-finding\n",
    "routines is Newton's method, also called the Newton-Raphson\n",
    "method. This method  requires the evaluation of both the\n",
    "function $f$ and its derivative $f'$ at arbitrary points. \n",
    "If you can only calculate the derivative\n",
    "numerically and/or your function is not of the smooth type, we\n",
    "normally discourage the use of this method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54f1537",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The equations\n",
    "\n",
    "The Newton-Raphson formula consists geometrically of extending the\n",
    "tangent line at a current point until it crosses zero, then setting\n",
    "the next guess to the abscissa of that zero-crossing.  The mathematics\n",
    "behind this method is rather simple. Employing a Taylor expansion for\n",
    "$x$ sufficiently close to the solution $s$, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ceb8c4",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:taylornr\"></div>\n",
    "\n",
    "$$\n",
    "f(s)=0=f(x)+(s-x)f'(x)+\\frac{(s-x)^2}{2}f''(x) +\\dots.\n",
    "    \\label{eq:taylornr} \\tag{13}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c521ea78",
   "metadata": {
    "editable": true
   },
   "source": [
    "For small enough values of the function and for well-behaved\n",
    "functions, the terms beyond linear are unimportant, hence we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a5705b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "f(x)+(s-x)f'(x)\\approx 0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f4e37f",
   "metadata": {
    "editable": true
   },
   "source": [
    "yielding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b80e55",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "s\\approx x-\\frac{f(x)}{f'(x)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b41299",
   "metadata": {
    "editable": true
   },
   "source": [
    "Having in mind an iterative procedure, it is natural to start iterating with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cae7879",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "x_{n+1}=x_n-\\frac{f(x_n)}{f'(x_n)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312859ed",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Simple geometric interpretation\n",
    "\n",
    "The above is Newton-Raphson's method. It has a simple geometric\n",
    "interpretation, namely $x_{n+1}$ is the point where the tangent from\n",
    "$(x_n,f(x_n))$ crosses the $x$-axis.  Close to the solution,\n",
    "Newton-Raphson converges fast to the desired result. However, if we\n",
    "are far from a root, where the higher-order terms in the series are\n",
    "important, the Newton-Raphson formula can give grossly inaccurate\n",
    "results. For instance, the initial guess for the root might be so far\n",
    "from the true root as to let the search interval include a local\n",
    "maximum or minimum of the function.  If an iteration places a trial\n",
    "guess near such a local extremum, so that the first derivative nearly\n",
    "vanishes, then Newton-Raphson may fail totally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cebd4f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Extending to more than one variable\n",
    "\n",
    "Newton's method can be generalized to systems of several non-linear equations\n",
    "and variables. Consider the case with two equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75dc1bc",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{array}{cc} f_1(x_1,x_2) &=0\\\\\n",
    "                     f_2(x_1,x_2) &=0,\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921d415d",
   "metadata": {
    "editable": true
   },
   "source": [
    "which we Taylor expand to obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a465d72",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{array}{cc} 0=f_1(x_1+h_1,x_2+h_2)=&f_1(x_1,x_2)+h_1\n",
    "                     \\partial f_1/\\partial x_1+h_2\n",
    "                     \\partial f_1/\\partial x_2+\\dots\\\\\n",
    "                     0=f_2(x_1+h_1,x_2+h_2)=&f_2(x_1,x_2)+h_1\n",
    "                     \\partial f_2/\\partial x_1+h_2\n",
    "                     \\partial f_2/\\partial x_2+\\dots\n",
    "                       \\end{array}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff22bf7",
   "metadata": {
    "editable": true
   },
   "source": [
    "Defining the Jacobian matrix $\\hat{J}$ we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bf78e0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{J}=\\left( \\begin{array}{cc}\n",
    "                         \\partial f_1/\\partial x_1  & \\partial f_1/\\partial x_2 \\\\\n",
    "                          \\partial f_2/\\partial x_1     &\\partial f_2/\\partial x_2\n",
    "             \\end{array} \\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfefbde",
   "metadata": {
    "editable": true
   },
   "source": [
    "we can rephrase Newton's method as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5588e8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\left(\\begin{array}{c} x_1^{n+1} \\\\ x_2^{n+1} \\end{array} \\right)=\n",
    "\\left(\\begin{array}{c} x_1^{n} \\\\ x_2^{n} \\end{array} \\right)+\n",
    "\\left(\\begin{array}{c} h_1^{n} \\\\ h_2^{n} \\end{array} \\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400029f5",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we have defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe04749",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\left(\\begin{array}{c} h_1^{n} \\\\ h_2^{n} \\end{array} \\right)=\n",
    "   -{\\bf \\hat{J}}^{-1}\n",
    "   \\left(\\begin{array}{c} f_1(x_1^{n},x_2^{n}) \\\\ f_2(x_1^{n},x_2^{n}) \\end{array} \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e5a6db",
   "metadata": {
    "editable": true
   },
   "source": [
    "We need thus to compute the inverse of the Jacobian matrix and it\n",
    "is to understand that difficulties  may\n",
    "arise in case $\\hat{J}$ is nearly singular.\n",
    "\n",
    "It is rather straightforward to extend the above scheme to systems of\n",
    "more than two non-linear equations. In our case, the Jacobian matrix is given by the Hessian that represents the second derivative of cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1bd442",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Steepest descent\n",
    "\n",
    "The basic idea of gradient descent is\n",
    "that a function $F(\\mathbf{x})$, \n",
    "$\\mathbf{x} \\equiv (x_1,\\cdots,x_n)$, decreases fastest if one goes from $\\bf {x}$ in the\n",
    "direction of the negative gradient $-\\nabla F(\\mathbf{x})$.\n",
    "\n",
    "It can be shown that if"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fd48ff",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\gamma_k \\nabla F(\\mathbf{x}_k),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9734a9",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $\\gamma_k > 0$.\n",
    "\n",
    "For $\\gamma_k$ small enough, then $F(\\mathbf{x}_{k+1}) \\leq\n",
    "F(\\mathbf{x}_k)$. This means that for a sufficiently small $\\gamma_k$\n",
    "we are always moving towards smaller function values, i.e a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5d549e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More on Steepest descent\n",
    "\n",
    "The previous observation is the basis of the method of steepest\n",
    "descent, which is also referred to as just gradient descent (GD). One\n",
    "starts with an initial guess $\\mathbf{x}_0$ for a minimum of $F$ and\n",
    "computes new approximations according to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6a2450",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\gamma_k \\nabla F(\\mathbf{x}_k), \\ \\ k \\geq 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab399ca",
   "metadata": {
    "editable": true
   },
   "source": [
    "The parameter $\\gamma_k$ is often referred to as the step length or\n",
    "the learning rate within the context of Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3cd9d8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The ideal\n",
    "\n",
    "Ideally the sequence $\\{\\mathbf{x}_k \\}_{k=0}$ converges to a global\n",
    "minimum of the function $F$. In general we do not know if we are in a\n",
    "global or local minimum. In the special case when $F$ is a convex\n",
    "function, all local minima are also global minima, so in this case\n",
    "gradient descent can converge to the global solution. The advantage of\n",
    "this scheme is that it is conceptually simple and straightforward to\n",
    "implement. However the method in this form has some severe\n",
    "limitations:\n",
    "\n",
    "In machine learing we are often faced with non-convex high dimensional\n",
    "cost functions with many local minima. Since GD is deterministic we\n",
    "will get stuck in a local minimum, if the method converges, unless we\n",
    "have a very good intial guess. This also implies that the scheme is\n",
    "sensitive to the chosen initial condition.\n",
    "\n",
    "Note that the gradient is a function of $\\mathbf{x} =\n",
    "(x_1,\\cdots,x_n)$ which makes it expensive to compute numerically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d09a4ff",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The sensitiveness of the gradient descent\n",
    "\n",
    "The gradient descent method \n",
    "is sensitive to the choice of learning rate $\\gamma_k$. This is due\n",
    "to the fact that we are only guaranteed that $F(\\mathbf{x}_{k+1}) \\leq\n",
    "F(\\mathbf{x}_k)$ for sufficiently small $\\gamma_k$. The problem is to\n",
    "determine an optimal learning rate. If the learning rate is chosen too\n",
    "small the method will take a long time to converge and if it is too\n",
    "large we can experience erratic behavior.\n",
    "\n",
    "Many of these shortcomings can be alleviated by introducing\n",
    "randomness. One such method is that of Stochastic Gradient Descent\n",
    "(SGD), see below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea26943",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Convex functions\n",
    "\n",
    "Ideally we want our cost/loss function to be convex(concave).\n",
    "\n",
    "First we give the definition of a convex set: A set $C$ in\n",
    "$\\mathbb{R}^n$ is said to be convex if, for all $x$ and $y$ in $C$ and\n",
    "all $t \\in (0,1)$ , the point $(1 − t)x + ty$ also belongs to\n",
    "C. Geometrically this means that every point on the line segment\n",
    "connecting $x$ and $y$ is in $C$ as discussed below.\n",
    "\n",
    "The convex subsets of $\\mathbb{R}$ are the intervals of\n",
    "$\\mathbb{R}$. Examples of convex sets of $\\mathbb{R}^2$ are the\n",
    "regular polygons (triangles, rectangles, pentagons, etc...)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc2e46b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Convex function\n",
    "\n",
    "**Convex function**: Let $X \\subset \\mathbb{R}^n$ be a convex set. Assume that the function $f: X \\rightarrow \\mathbb{R}$ is continuous, then $f$ is said to be convex if $$f(tx_1 + (1-t)x_2) \\leq tf(x_1) + (1-t)f(x_2) $$ for all $x_1, x_2 \\in X$ and for all $t \\in [0,1]$. If $\\leq$ is replaced with a strict inequaltiy in the definition, we demand $x_1 \\neq x_2$ and $t\\in(0,1)$ then $f$ is said to be strictly convex. For a single variable function, convexity means that if you draw a straight line connecting $f(x_1)$ and $f(x_2)$, the value of the function on the interval $[x_1,x_2]$ is always below the line as illustrated below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f7ae21",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Conditions on convex functions\n",
    "\n",
    "In the following we state first and second-order conditions which\n",
    "ensures convexity of a function $f$. We write $D_f$ to denote the\n",
    "domain of $f$, i.e the subset of $R^n$ where $f$ is defined. For more\n",
    "details and proofs we refer to: [S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press](http://stanford.edu/boyd/cvxbook/, 2004).\n",
    "\n",
    "**First order condition.**\n",
    "\n",
    "Suppose $f$ is differentiable (i.e $\\nabla f(x)$ is well defined for\n",
    "all $x$ in the domain of $f$). Then $f$ is convex if and only if $D_f$\n",
    "is a convex set and $$f(y) \\geq f(x) + \\nabla f(x)^T (y-x) $$ holds\n",
    "for all $x,y \\in D_f$. This condition means that for a convex function\n",
    "the first order Taylor expansion (right hand side above) at any point\n",
    "a global under estimator of the function. To convince yourself you can\n",
    "make a drawing of $f(x) = x^2+1$ and draw the tangent line to $f(x)$ and\n",
    "note that it is always below the graph.\n",
    "\n",
    "**Second order condition.**\n",
    "\n",
    "Assume that $f$ is twice\n",
    "differentiable, i.e the Hessian matrix exists at each point in\n",
    "$D_f$. Then $f$ is convex if and only if $D_f$ is a convex set and its\n",
    "Hessian is positive semi-definite for all $x\\in D_f$. For a\n",
    "single-variable function this reduces to $f''(x) \\geq 0$. Geometrically this means that $f$ has nonnegative curvature\n",
    "everywhere.\n",
    "\n",
    "This condition is particularly useful since it gives us an procedure for determining if the function under consideration is convex, apart from using the definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef1167e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More on convex functions\n",
    "\n",
    "The next result is of great importance to us and the reason why we are\n",
    "going on about convex functions. In machine learning we frequently\n",
    "have to minimize a loss/cost function in order to find the best\n",
    "parameters for the model we are considering. \n",
    "\n",
    "Ideally we want the\n",
    "global minimum (for high-dimensional models it is hard to know\n",
    "if we have local or global minimum). However, if the cost/loss function\n",
    "is convex the following result provides invaluable information:\n",
    "\n",
    "**Any minimum is global for convex functions.**\n",
    "\n",
    "Consider the problem of finding $x \\in \\mathbb{R}^n$ such that $f(x)$\n",
    "is minimal, where $f$ is convex and differentiable. Then, any point\n",
    "$x^*$ that satisfies $\\nabla f(x^*) = 0$ is a global minimum.\n",
    "\n",
    "This result means that if we know that the cost/loss function is convex and we are able to find a minimum, we are guaranteed that it is a global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f7e5d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Some simple problems\n",
    "\n",
    "1. Show that $f(x)=x^2$ is convex for $x \\in \\mathbb{R}$ using the definition of convexity. Hint: If you re-write the definition, $f$ is convex if the following holds for all $x,y \\in D_f$ and any $\\lambda \\in [0,1]$ $\\lambda f(x)+(1-\\lambda)f(y)-f(\\lambda x + (1-\\lambda) y ) \\geq 0$.\n",
    "\n",
    "2. Using the second order condition show that the following functions are convex on the specified domain.\n",
    "\n",
    " * $f(x) = e^x$ is convex for $x \\in \\mathbb{R}$.\n",
    "\n",
    " * $g(x) = -\\ln(x)$ is convex for $x \\in (0,\\infty)$.\n",
    "\n",
    "3. Let $f(x) = x^2$ and $g(x) = e^x$. Show that $f(g(x))$ and $g(f(x))$ is convex for $x \\in \\mathbb{R}$. Also show that if $f(x)$ is any convex function than $h(x) = e^{f(x)}$ is convex.\n",
    "\n",
    "4. A norm is any function that satisfy the following properties\n",
    "\n",
    " * $f(\\alpha x) = |\\alpha| f(x)$ for all $\\alpha \\in \\mathbb{R}$.\n",
    "\n",
    " * $f(x+y) \\leq f(x) + f(y)$\n",
    "\n",
    " * $f(x) \\leq 0$ for all $x \\in \\mathbb{R}^n$ with equality if and only if $x = 0$\n",
    "\n",
    "Using the definition of convexity, try to show that a function satisfying the properties above is convex (the third condition is not needed to show this)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6963ac7d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Standard steepest descent\n",
    "\n",
    "Before we proceed, we would like to discuss the approach called the\n",
    "**standard Steepest descent**, which again leads to us having to be able\n",
    "to compute a matrix. It belongs to the class of Conjugate Gradient methods (CG).\n",
    "\n",
    "[The success of the CG method](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf)\n",
    "for finding solutions of non-linear problems is based on the theory\n",
    "of conjugate gradients for linear systems of equations. It belongs to\n",
    "the class of iterative methods for solving problems from linear\n",
    "algebra of the type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972ca15f",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{A}\\hat{x} = \\hat{b}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6da99b6",
   "metadata": {
    "editable": true
   },
   "source": [
    "In the iterative process we end up with a problem like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bb0feb",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{r}= \\hat{b}-\\hat{A}\\hat{x},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6280b021",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\hat{r}$ is the so-called residual or error in the iterative process.\n",
    "\n",
    "When we have found the exact solution, $\\hat{r}=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42903bf1",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Gradient method\n",
    "\n",
    "The residual is zero when we reach the minimum of the quadratic equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f5e04d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "P(\\hat{x})=\\frac{1}{2}\\hat{x}^T\\hat{A}\\hat{x} - \\hat{x}^T\\hat{b},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ba1988",
   "metadata": {
    "editable": true
   },
   "source": [
    "with the constraint that the matrix $\\hat{A}$ is positive definite and\n",
    "symmetric.  This defines also the Hessian and we want it to be  positive definite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481c8a21",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Steepest descent  method\n",
    "\n",
    "We denote the initial guess for $\\hat{x}$ as $\\hat{x}_0$. \n",
    "We can assume without loss of generality that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c17d9e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{x}_0=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a9c3b2",
   "metadata": {
    "editable": true
   },
   "source": [
    "or consider the system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b792ed72",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{A}\\hat{z} = \\hat{b}-\\hat{A}\\hat{x}_0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5824ae67",
   "metadata": {
    "editable": true
   },
   "source": [
    "instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5650b8d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Steepest descent  method\n",
    "One can show that the solution $\\hat{x}$ is also the unique minimizer of the quadratic form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28934e57",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "f(\\hat{x}) = \\frac{1}{2}\\hat{x}^T\\hat{A}\\hat{x} - \\hat{x}^T \\hat{x} , \\quad \\hat{x}\\in\\mathbf{R}^n.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1c1f8c",
   "metadata": {
    "editable": true
   },
   "source": [
    "This suggests taking the first basis vector $\\hat{r}_1$ (see below for definition) \n",
    "to be the gradient of $f$ at $\\hat{x}=\\hat{x}_0$, \n",
    "which equals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4af0b0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{A}\\hat{x}_0-\\hat{b},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3b4f61",
   "metadata": {
    "editable": true
   },
   "source": [
    "and \n",
    "$\\hat{x}_0=0$ it is equal $-\\hat{b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234b543b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Final expressions\n",
    "We can compute the residual iteratively as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc4deb1",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{r}_{k+1}=\\hat{b}-\\hat{A}\\hat{x}_{k+1},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d496f0c",
   "metadata": {
    "editable": true
   },
   "source": [
    "which equals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258dbeb3",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{b}-\\hat{A}(\\hat{x}_k+\\alpha_k\\hat{r}_k),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2e223e",
   "metadata": {
    "editable": true
   },
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74e67bc",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "(\\hat{b}-\\hat{A}\\hat{x}_k)-\\alpha_k\\hat{A}\\hat{r}_k,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4e1cae",
   "metadata": {
    "editable": true
   },
   "source": [
    "which gives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a4f29b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\alpha_k = \\frac{\\hat{r}_k^T\\hat{r}_k}{\\hat{r}_k^T\\hat{A}\\hat{r}_k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbda456",
   "metadata": {
    "editable": true
   },
   "source": [
    "leading to the iterative scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9c7344",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{x}_{k+1}=\\hat{x}_k-\\alpha_k\\hat{r}_{k},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7a06b1",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Broyden–Fletcher–Goldfarb–Shanno algorithm\n",
    "The optimization problem is to minimize $f(\\mathbf {x} )$ where $\\mathbf {x}$  is a vector in $R^{n}$, and $f$ is a differentiable scalar function. There are no constraints on the values that  $\\mathbf {x}$  can take.\n",
    "\n",
    "The algorithm begins at an initial estimate for the optimal value $\\mathbf {x}_{0}$ and proceeds iteratively to get a better estimate at each stage.\n",
    "\n",
    "The search direction $p_k$ at stage $k$ is given by the solution of the analogue of the Newton equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec55cde",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "B_{k}\\mathbf {p} _{k}=-\\nabla f(\\mathbf {x}_{k}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7c3cf7",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $B_{k}$ is an approximation to the Hessian matrix, which is\n",
    "updated iteratively at each stage, and $\\nabla f(\\mathbf {x} _{k})$\n",
    "is the gradient of the function\n",
    "evaluated at $x_k$. \n",
    "A line search in the direction $p_k$ is then used to\n",
    "find the next point $x_{k+1}$ by minimising"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51e2756",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "f(\\mathbf {x}_{k}+\\alpha \\mathbf {p}_{k}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b6259a",
   "metadata": {
    "editable": true
   },
   "source": [
    "over the scalar $\\alpha > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04d8d66",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Using gradient descent methods, limitations\n",
    "\n",
    "* **Gradient descent (GD) finds local minima of our function**. Since the GD algorithm is deterministic, if it converges, it will converge to a local minimum of our cost/loss/risk function. Because in ML we are often dealing with extremely rugged landscapes with many local minima, this can lead to poor performance.\n",
    "\n",
    "* **GD is sensitive to initial conditions**. One consequence of the local nature of GD is that initial conditions matter. Depending on where one starts, one will end up at a different local minima. Therefore, it is very important to think about how one initializes the training process. This is true for GD as well as more complicated variants of GD.\n",
    "\n",
    "* **Gradients are computationally expensive to calculate for large datasets**. In many cases in statistics and ML, the cost/loss/risk function is a sum of terms, with one term for each data point. For example, in linear regression, $E \\propto \\sum_{i=1}^n (y_i - \\mathbf{w}^T\\cdot\\mathbf{x}_i)^2$; for logistic regression, the square error is replaced by the cross entropy. To calculate the gradient we have to sum over *all* $n$ data points. Doing this at every GD step becomes extremely computationally expensive. An ingenious solution to this, is to calculate the gradients using small subsets of the data called \"mini batches\". This has the added benefit of introducing stochasticity into our algorithm.\n",
    "\n",
    "* **GD is very sensitive to choices of learning rates**. GD is extremely sensitive to the choice of learning rates. If the learning rate is very small, the training process take an extremely long time. For larger learning rates, GD can diverge and give poor results. Furthermore, depending on what the local landscape looks like, we have to modify the learning rates to ensure convergence. Ideally, we would *adaptively* choose the learning rates to match the landscape.\n",
    "\n",
    "* **GD treats all directions in parameter space uniformly.** Another major drawback of GD is that unlike Newton's method, the learning rate for GD is the same in all directions in parameter space. For this reason, the maximum learning rate is set by the behavior of the steepest direction and this can significantly slow down training. Ideally, we would like to take large steps in flat directions and small steps in steep directions. Since we are exploring rugged landscapes where curvatures change, this requires us to keep track of not only the gradient but second derivatives. The ideal scenario would be to calculate the Hessian but this proves to be too computationally expensive. \n",
    "\n",
    "* GD can take exponential time to escape saddle points, even with random initialization. As we mentioned, GD is extremely sensitive to initial condition since it determines the particular local minimum GD would eventually reach. However, even with a good initialization scheme, through the introduction of randomness, GD can still take exponential time to escape saddle points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f70e03",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Improving gradient descent with momentum\n",
    "\n",
    "We discuss here some simple examples where we introduce what is called 'memory'about previous steps, or what is normally called momentum gradient descent. The mathematics is explained below in connection with Stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb7624f9",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import arange\n",
    "from numpy.random import rand\n",
    "from numpy.random import seed\n",
    "from matplotlib import pyplot\n",
    " \n",
    "# objective function\n",
    "def objective(x):\n",
    "\treturn x**2.0\n",
    " \n",
    "# derivative of objective function\n",
    "def derivative(x):\n",
    "\treturn x * 2.0\n",
    " \n",
    "# gradient descent algorithm\n",
    "def gradient_descent(objective, derivative, bounds, n_iter, step_size):\n",
    "\t# track all solutions\n",
    "\tsolutions, scores = list(), list()\n",
    "\t# generate an initial point\n",
    "\tsolution = bounds[:, 0] + rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0])\n",
    "\t# run the gradient descent\n",
    "\tfor i in range(n_iter):\n",
    "\t\t# calculate gradient\n",
    "\t\tgradient = derivative(solution)\n",
    "\t\t# take a step\n",
    "\t\tsolution = solution - step_size * gradient\n",
    "\t\t# evaluate candidate point\n",
    "\t\tsolution_eval = objective(solution)\n",
    "\t\t# store solution\n",
    "\t\tsolutions.append(solution)\n",
    "\t\tscores.append(solution_eval)\n",
    "\t\t# report progress\n",
    "\t\tprint('>%d f(%s) = %.5f' % (i, solution, solution_eval))\n",
    "\treturn [solutions, scores]\n",
    " \n",
    "# seed the pseudo random number generator\n",
    "seed(4)\n",
    "# define range for input\n",
    "bounds = asarray([[-1.0, 1.0]])\n",
    "# define the total iterations\n",
    "n_iter = 30\n",
    "# define the step size\n",
    "step_size = 0.1\n",
    "# perform the gradient descent search\n",
    "solutions, scores = gradient_descent(objective, derivative, bounds, n_iter, step_size)\n",
    "# sample input range uniformly at 0.1 increments\n",
    "inputs = arange(bounds[0,0], bounds[0,1]+0.1, 0.1)\n",
    "# compute targets\n",
    "results = objective(inputs)\n",
    "# create a line plot of input vs result\n",
    "pyplot.plot(inputs, results)\n",
    "# plot the solutions found\n",
    "pyplot.plot(solutions, scores, '.-', color='red')\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e4eb1d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Same code but now with momentum gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aee11de4",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import arange\n",
    "from numpy.random import rand\n",
    "from numpy.random import seed\n",
    "from matplotlib import pyplot\n",
    " \n",
    "# objective function\n",
    "def objective(x):\n",
    "\treturn x**2.0\n",
    " \n",
    "# derivative of objective function\n",
    "def derivative(x):\n",
    "\treturn x * 2.0\n",
    " \n",
    "# gradient descent algorithm\n",
    "def gradient_descent(objective, derivative, bounds, n_iter, step_size, momentum):\n",
    "\t# track all solutions\n",
    "\tsolutions, scores = list(), list()\n",
    "\t# generate an initial point\n",
    "\tsolution = bounds[:, 0] + rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0])\n",
    "\t# keep track of the change\n",
    "\tchange = 0.0\n",
    "\t# run the gradient descent\n",
    "\tfor i in range(n_iter):\n",
    "\t\t# calculate gradient\n",
    "\t\tgradient = derivative(solution)\n",
    "\t\t# calculate update\n",
    "\t\tnew_change = step_size * gradient + momentum * change\n",
    "\t\t# take a step\n",
    "\t\tsolution = solution - new_change\n",
    "\t\t# save the change\n",
    "\t\tchange = new_change\n",
    "\t\t# evaluate candidate point\n",
    "\t\tsolution_eval = objective(solution)\n",
    "\t\t# store solution\n",
    "\t\tsolutions.append(solution)\n",
    "\t\tscores.append(solution_eval)\n",
    "\t\t# report progress\n",
    "\t\tprint('>%d f(%s) = %.5f' % (i, solution, solution_eval))\n",
    "\treturn [solutions, scores]\n",
    " \n",
    "# seed the pseudo random number generator\n",
    "seed(4)\n",
    "# define range for input\n",
    "bounds = asarray([[-1.0, 1.0]])\n",
    "# define the total iterations\n",
    "n_iter = 30\n",
    "# define the step size\n",
    "step_size = 0.1\n",
    "# define momentum\n",
    "momentum = 0.3\n",
    "# perform the gradient descent search with momentum\n",
    "solutions, scores = gradient_descent(objective, derivative, bounds, n_iter, step_size, momentum)\n",
    "# sample input range uniformly at 0.1 increments\n",
    "inputs = arange(bounds[0,0], bounds[0,1]+0.1, 0.1)\n",
    "# compute targets\n",
    "results = objective(inputs)\n",
    "# create a line plot of input vs result\n",
    "pyplot.plot(inputs, results)\n",
    "# plot the solutions found\n",
    "pyplot.plot(solutions, scores, '.-', color='red')\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe341412",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Overview video on Stochastic Gradient Descent\n",
    "\n",
    "[What is Stochastic Gradient Descent](https://www.youtube.com/watch?v=vMh0zPT0tLI&ab_channel=StatQuestwithJoshStarmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975d2407",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Batches and mini-batches\n",
    "\n",
    "In gradient descent we compute the cost function and its gradient for all data points we have.\n",
    "\n",
    "In large-scale applications such as the [ILSVRC challenge](https://www.image-net.org/challenges/LSVRC/), the\n",
    "training data can have on order of millions of examples. Hence, it\n",
    "seems wasteful to compute the full cost function over the entire\n",
    "training set in order to perform only a single parameter update. A\n",
    "very common approach to addressing this challenge is to compute the\n",
    "gradient over batches of the training data. For example, a typical batch could contain some thousand  examples from\n",
    "an  entire training set of several millions. This batch is then used to\n",
    "perform a parameter update."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc12b86",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "In stochastic gradient descent, the extreme case is the case where we\n",
    "have only one batch, that is we include the whole data set.\n",
    "\n",
    "This process is called Stochastic Gradient\n",
    "Descent (SGD) (or also sometimes on-line gradient descent). This is\n",
    "relatively less common to see because in practice due to vectorized\n",
    "code optimizations it can be computationally much more efficient to\n",
    "evaluate the gradient for 100 examples, than the gradient for one\n",
    "example 100 times. Even though SGD technically refers to using a\n",
    "single example at a time to evaluate the gradient, you will hear\n",
    "people use the term SGD even when referring to mini-batch gradient\n",
    "descent (i.e. mentions of MGD for “Minibatch Gradient Descent”, or BGD\n",
    "for “Batch gradient descent” are rare to see), where it is usually\n",
    "assumed that mini-batches are used. The size of the mini-batch is a\n",
    "hyperparameter but it is not very common to cross-validate or bootstrap it. It is\n",
    "usually based on memory constraints (if any), or set to some value,\n",
    "e.g. 32, 64 or 128. We use powers of 2 in practice because many\n",
    "vectorized operation implementations work faster when their inputs are\n",
    "sized in powers of 2.\n",
    "\n",
    "In our notes with  SGD we mean stochastic gradient descent with mini-batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2fba76",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "Stochastic gradient descent (SGD) and variants thereof address some of\n",
    "the shortcomings of the Gradient descent method discussed above.\n",
    "\n",
    "The underlying idea of SGD comes from the observation that the cost\n",
    "function, which we want to minimize, can almost always be written as a\n",
    "sum over $n$ data points $\\{\\mathbf{x}_i\\}_{i=1}^n$,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da606cc",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\mathbf{\\beta}) = \\sum_{i=1}^n c_i(\\mathbf{x}_i,\n",
    "\\mathbf{\\beta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c08d48",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Computation of gradients\n",
    "\n",
    "This in turn means that the gradient can be\n",
    "computed as a sum over $i$-gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceb6962",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_\\beta C(\\mathbf{\\beta}) = \\sum_i^n \\nabla_\\beta c_i(\\mathbf{x}_i,\n",
    "\\mathbf{\\beta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e09b8b",
   "metadata": {
    "editable": true
   },
   "source": [
    "Stochasticity/randomness is introduced by only taking the\n",
    "gradient on a subset of the data called minibatches.  If there are $n$\n",
    "data points and the size of each minibatch is $M$, there will be $n/M$\n",
    "minibatches. We denote these minibatches by $B_k$ where\n",
    "$k=1,\\cdots,n/M$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25af114c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## SGD example\n",
    "As an example, suppose we have $10$ data points $(\\mathbf{x}_1,\\cdots, \\mathbf{x}_{10})$ \n",
    "and we choose to have $M=5$ minibathces,\n",
    "then each minibatch contains two data points. In particular we have\n",
    "$B_1 = (\\mathbf{x}_1,\\mathbf{x}_2), \\cdots, B_5 =\n",
    "(\\mathbf{x}_9,\\mathbf{x}_{10})$. Note that if you choose $M=1$ you\n",
    "have only a single batch with all data points and on the other extreme,\n",
    "you may choose $M=n$ resulting in a minibatch for each datapoint, i.e\n",
    "$B_k = \\mathbf{x}_k$.\n",
    "\n",
    "The idea is now to approximate the gradient by replacing the sum over\n",
    "all data points with a sum over the data points in one the minibatches\n",
    "picked at random in each gradient descent step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f02e8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\nabla_{\\beta}\n",
    "C(\\mathbf{\\beta}) = \\sum_{i=1}^n \\nabla_\\beta c_i(\\mathbf{x}_i,\n",
    "\\mathbf{\\beta}) \\rightarrow \\sum_{i \\in B_k}^n \\nabla_\\beta\n",
    "c_i(\\mathbf{x}_i, \\mathbf{\\beta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e425741a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The gradient step\n",
    "\n",
    "Thus a gradient descent step now looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3540fc",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\beta_{j+1} = \\beta_j - \\gamma_j \\sum_{i \\in B_k}^n \\nabla_\\beta c_i(\\mathbf{x}_i,\n",
    "\\mathbf{\\beta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca0ea78",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $k$ is picked at random with equal\n",
    "probability from $[1,n/M]$. An iteration over the number of\n",
    "minibathces (n/M) is commonly referred to as an epoch. Thus it is\n",
    "typical to choose a number of epochs and for each epoch iterate over\n",
    "the number of minibatches, as exemplified in the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31094ae3",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Simple example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d986dcad",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "n = 100 #100 datapoints \n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "n_epochs = 10 #number of epochs\n",
    "\n",
    "j = 0\n",
    "for epoch in range(1,n_epochs+1):\n",
    "    for i in range(m):\n",
    "        k = np.random.randint(m) #Pick the k-th minibatch at random\n",
    "        #Compute the gradient using the data in minibatch Bk\n",
    "        #Compute new suggestion for \n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6357695",
   "metadata": {
    "editable": true
   },
   "source": [
    "Taking the gradient only on a subset of the data has two important\n",
    "benefits. First, it introduces randomness which decreases the chance\n",
    "that our opmization scheme gets stuck in a local minima. Second, if\n",
    "the size of the minibatches are small relative to the number of\n",
    "datapoints ($M <  n$), the computation of the gradient is much\n",
    "cheaper since we sum over the datapoints in the $k-th$ minibatch and not\n",
    "all $n$ datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e964ce",
   "metadata": {
    "editable": true
   },
   "source": [
    "## When do we stop?\n",
    "\n",
    "A natural question is when do we stop the search for a new minimum?\n",
    "One possibility is to compute the full gradient after a given number\n",
    "of epochs and check if the norm of the gradient is smaller than some\n",
    "threshold and stop if true. However, the condition that the gradient\n",
    "is zero is valid also for local minima, so this would only tell us\n",
    "that we are close to a local/global minimum. However, we could also\n",
    "evaluate the cost function at this point, store the result and\n",
    "continue the search. If the test kicks in at a later stage we can\n",
    "compare the values of the cost function and keep the $\\beta$ that\n",
    "gave the lowest value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ababab2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Slightly different approach\n",
    "\n",
    "Another approach is to let the step length $\\gamma_j$ depend on the\n",
    "number of epochs in such a way that it becomes very small after a\n",
    "reasonable time such that we do not move at all. Such approaches are\n",
    "also called scaling. There are many such ways to [scale the learning\n",
    "rate](https://towardsdatascience.com/gradient-descent-the-learning-rate-and-the-importance-of-feature-scaling-6c0b416596e1)\n",
    "and [discussions here](https://www.jmlr.org/papers/volume23/20-1258/20-1258.pdf). See\n",
    "also\n",
    "<https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1>\n",
    "for a discussion of different scaling functions for the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11697828",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Time decay rate\n",
    "\n",
    "As an example, let $e = 0,1,2,3,\\cdots$ denote the current epoch and let $t_0, t_1 > 0$ be two fixed numbers. Furthermore, let $t = e \\cdot m + i$ where $m$ is the number of minibatches and $i=0,\\cdots,m-1$. Then the function $$\\gamma_j(t; t_0, t_1) = \\frac{t_0}{t+t_1} $$ goes to zero as the number of epochs gets large. I.e. we start with a step length $\\gamma_j (0; t_0, t_1) = t_0/t_1$ which decays in *time* $t$.\n",
    "\n",
    "In this way we can fix the number of epochs, compute $\\beta$ and\n",
    "evaluate the cost function at the end. Repeating the computation will\n",
    "give a different result since the scheme is random by design. Then we\n",
    "pick the final $\\beta$ that gives the lowest value of the cost\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7af3a923",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def step_length(t,t0,t1):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "n = 100 #100 datapoints \n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "n_epochs = 500 #number of epochs\n",
    "t0 = 1.0\n",
    "t1 = 10\n",
    "\n",
    "gamma_j = t0/t1\n",
    "j = 0\n",
    "for epoch in range(1,n_epochs+1):\n",
    "    for i in range(m):\n",
    "        k = np.random.randint(m) #Pick the k-th minibatch at random\n",
    "        #Compute the gradient using the data in minibatch Bk\n",
    "        #Compute new suggestion for beta\n",
    "        t = epoch*m+i\n",
    "        gamma_j = step_length(t,t0,t1)\n",
    "        j += 1\n",
    "\n",
    "print(\"gamma_j after %d epochs: %g\" % (n_epochs,gamma_j))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50d102e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Code with a Number of Minibatches which varies\n",
    "\n",
    "In the code here we vary the number of mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b930c5b",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Importing various packages\n",
    "from math import exp, sqrt\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.inv(X.T @ X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* XT_X\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "eta = 1.0/np.max(EigValues)\n",
    "Niterations = 1000\n",
    "\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradients = 2.0/n*X.T @ ((X @ theta)-y)\n",
    "    theta -= eta*gradients\n",
    "print(\"theta from own gd\")\n",
    "print(theta)\n",
    "\n",
    "xnew = np.array([[0],[2]])\n",
    "Xnew = np.c_[np.ones((2,1)), xnew]\n",
    "ypredict = Xnew.dot(theta)\n",
    "ypredict2 = Xnew.dot(theta_linreg)\n",
    "\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "t0, t1 = 5, 50\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "# Can you figure out a better way of setting up the contributions to each batch?\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (2.0/M)* xi.T @ ((xi @ theta)-yi)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        theta = theta - eta*gradients\n",
    "print(\"theta from own sdg\")\n",
    "print(theta)\n",
    "\n",
    "plt.plot(xnew, ypredict, \"r-\")\n",
    "plt.plot(xnew, ypredict2, \"b-\")\n",
    "plt.plot(x, y ,'ro')\n",
    "plt.axis([0,2.0,0, 15.0])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.title(r'Random numbers ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a069c45a",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Replace or not\n",
    "\n",
    "In the above code, we have use replacement in setting up the\n",
    "mini-batches. The discussion\n",
    "[here](https://sebastianraschka.com/faq/docs/sgd-methods.html) may be\n",
    "useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958694db",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Momentum based GD\n",
    "\n",
    "The stochastic gradient descent (SGD) is almost always used with a\n",
    "*momentum* or inertia term that serves as a memory of the direction we\n",
    "are moving in parameter space.  This is typically implemented as\n",
    "follows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e271fc",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{v}_{t}=\\gamma \\mathbf{v}_{t-1}+\\eta_{t}\\nabla_\\theta E(\\boldsymbol{\\theta}_t) \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e89ab04",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\\boldsymbol{\\theta}_{t+1}= \\boldsymbol{\\theta}_t -\\mathbf{v}_{t},\n",
    "\\label{_auto3} \\tag{14}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3899a907",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we have introduced a momentum parameter $\\gamma$, with\n",
    "$0\\le\\gamma\\le 1$, and for brevity we dropped the explicit notation to\n",
    "indicate the gradient is to be taken over a different mini-batch at\n",
    "each step. We call this algorithm gradient descent with momentum\n",
    "(GDM). From these equations, it is clear that $\\mathbf{v}_t$ is a\n",
    "running average of recently encountered gradients and\n",
    "$(1-\\gamma)^{-1}$ sets the characteristic time scale for the memory\n",
    "used in the averaging procedure. Consistent with this, when\n",
    "$\\gamma=0$, this just reduces down to ordinary SGD as discussed\n",
    "earlier. An equivalent way of writing the updates is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f192d867",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\Delta \\boldsymbol{\\theta}_{t+1} = \\gamma \\Delta \\boldsymbol{\\theta}_t -\\ \\eta_{t}\\nabla_\\theta E(\\boldsymbol{\\theta}_t),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b864b87",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we have defined $\\Delta \\boldsymbol{\\theta}_{t}= \\boldsymbol{\\theta}_t-\\boldsymbol{\\theta}_{t-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cbcd0f",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More on momentum based approaches\n",
    "\n",
    "Let us try to get more intuition from these equations. It is helpful\n",
    "to consider a simple physical analogy with a particle of mass $m$\n",
    "moving in a viscous medium with drag coefficient $\\mu$ and potential\n",
    "$E(\\mathbf{w})$. If we denote the particle's position by $\\mathbf{w}$,\n",
    "then its motion is described by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbc0406",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "m {d^2 \\mathbf{w} \\over dt^2} + \\mu {d \\mathbf{w} \\over dt }= -\\nabla_w E(\\mathbf{w}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4724d9",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can discretize this equation in the usual way to get"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a844a96",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "m { \\mathbf{w}_{t+\\Delta t}-2 \\mathbf{w}_{t} +\\mathbf{w}_{t-\\Delta t} \\over (\\Delta t)^2}+\\mu {\\mathbf{w}_{t+\\Delta t}- \\mathbf{w}_{t} \\over \\Delta t} = -\\nabla_w E(\\mathbf{w}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5347212",
   "metadata": {
    "editable": true
   },
   "source": [
    "Rearranging this equation, we can rewrite this as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2502e8d8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\Delta \\mathbf{w}_{t +\\Delta t}= - { (\\Delta t)^2 \\over m +\\mu \\Delta t} \\nabla_w E(\\mathbf{w})+ {m \\over m +\\mu \\Delta t} \\Delta \\mathbf{w}_t.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c782ccc2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Momentum parameter\n",
    "\n",
    "Notice that this equation is identical to previous one if we identify\n",
    "the position of the particle, $\\mathbf{w}$, with the parameters\n",
    "$\\boldsymbol{\\theta}$. This allows us to identify the momentum\n",
    "parameter and learning rate with the mass of the particle and the\n",
    "viscous drag as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa45fcab",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\gamma= {m \\over m +\\mu \\Delta t }, \\qquad \\eta = {(\\Delta t)^2 \\over m +\\mu \\Delta t}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c96e8da",
   "metadata": {
    "editable": true
   },
   "source": [
    "Thus, as the name suggests, the momentum parameter is proportional to\n",
    "the mass of the particle and effectively provides inertia.\n",
    "Furthermore, in the large viscosity/small learning rate limit, our\n",
    "memory time scales as $(1-\\gamma)^{-1} \\approx m/(\\mu \\Delta t)$.\n",
    "\n",
    "Why is momentum useful? SGD momentum helps the gradient descent\n",
    "algorithm gain speed in directions with persistent but small gradients\n",
    "even in the presence of stochasticity, while suppressing oscillations\n",
    "in high-curvature directions. This becomes especially important in\n",
    "situations where the landscape is shallow and flat in some directions\n",
    "and narrow and steep in others. It has been argued that first-order\n",
    "methods (with appropriate initial conditions) can perform comparable\n",
    "to more expensive second order methods, especially in the context of\n",
    "complex deep learning models.\n",
    "\n",
    "These beneficial properties of momentum can sometimes become even more\n",
    "pronounced by using a slight modification of the classical momentum\n",
    "algorithm called Nesterov Accelerated Gradient (NAG).\n",
    "\n",
    "In the NAG algorithm, rather than calculating the gradient at the\n",
    "current parameters, $\\nabla_\\theta E(\\boldsymbol{\\theta}_t)$, one\n",
    "calculates the gradient at the expected value of the parameters given\n",
    "our current momentum, $\\nabla_\\theta E(\\boldsymbol{\\theta}_t +\\gamma\n",
    "\\mathbf{v}_{t-1})$. This yields the NAG update rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d55487",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{v}_{t}=\\gamma \\mathbf{v}_{t-1}+\\eta_{t}\\nabla_\\theta E(\\boldsymbol{\\theta}_t +\\gamma \\mathbf{v}_{t-1}) \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c643c50f",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto4\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\\boldsymbol{\\theta}_{t+1}= \\boldsymbol{\\theta}_t -\\mathbf{v}_{t}.\n",
    "\\label{_auto4} \\tag{15}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf006f13",
   "metadata": {
    "editable": true
   },
   "source": [
    "One of the major advantages of NAG is that it allows for the use of a larger learning rate than GDM for the same choice of $\\gamma$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ecdbb8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Second moment of the gradient\n",
    "\n",
    "In stochastic gradient descent, with and without momentum, we still\n",
    "have to specify a schedule for tuning the learning rates $\\eta_t$\n",
    "as a function of time.  As discussed in the context of Newton's\n",
    "method, this presents a number of dilemmas. The learning rate is\n",
    "limited by the steepest direction which can change depending on the\n",
    "current position in the landscape. To circumvent this problem, ideally\n",
    "our algorithm would keep track of curvature and take large steps in\n",
    "shallow, flat directions and small steps in steep, narrow directions.\n",
    "Second-order methods accomplish this by calculating or approximating\n",
    "the Hessian and normalizing the learning rate by the\n",
    "curvature. However, this is very computationally expensive for\n",
    "extremely large models. Ideally, we would like to be able to\n",
    "adaptively change the step size to match the landscape without paying\n",
    "the steep computational price of calculating or approximating\n",
    "Hessians.\n",
    "\n",
    "Recently, a number of methods have been introduced that accomplish\n",
    "this by tracking not only the gradient, but also the second moment of\n",
    "the gradient. These methods include AdaGrad, AdaDelta, Root Mean Squared Propagation (RMS-Prop), and\n",
    "[ADAM](https://arxiv.org/abs/1412.6980)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f725c18d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RMS prop\n",
    "\n",
    "In RMS prop, in addition to keeping a running average of the first\n",
    "moment of the gradient, we also keep track of the second moment\n",
    "denoted by $\\mathbf{s}_t=\\mathbb{E}[\\mathbf{g}_t^2]$. The update rule\n",
    "for RMS prop is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cb84cd",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto5\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{g}_t = \\nabla_\\theta E(\\boldsymbol{\\theta}) \n",
    "\\label{_auto5} \\tag{16}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecec94e2",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{s}_t =\\beta \\mathbf{s}_{t-1} +(1-\\beta)\\mathbf{g}_t^2 \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948d8682",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\theta}_{t+1}=\\boldsymbol{\\theta}_t - \\eta_t { \\mathbf{g}_t \\over \\sqrt{\\mathbf{s}_t +\\epsilon}}, \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fbad6b",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\beta$ controls the averaging time of the second moment and is\n",
    "typically taken to be about $\\beta=0.9$, $\\eta_t$ is a learning rate\n",
    "typically chosen to be $10^{-3}$, and $\\epsilon\\sim 10^{-8} $ is a\n",
    "small regularization constant to prevent divergences. Multiplication\n",
    "and division by vectors is understood as an element-wise operation. It\n",
    "is clear from this formula that the learning rate is reduced in\n",
    "directions where the norm of the gradient is consistently large. This\n",
    "greatly speeds up the convergence by allowing us to use a larger\n",
    "learning rate for flat directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3badae71",
   "metadata": {
    "editable": true
   },
   "source": [
    "## [ADAM optimizer](https://arxiv.org/abs/1412.6980)\n",
    "\n",
    "A related algorithm is the ADAM optimizer. In\n",
    "[ADAM](https://arxiv.org/abs/1412.6980), we keep a running average of\n",
    "both the first and second moment of the gradient and use this\n",
    "information to adaptively change the learning rate for different\n",
    "parameters.  The method isefficient when working with large\n",
    "problems involving lots data and/or parameters.  It is a combination of the\n",
    "gradient descent with momentum algorithm and the RMSprop algorithm\n",
    "discussed above.\n",
    "\n",
    "In addition to keeping a running average of the first and\n",
    "second moments of the gradient\n",
    "(i.e. $\\mathbf{m}_t=\\mathbb{E}[\\mathbf{g}_t]$ and\n",
    "$\\mathbf{s}_t=\\mathbb{E}[\\mathbf{g}^2_t]$, respectively), ADAM\n",
    "performs an additional bias correction to account for the fact that we\n",
    "are estimating the first two moments of the gradient using a running\n",
    "average (denoted by the hats in the update rule below). The update\n",
    "rule for ADAM is given by (where multiplication and division are once\n",
    "again understood to be element-wise operations below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188a3062",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto6\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{g}_t = \\nabla_\\theta E(\\boldsymbol{\\theta}) \n",
    "\\label{_auto6} \\tag{17}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1879e84",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{m}_t = \\beta_1 \\mathbf{m}_{t-1} + (1-\\beta_1) \\mathbf{g}_t \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7220f8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mathbf{s}_t =\\beta_2 \\mathbf{s}_{t-1} +(1-\\beta_2)\\mathbf{g}_t^2 \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a0aad0",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\mathbf{m}}_t={\\mathbf{m}_t \\over 1-\\beta_1^t} \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b54b96f",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\mathbf{s}}_t ={\\mathbf{s}_t \\over1-\\beta_2^t} \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d417f12b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\theta}_{t+1}=\\boldsymbol{\\theta}_t - \\eta_t { \\boldsymbol{\\mathbf{m}}_t \\over \\sqrt{\\boldsymbol{\\mathbf{s}}_t} +\\epsilon}, \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590ec9d4",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto7\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\\label{_auto7} \\tag{18}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97880d6",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\beta_1$ and $\\beta_2$ set the memory lifetime of the first and\n",
    "second moment and are typically taken to be $0.9$ and $0.99$\n",
    "respectively, and $\\eta$ and $\\epsilon$ are identical to RMSprop.\n",
    "\n",
    "Like in RMSprop, the effective step size of a parameter depends on the\n",
    "magnitude of its gradient squared.  To understand this better, let us\n",
    "rewrite this expression in terms of the variance\n",
    "$\\boldsymbol{\\sigma}_t^2 = \\boldsymbol{\\mathbf{s}}_t -\n",
    "(\\boldsymbol{\\mathbf{m}}_t)^2$. Consider a single parameter $\\theta_t$. The\n",
    "update rule for this parameter is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37541925",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\Delta \\theta_{t+1}= -\\eta_t { \\boldsymbol{m}_t \\over \\sqrt{\\sigma_t^2 +  m_t^2 }+\\epsilon}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299fd856",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Algorithms and codes for Adagrad, RMSprop and Adam\n",
    "\n",
    "The algorithms we have implemented are well described in the text by [Goodfellow, Bengio and Courville, chapter 8](https://www.deeplearningbook.org/contents/optimization.html).\n",
    "\n",
    "The codes which implement these algorithms are discussed after our presentation of automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121462f7",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Practical tips\n",
    "\n",
    "* **Randomize the data when making mini-batches**. It is always important to randomly shuffle the data when forming mini-batches. Otherwise, the gradient descent method can fit spurious correlations resulting from the order in which data is presented.\n",
    "\n",
    "* **Transform your inputs**. Learning becomes difficult when our landscape has a mixture of steep and flat directions. One simple trick for minimizing these situations is to standardize the data by subtracting the mean and normalizing the variance of input variables. Whenever possible, also decorrelate the inputs. To understand why this is helpful, consider the case of linear regression. It is easy to show that for the squared error cost function, the Hessian of the cost function is just the correlation matrix between the inputs. Thus, by standardizing the inputs, we are ensuring that the landscape looks homogeneous in all directions in parameter space. Since most deep networks can be viewed as linear transformations followed by a non-linearity at each layer, we expect this intuition to hold beyond the linear case.\n",
    "\n",
    "* **Monitor the out-of-sample performance.** Always monitor the performance of your model on a validation set (a small portion of the training data that is held out of the training process to serve as a proxy for the test set. If the validation error starts increasing, then the model is beginning to overfit. Terminate the learning process. This *early stopping* significantly improves performance in many settings.\n",
    "\n",
    "* **Adaptive optimization methods don't always have good generalization.** Recent studies have shown that adaptive methods such as ADAM, RMSPorp, and AdaGrad tend to have poor generalization compared to SGD or SGD with momentum, particularly in the high-dimensional limit (i.e. the number of parameters exceeds the number of data points). Although it is not clear at this stage why these methods perform so well in training deep neural networks, simpler procedures like properly-tuned SGD may work as well or better in these applications.\n",
    "\n",
    "Geron's text, see chapter 11, has several interesting discussions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c65873",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Automatic differentiation\n",
    "\n",
    "[Automatic differentiation (AD)](https://en.wikipedia.org/wiki/Automatic_differentiation), \n",
    "also called algorithmic\n",
    "differentiation or computational differentiation,is a set of\n",
    "techniques to numerically evaluate the derivative of a function\n",
    "specified by a computer program. AD exploits the fact that every\n",
    "computer program, no matter how complicated, executes a sequence of\n",
    "elementary arithmetic operations (addition, subtraction,\n",
    "multiplication, division, etc.) and elementary functions (exp, log,\n",
    "sin, cos, etc.). By applying the chain rule repeatedly to these\n",
    "operations, derivatives of arbitrary order can be computed\n",
    "automatically, accurately to working precision, and using at most a\n",
    "small constant factor more arithmetic operations than the original\n",
    "program.\n",
    "\n",
    "Automatic differentiation is neither:\n",
    "\n",
    "* Symbolic differentiation, nor\n",
    "\n",
    "* Numerical differentiation (the method of finite differences).\n",
    "\n",
    "Symbolic differentiation can lead to inefficient code and faces the\n",
    "difficulty of converting a computer program into a single expression,\n",
    "while numerical differentiation can introduce round-off errors in the\n",
    "discretization process and cancellation\n",
    "\n",
    "Python has tools for so-called **automatic differentiation**.\n",
    "Consider the following example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9397e0b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "f(x) = \\sin\\left(2\\pi x + x^2\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a08751",
   "metadata": {
    "editable": true
   },
   "source": [
    "which has the following derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bb448a",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "f'(x) = \\cos\\left(2\\pi x + x^2\\right)\\left(2\\pi + 2x\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb51b8d7",
   "metadata": {
    "editable": true
   },
   "source": [
    "Using **autograd** we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cef97fd",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "\n",
    "# To do elementwise differentiation:\n",
    "from autograd import elementwise_grad as egrad \n",
    "\n",
    "# To plot:\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(2*np.pi*x + x**2)\n",
    "\n",
    "def f_grad_analytic(x):\n",
    "    return np.cos(2*np.pi*x + x**2)*(2*np.pi + 2*x)\n",
    "\n",
    "# Do the comparison:\n",
    "x = np.linspace(0,1,1000)\n",
    "\n",
    "f_grad = egrad(f)\n",
    "\n",
    "computed = f_grad(x)\n",
    "analytic = f_grad_analytic(x)\n",
    "\n",
    "plt.title('Derivative computed from Autograd compared with the analytical derivative')\n",
    "plt.plot(x,computed,label='autograd')\n",
    "plt.plot(x,analytic,label='analytic')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"The max absolute difference is: %g\"%(np.max(np.abs(computed - analytic))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba234a4",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Using autograd\n",
    "\n",
    "Here we\n",
    "experiment with what kind of functions Autograd is capable\n",
    "of finding the gradient of. The following Python functions are just\n",
    "meant to illustrate what Autograd can do, but please feel free to\n",
    "experiment with other, possibly more complicated, functions as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d69308d",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "def f1(x):\n",
    "    return x**3 + 1\n",
    "\n",
    "f1_grad = grad(f1)\n",
    "\n",
    "# Remember to send in float as argument to the computed gradient from Autograd!\n",
    "a = 1.0\n",
    "\n",
    "# See the evaluated gradient at a using autograd:\n",
    "print(\"The gradient of f1 evaluated at a = %g using autograd is: %g\"%(a,f1_grad(a)))\n",
    "\n",
    "# Compare with the analytical derivative, that is f1'(x) = 3*x**2 \n",
    "grad_analytical = 3*a**2\n",
    "print(\"The gradient of f1 evaluated at a = %g by finding the analytic expression is: %g\"%(a,grad_analytical))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d74b9c5",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Autograd with more complicated functions\n",
    "\n",
    "To differentiate with respect to two (or more) arguments of a Python\n",
    "function, Autograd need to know at which variable the function if\n",
    "being differentiated with respect to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e5d9acd",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "def f2(x1,x2):\n",
    "    return 3*x1**3 + x2*(x1 - 5) + 1\n",
    "\n",
    "# By sending the argument 0, Autograd will compute the derivative w.r.t the first variable, in this case x1\n",
    "f2_grad_x1 = grad(f2,0)\n",
    "\n",
    "# ... and differentiate w.r.t x2 by sending 1 as an additional arugment to grad\n",
    "f2_grad_x2 = grad(f2,1)\n",
    "\n",
    "x1 = 1.0\n",
    "x2 = 3.0 \n",
    "\n",
    "print(\"Evaluating at x1 = %g, x2 = %g\"%(x1,x2))\n",
    "print(\"-\"*30)\n",
    "\n",
    "# Compare with the analytical derivatives:\n",
    "\n",
    "# Derivative of f2 w.r.t x1 is: 9*x1**2 + x2:\n",
    "f2_grad_x1_analytical = 9*x1**2 + x2\n",
    "\n",
    "# Derivative of f2 w.r.t x2 is: x1 - 5:\n",
    "f2_grad_x2_analytical = x1 - 5\n",
    "\n",
    "# See the evaluated derivations:\n",
    "print(\"The derivative of f2 w.r.t x1: %g\"%( f2_grad_x1(x1,x2) ))\n",
    "print(\"The analytical derivative of f2 w.r.t x1: %g\"%( f2_grad_x1(x1,x2) ))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"The derivative of f2 w.r.t x2: %g\"%( f2_grad_x2(x1,x2) ))\n",
    "print(\"The analytical derivative of f2 w.r.t x2: %g\"%( f2_grad_x2(x1,x2) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13346ef1",
   "metadata": {
    "editable": true
   },
   "source": [
    "Note that the grad function will not produce the true gradient of the function. The true gradient of a function with two or more variables will produce a vector, where each element is the function differentiated w.r.t a variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12435b8d",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More complicated functions using the elements of their arguments directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f833990",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "def f3(x): # Assumes x is an array of length 5 or higher\n",
    "    return 2*x[0] + 3*x[1] + 5*x[2] + 7*x[3] + 11*x[4]**2\n",
    "\n",
    "f3_grad = grad(f3)\n",
    "\n",
    "x = np.linspace(0,4,5)\n",
    "\n",
    "# Print the computed gradient:\n",
    "print(\"The computed gradient of f3 is: \", f3_grad(x))\n",
    "\n",
    "# The analytical gradient is: (2, 3, 5, 7, 22*x[4])\n",
    "f3_grad_analytical = np.array([2, 3, 5, 7, 22*x[4]])\n",
    "\n",
    "# Print the analytical gradient:\n",
    "print(\"The analytical gradient of f3 is: \", f3_grad_analytical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4414b6",
   "metadata": {
    "editable": true
   },
   "source": [
    "Note that in this case, when sending an array as input argument, the\n",
    "output from Autograd is another array. This is the true gradient of\n",
    "the function, as opposed to the function in the previous example. By\n",
    "using arrays to represent the variables, the output from Autograd\n",
    "might be easier to work with, as the output is closer to what one\n",
    "could expect form a gradient-evaluting function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3531b294",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Functions using mathematical functions from Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9787377",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "def f4(x):\n",
    "    return np.sqrt(1+x**2) + np.exp(x) + np.sin(2*np.pi*x)\n",
    "\n",
    "f4_grad = grad(f4)\n",
    "\n",
    "x = 2.7\n",
    "\n",
    "# Print the computed derivative:\n",
    "print(\"The computed derivative of f4 at x = %g is: %g\"%(x,f4_grad(x)))\n",
    "\n",
    "# The analytical derivative is: x/sqrt(1 + x**2) + exp(x) + cos(2*pi*x)*2*pi\n",
    "f4_grad_analytical = x/np.sqrt(1 + x**2) + np.exp(x) + np.cos(2*np.pi*x)*2*np.pi\n",
    "\n",
    "# Print the analytical gradient:\n",
    "print(\"The analytical gradient of f4 at x = %g is: %g\"%(x,f4_grad_analytical))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc74bcc",
   "metadata": {
    "editable": true
   },
   "source": [
    "## More autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5dd6bcfd",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "def f5(x):\n",
    "    if x >= 0:\n",
    "        return x**2\n",
    "    else:\n",
    "        return -3*x + 1\n",
    "\n",
    "f5_grad = grad(f5)\n",
    "\n",
    "x = 2.7\n",
    "\n",
    "# Print the computed derivative:\n",
    "print(\"The computed derivative of f5 at x = %g is: %g\"%(x,f5_grad(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a5307e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## And  with loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b66926a6",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "def f6_for(x):\n",
    "    val = 0\n",
    "    for i in range(10):\n",
    "        val = val + x**i\n",
    "    return val\n",
    "\n",
    "def f6_while(x):\n",
    "    val = 0\n",
    "    i = 0\n",
    "    while i < 10:\n",
    "        val = val + x**i\n",
    "        i = i + 1\n",
    "    return val\n",
    "\n",
    "f6_for_grad = grad(f6_for)\n",
    "f6_while_grad = grad(f6_while)\n",
    "\n",
    "x = 0.5\n",
    "\n",
    "# Print the computed derivaties of f6_for and f6_while\n",
    "print(\"The computed derivative of f6_for at x = %g is: %g\"%(x,f6_for_grad(x)))\n",
    "print(\"The computed derivative of f6_while at x = %g is: %g\"%(x,f6_while_grad(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f8ddf1c",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "# Both of the functions are implementation of the sum: sum(x**i) for i = 0, ..., 9\n",
    "# The analytical derivative is: sum(i*x**(i-1)) \n",
    "f6_grad_analytical = 0\n",
    "for i in range(10):\n",
    "    f6_grad_analytical += i*x**(i-1)\n",
    "\n",
    "print(\"The analytical derivative of f6 at x = %g is: %g\"%(x,f6_grad_analytical))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28079f5",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Using recursion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cebfab44",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "def f7(n): # Assume that n is an integer\n",
    "    if n == 1 or n == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return n*f7(n-1)\n",
    "\n",
    "f7_grad = grad(f7)\n",
    "\n",
    "n = 2.0\n",
    "\n",
    "print(\"The computed derivative of f7 at n = %d is: %g\"%(n,f7_grad(n)))\n",
    "\n",
    "# The function f7 is an implementation of the factorial of n.\n",
    "# By using the product rule, one can find that the derivative is:\n",
    "\n",
    "f7_grad_analytical = 0\n",
    "for i in range(int(n)-1):\n",
    "    tmp = 1\n",
    "    for k in range(int(n)-1):\n",
    "        if k != i:\n",
    "            tmp *= (n - k)\n",
    "    f7_grad_analytical += tmp\n",
    "\n",
    "print(\"The analytical derivative of f7 at n = %d is: %g\"%(n,f7_grad_analytical))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a97434",
   "metadata": {
    "editable": true
   },
   "source": [
    "Note that if n is equal to zero or one, Autograd will give an error message. This message appears when the output is independent on input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02046e47",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Unsupported functions\n",
    "Autograd supports many features. However, there are some functions that is not supported (yet) by Autograd.\n",
    "\n",
    "Assigning a value to the variable being differentiated with respect to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15862bbd",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "def f8(x): # Assume x is an array\n",
    "    x[2] = 3\n",
    "    return x*2\n",
    "\n",
    "f8_grad = grad(f8)\n",
    "\n",
    "x = 8.4\n",
    "\n",
    "print(\"The derivative of f8 is:\",f8_grad(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f608c96",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here, Autograd tells us that an 'ArrayBox' does not support item assignment. The item assignment is done when the program tries to assign x[2] to the value 3. However, Autograd has implemented the computation of the derivative such that this assignment is not possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654c6abe",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The syntax a.dot(b) when finding the dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "917f85cb",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "def f9(a): # Assume a is an array with 2 elements\n",
    "    b = np.array([1.0,2.0])\n",
    "    return a.dot(b)\n",
    "\n",
    "f9_grad = grad(f9)\n",
    "\n",
    "x = np.array([1.0,0.0])\n",
    "\n",
    "print(\"The derivative of f9 is:\",f9_grad(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ca72cc",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here we are told that the 'dot' function does not belong to Autograd's\n",
    "version of a Numpy array.  To overcome this, an alternative syntax\n",
    "which also computed the dot product can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a20ab38",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "def f9_alternative(x): # Assume a is an array with 2 elements\n",
    "    b = np.array([1.0,2.0])\n",
    "    return np.dot(x,b) # The same as x_1*b_1 + x_2*b_2\n",
    "\n",
    "f9_alternative_grad = grad(f9_alternative)\n",
    "\n",
    "x = np.array([3.0,0.0])\n",
    "\n",
    "print(\"The gradient of f9 is:\",f9_alternative_grad(x))\n",
    "\n",
    "# The analytical gradient of the dot product of vectors x and b with two elements (x_1,x_2) and (b_1, b_2) respectively\n",
    "# w.r.t x is (b_1, b_2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8183cae5",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Recommended to avoid\n",
    "The documentation recommends to avoid inplace operations such as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fb01d03",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a += b\n",
    "a -= b\n",
    "a*= b\n",
    "a /=b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf06b0f5",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Using Autograd with OLS\n",
    "\n",
    "We conclude the part on optmization by showing how we can make codes\n",
    "for linear regression and logistic regression using **autograd**. The\n",
    "first example shows results with ordinary leats squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b115b89a",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients for OLS\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "def CostOLS(beta):\n",
    "    return (1.0/n)*np.sum((y-X @ beta)**2)\n",
    "\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* XT_X\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "eta = 1.0/np.max(EigValues)\n",
    "Niterations = 1000\n",
    "# define the gradient\n",
    "training_gradient = grad(CostOLS)\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradients = training_gradient(theta)\n",
    "    theta -= eta*gradients\n",
    "print(\"theta from own gd\")\n",
    "print(theta)\n",
    "\n",
    "xnew = np.array([[0],[2]])\n",
    "Xnew = np.c_[np.ones((2,1)), xnew]\n",
    "ypredict = Xnew.dot(theta)\n",
    "ypredict2 = Xnew.dot(theta_linreg)\n",
    "\n",
    "plt.plot(xnew, ypredict, \"r-\")\n",
    "plt.plot(xnew, ypredict2, \"b-\")\n",
    "plt.plot(x, y ,'ro')\n",
    "plt.axis([0,2.0,0, 15.0])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.title(r'Random numbers ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15769c4",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Same code but now with momentum gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ef55cfe",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients for OLS\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "def CostOLS(beta):\n",
    "    return (1.0/n)*np.sum((y-X @ beta)**2)\n",
    "\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x#+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* XT_X\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "eta = 1.0/np.max(EigValues)\n",
    "Niterations = 30\n",
    "\n",
    "# define the gradient\n",
    "training_gradient = grad(CostOLS)\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradients = training_gradient(theta)\n",
    "    theta -= eta*gradients\n",
    "    print(iter,gradients[0],gradients[1])\n",
    "print(\"theta from own gd\")\n",
    "print(theta)\n",
    "\n",
    "# Now improve with momentum gradient descent\n",
    "change = 0.0\n",
    "delta_momentum = 0.3\n",
    "for iter in range(Niterations):\n",
    "    # calculate gradient\n",
    "    gradients = training_gradient(theta)\n",
    "    # calculate update\n",
    "    new_change = eta*gradients+delta_momentum*change\n",
    "    # take a step\n",
    "    theta -= new_change\n",
    "    # save the change\n",
    "    change = new_change\n",
    "    print(iter,gradients[0],gradients[1])\n",
    "print(\"theta from own gd wth momentum\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bd2731",
   "metadata": {
    "editable": true
   },
   "source": [
    "## But noen of these can compete with Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6eb4a44",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Using Newton's method\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "def CostOLS(beta):\n",
    "    return (1.0/n)*np.sum((y-X @ beta)**2)\n",
    "\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "XT_X = X.T @ X\n",
    "beta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(beta_linreg)\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* XT_X\n",
    "# Note that here the Hessian does not depend on the parameters beta\n",
    "invH = np.linalg.pinv(H)\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "beta = np.random.randn(2,1)\n",
    "Niterations = 5\n",
    "\n",
    "# define the gradient\n",
    "training_gradient = grad(CostOLS)\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradients = training_gradient(beta)\n",
    "    beta -= invH @ gradients\n",
    "    print(iter,gradients[0],gradients[1])\n",
    "print(\"beta from own Newton code\")\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e886f8",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Including Stochastic Gradient Descent with Autograd\n",
    "In this code we include the stochastic gradient descent approach discussed above. Note here that we specify which argument we are taking the derivative with respect to when using **autograd**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b79bd552",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients using SGD\n",
    "# OLS example\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "# Note change from previous example\n",
    "def CostOLS(y,X,theta):\n",
    "    return np.sum((y-X @ theta)**2)\n",
    "\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* XT_X\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "eta = 1.0/np.max(EigValues)\n",
    "Niterations = 1000\n",
    "\n",
    "# Note that we request the derivative wrt third argument (theta, 2 here)\n",
    "training_gradient = grad(CostOLS,2)\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradients = (1.0/n)*training_gradient(y, X, theta)\n",
    "    theta -= eta*gradients\n",
    "print(\"theta from own gd\")\n",
    "print(theta)\n",
    "\n",
    "xnew = np.array([[0],[2]])\n",
    "Xnew = np.c_[np.ones((2,1)), xnew]\n",
    "ypredict = Xnew.dot(theta)\n",
    "ypredict2 = Xnew.dot(theta_linreg)\n",
    "\n",
    "plt.plot(xnew, ypredict, \"r-\")\n",
    "plt.plot(xnew, ypredict2, \"b-\")\n",
    "plt.plot(x, y ,'ro')\n",
    "plt.axis([0,2.0,0, 15.0])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.title(r'Random numbers ')\n",
    "plt.show()\n",
    "\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "t0, t1 = 5, 50\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "# Can you figure out a better way of setting up the contributions to each batch?\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (1.0/M)*training_gradient(yi, xi, theta)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        theta = theta - eta*gradients\n",
    "print(\"theta from own sdg\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8fabea",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Same code but now with momentum gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a37c17d",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients using SGD\n",
    "# OLS example\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "# Note change from previous example\n",
    "def CostOLS(y,X,theta):\n",
    "    return np.sum((y-X @ theta)**2)\n",
    "\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* XT_X\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "print(f\"Eigenvalues of Hessian Matrix:{EigValues}\")\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "eta = 1.0/np.max(EigValues)\n",
    "Niterations = 100\n",
    "\n",
    "# Note that we request the derivative wrt third argument (theta, 2 here)\n",
    "training_gradient = grad(CostOLS,2)\n",
    "\n",
    "for iter in range(Niterations):\n",
    "    gradients = (1.0/n)*training_gradient(y, X, theta)\n",
    "    theta -= eta*gradients\n",
    "print(\"theta from own gd\")\n",
    "print(theta)\n",
    "\n",
    "\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "t0, t1 = 5, 50\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "change = 0.0\n",
    "delta_momentum = 0.3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (1.0/M)*training_gradient(yi, xi, theta)\n",
    "        eta = learning_schedule(epoch*m+i)\n",
    "        # calculate update\n",
    "        new_change = eta*gradients+delta_momentum*change\n",
    "        # take a step\n",
    "        theta -= new_change\n",
    "        # save the change\n",
    "        change = new_change\n",
    "print(\"theta from own sdg with momentum\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cbd45b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Similar (second order function now) problem but now with AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba5fe7d7",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients using AdaGrad and Stochastic Gradient descent\n",
    "# OLS example\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "# Note change from previous example\n",
    "def CostOLS(y,X,theta):\n",
    "    return np.sum((y-X @ theta)**2)\n",
    "\n",
    "n = 10000\n",
    "x = np.random.rand(n,1)\n",
    "y = 2.0+3*x +4*x*x# +np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x, x*x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "\n",
    "\n",
    "# Note that we request the derivative wrt third argument (theta, 2 here)\n",
    "training_gradient = grad(CostOLS,2)\n",
    "# Define parameters for Stochastic Gradient Descent\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "# Guess for unknown parameters theta\n",
    "theta = np.random.randn(3,1)\n",
    "\n",
    "# Value for learning rate\n",
    "eta = 0.01\n",
    "# Including AdaGrad parameter to avoid possible division by zero\n",
    "delta  = 1e-8\n",
    "for epoch in range(n_epochs):\n",
    "    # The outer product is calculated from scratch for each epoch\n",
    "    Giter = np.zeros(shape=(3,3))\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (1.0/M)*training_gradient(yi, xi, theta)\n",
    "\t# Calculate the outer product of the gradients\n",
    "        Giter +=gradients @ gradients.T\n",
    "\t# Simpler algorithm with only diagonal elements\n",
    "        Ginverse = np.c_[eta/(delta+np.sqrt(np.diagonal(Giter)))]\n",
    "        # compute update\n",
    "        update = np.multiply(Ginverse,gradients)\n",
    "        theta -= update\n",
    "print(\"theta from own AdaGrad\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553f92d2",
   "metadata": {
    "editable": true
   },
   "source": [
    "Running this code we note an almost perfect agreement with the results from matrix inversion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9336a0f4",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RMSprop for adaptive learning rate with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7aa98870",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Using Autograd to calculate gradients using RMSprop  and Stochastic Gradient descent\n",
    "# OLS example\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "# Note change from previous example\n",
    "def CostOLS(y,X,theta):\n",
    "    return np.sum((y-X @ theta)**2)\n",
    "\n",
    "n = 10000\n",
    "x = np.random.rand(n,1)\n",
    "y = 2.0+3*x +4*x*x# +np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x, x*x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "\n",
    "\n",
    "# Note that we request the derivative wrt third argument (theta, 2 here)\n",
    "training_gradient = grad(CostOLS,2)\n",
    "# Define parameters for Stochastic Gradient Descent\n",
    "n_epochs = 50\n",
    "M = 5   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "# Guess for unknown parameters theta\n",
    "theta = np.random.randn(3,1)\n",
    "\n",
    "# Value for learning rate\n",
    "eta = 0.01\n",
    "# Value for parameter rho\n",
    "rho = 0.99\n",
    "# Including AdaGrad parameter to avoid possible division by zero\n",
    "delta  = 1e-8\n",
    "for epoch in range(n_epochs):\n",
    "    Giter = np.zeros(shape=(3,3))\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (1.0/M)*training_gradient(yi, xi, theta)\n",
    "\t# Previous value for the outer product of gradients\n",
    "        Previous = Giter\n",
    "\t# Accumulated gradient\n",
    "        Giter +=gradients @ gradients.T\n",
    "\t# Scaling with rho the new and the previous results\n",
    "        Gnew = (rho*Previous+(1-rho)*Giter)\n",
    "\t# Taking the diagonal only and inverting\n",
    "        Ginverse = np.c_[eta/(delta+np.sqrt(np.diagonal(Gnew)))]\n",
    "\t# Hadamard product\n",
    "        update = np.multiply(Ginverse,gradients)\n",
    "        theta -= update\n",
    "print(\"theta from own RMSprop\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd107642",
   "metadata": {
    "editable": true
   },
   "source": [
    "## And Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80f66fe5",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 0.5 * (np.tanh(x / 2.) + 1)\n",
    "\n",
    "def logistic_predictions(weights, inputs):\n",
    "    # Outputs probability of a label being true according to logistic model.\n",
    "    return sigmoid(np.dot(inputs, weights))\n",
    "\n",
    "def training_loss(weights):\n",
    "    # Training loss is the negative log-likelihood of the training labels.\n",
    "    preds = logistic_predictions(weights, inputs)\n",
    "    label_probabilities = preds * targets + (1 - preds) * (1 - targets)\n",
    "    return -np.sum(np.log(label_probabilities))\n",
    "\n",
    "# Build a toy dataset.\n",
    "inputs = np.array([[0.52, 1.12,  0.77],\n",
    "                   [0.88, -1.08, 0.15],\n",
    "                   [0.52, 0.06, -1.30],\n",
    "                   [0.74, -2.49, 1.39]])\n",
    "targets = np.array([True, True, False, True])\n",
    "\n",
    "# Define a function that returns gradients of training loss using Autograd.\n",
    "training_gradient_fun = grad(training_loss)\n",
    "\n",
    "# Optimize weights using gradient descent.\n",
    "weights = np.array([0.0, 0.0, 0.0])\n",
    "print(\"Initial loss:\", training_loss(weights))\n",
    "for i in range(100):\n",
    "    weights -= training_gradient_fun(weights) * 0.01\n",
    "\n",
    "print(\"Trained loss:\", training_loss(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3d47ba",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Introducing [JAX](https://jax.readthedocs.io/en/latest/)\n",
    "\n",
    "Presently, instead of using **autograd**, we recommend using [JAX](https://jax.readthedocs.io/en/latest/)\n",
    "\n",
    "**JAX** is Autograd and [XLA (Accelerated Linear Algebra))](https://www.tensorflow.org/xla),\n",
    "brought together for high-performance numerical computing and machine learning research.\n",
    "It provides composable transformations of Python+NumPy programs: differentiate, vectorize, parallelize, Just-In-Time compile to GPU/TPU, and more.\n",
    "\n",
    "Here's a simple example on how you can use **JAX** to compute the derivate of the logistic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c718732",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "\n",
    "def sum_logistic(x):\n",
    "  return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
    "\n",
    "x_small = jnp.arange(3.)\n",
    "derivative_fn = grad(sum_logistic)\n",
    "print(derivative_fn(x_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80330796",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Boltzmann Machines\n",
    "\n",
    "* A generative model can learn to represent and sample from a probability distribution. The core idea is to learn a parametric model of the probability distribution from which the training data was drawn. As an example\n",
    "\n",
    "a. A model for images could learn to draw new examples of cats and dogs, given a training dataset of images of cats and dogs.\n",
    "\n",
    "b. Generate a sample of an ordered or disordered Ising model phase, having been given samples of such phases.\n",
    "\n",
    "c. Model the trial function for Monte Carlo calculations\n",
    "\n",
    "4. Both use gradient-descent based learning procedures for minimizing cost functions\n",
    "\n",
    "5. Energy based models don't use backpropagation and automatic differentiation for computing gradients, instead turning to Markov Chain Monte Carlo methods.\n",
    "\n",
    "6. DNNs often have several hidden layers. A restricted Boltzmann machine has only one hidden layer, however several RBMs can be stacked to make up Deep Belief Networks, of which they constitute the building blocks.\n",
    "\n",
    "History: The RBM was developed by amongst others Geoffrey Hinton, called by some the \"Godfather of Deep Learning\", working with the University of Toronto and Google.\n",
    "\n",
    "A BM is what we would call an undirected probabilistic graphical model\n",
    "with stochastic continuous or discrete units.\n",
    "\n",
    "It is interpreted as a stochastic recurrent neural network where the\n",
    "state of each unit(neurons/nodes) depends on the units it is connected\n",
    "to. The weights in the network represent thus the strength of the\n",
    "interaction between various units/nodes.\n",
    "\n",
    "It turns into a Hopfield network if we choose deterministic rather\n",
    "than stochastic units. In contrast to a Hopfield network, a BM is a\n",
    "so-called generative model. It allows us to generate new samples from\n",
    "the learned distribution.\n",
    "\n",
    "A standard BM network is divided into a set of observable and visible units $\\hat{x}$ and a set of unknown hidden units/nodes $\\hat{h}$.\n",
    "\n",
    "Additionally there can be bias nodes for the hidden and visible layers. These biases are normally set to $1$.\n",
    "\n",
    "BMs are stackable, meaning they cwe can train a BM which serves as input to another BM. We can construct deep networks for learning complex PDFs. The layers can be trained one after another, a feature which makes them popular in deep learning\n",
    "\n",
    "However, they are often hard to train. This leads to the introduction of so-called restricted BMs, or RBMS.\n",
    "Here we take away all lateral connections between nodes in the visible layer as well as connections between nodes in the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db321242",
   "metadata": {
    "editable": true
   },
   "source": [
    "## The network\n",
    "\n",
    "**The network layers**:\n",
    "1. A function $\\mathbf{x}$ that represents the visible layer, a vector of $M$ elements (nodes). This layer represents both what the RBM might be given as training input, and what we want it to be able to reconstruct. This might for example be the pixels of an image, the spin values of the Ising model, or coefficients representing speech.\n",
    "\n",
    "2. The function $\\mathbf{h}$ represents the hidden, or latent, layer. A vector of $N$ elements (nodes). Also called \"feature detectors\".\n",
    "\n",
    "The goal of the hidden layer is to increase the model's expressive power. We encode complex interactions between visible variables by introducing additional, hidden variables that interact with visible degrees of freedom in a simple manner, yet still reproduce the complex correlations between visible degrees in the data once marginalized over (integrated out).\n",
    "\n",
    "Examples of this trick being employed in physics: \n",
    "1. The Hubbard-Stratonovich transformation\n",
    "\n",
    "2. The introduction of ghost fields in gauge theory\n",
    "\n",
    "3. Shadow wave functions in Quantum Monte Carlo simulations\n",
    "\n",
    "**The network parameters, to be optimized/learned**:\n",
    "1. $\\mathbf{a}$ represents the visible bias, a vector of same length as $\\mathbf{x}$.\n",
    "\n",
    "2. $\\mathbf{b}$ represents the hidden bias, a vector of same lenght as $\\mathbf{h}$.\n",
    "\n",
    "3. $W$ represents the interaction weights, a matrix of size $M\\times N$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf6eeb0",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Joint distribution\n",
    "\n",
    "The restricted Boltzmann machine is described by a Bolztmann distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91af1bd1",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto8\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\tP_{rbm}(\\mathbf{x},\\mathbf{h}) = \\frac{1}{Z} e^{-\\frac{1}{T_0}E(\\mathbf{x},\\mathbf{h})},\n",
    "\\label{_auto8} \\tag{19}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9196475f",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $Z$ is the normalization constant or partition function, defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164f7097",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto9\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\tZ = \\int \\int e^{-\\frac{1}{T_0}E(\\mathbf{x},\\mathbf{h})} d\\mathbf{x} d\\mathbf{h}.\n",
    "\\label{_auto9} \\tag{20}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f78f4f9",
   "metadata": {
    "editable": true
   },
   "source": [
    "It is common to ignore $T_0$ by setting it to one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4b67e0",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Network Elements, the energy function\n",
    "\n",
    "The function $E(\\mathbf{x},\\mathbf{h})$ gives the **energy** of a\n",
    "configuration (pair of vectors) $(\\mathbf{x}, \\mathbf{h})$. The lower\n",
    "the energy of a configuration, the higher the probability of it. This\n",
    "function also depends on the parameters $\\mathbf{a}$, $\\mathbf{b}$ and\n",
    "$W$. Thus, when we adjust them during the learning procedure, we are\n",
    "adjusting the energy function to best fit our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaac2700",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Defining different types of RBMs\n",
    "\n",
    "There are different variants of RBMs, and the differences lie in the types of visible and hidden units we choose as well as in the implementation of the energy function $E(\\mathbf{x},\\mathbf{h})$. The connection between the nodes in the two layers is given by the weights $w_{ij}$. \n",
    "\n",
    "**Binary-Binary RBM:**\n",
    "\n",
    "RBMs were first developed using binary units in both the visible and hidden layer. The corresponding energy function is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadfea6",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto10\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\tE(\\mathbf{x}, \\mathbf{h}) = - \\sum_i^M x_i a_i- \\sum_j^N b_j h_j - \\sum_{i,j}^{M,N} x_i w_{ij} h_j,\n",
    "\\label{_auto10} \\tag{21}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5bba1b",
   "metadata": {
    "editable": true
   },
   "source": [
    "where the binary values taken on by the nodes are most commonly 0 and 1.\n",
    "\n",
    "**Gaussian-Binary RBM:**\n",
    "\n",
    "Another varient is the RBM where the visible units are Gaussian while the hidden units remain binary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0928fd3b",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto11\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\tE(\\mathbf{x}, \\mathbf{h}) = \\sum_i^M \\frac{(x_i - a_i)^2}{2\\sigma_i^2} - \\sum_j^N b_j h_j - \\sum_{i,j}^{M,N} \\frac{x_i w_{ij} h_j}{\\sigma_i^2}. \n",
    "\\label{_auto11} \\tag{22}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d202766e",
   "metadata": {
    "editable": true
   },
   "source": [
    "1. RBMs are Useful when we model continuous data (i.e., we wish $\\mathbf{x}$ to be continuous)\n",
    "\n",
    "2. Requires a smaller learning rate, since there's no upper bound to the value a component might take in the reconstruction\n",
    "\n",
    "Other types of units include:\n",
    "1. Softmax and multinomial units\n",
    "\n",
    "2. Gaussian visible and hidden units\n",
    "\n",
    "3. Binomial units\n",
    "\n",
    "4. Rectified linear units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b730179b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## RBMs for the quantum many body problem\n",
    "\n",
    "The idea of applying RBMs to quantum many body problems was presented by G. Carleo and M. Troyer, working with ETH Zurich and Microsoft Research.\n",
    "\n",
    "Some of their motivation included\n",
    "\n",
    "* The wave function $\\Psi$ is a monolithic mathematical quantity that contains all the information on a quantum state, be it a single particle or a complex molecule. In principle, an exponential amount of information is needed to fully encode a generic many-body quantum state.\n",
    "\n",
    "* There are still interesting open problems, including fundamental questions ranging from the dynamical properties of high-dimensional systems to the exact ground-state properties of strongly interacting fermions.\n",
    "\n",
    "* The difficulty lies in finding a general strategy to reduce the exponential complexity of the full many-body wave function down to its most essential features. That is\n",
    "\n",
    "a. Dimensional reduction\n",
    "\n",
    "b. Feature extraction\n",
    "\n",
    "* Among the most successful techniques to attack these challenges, artifical neural networks play a prominent role.\n",
    "\n",
    "* Want to understand whether an artifical neural network may adapt to describe a quantum system.\n",
    "\n",
    "Carleo and Troyer applied the RBM to the quantum mechanical spin lattice systems of the Ising model and Heisenberg model, with encouraging results. Our goal is to test the method on systems of moving particles. For the spin lattice systems it was natural to use a binary-binary RBM, with the nodes taking values of 1 and -1. For moving particles, on the other hand, we want the visible nodes to be continuous, representing position coordinates. Thus, we start by choosing a Gaussian-binary RBM, where the visible nodes are continuous and hidden nodes take on values of 0 or 1. If eventually we would like the hidden nodes to be continuous as well the rectified linear units seem like the most relevant choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1650660",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Representing the wave function\n",
    "\n",
    "The wavefunction should be a probability amplitude depending on\n",
    " $\\boldsymbol{x}$. The RBM model is given by the joint distribution of\n",
    " $\\boldsymbol{x}$ and $\\boldsymbol{h}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7cdbae",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto12\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "        F_{rbm}(\\mathbf{x},\\mathbf{h}) = \\frac{1}{Z} e^{-\\frac{1}{T_0}E(\\mathbf{x},\\mathbf{h})}.\n",
    "\\label{_auto12} \\tag{23}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a929c5c5",
   "metadata": {
    "editable": true
   },
   "source": [
    "To find the marginal distribution of $\\boldsymbol{x}$ we set:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c51347",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto13\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "        F_{rbm}(\\mathbf{x}) = \\sum_\\mathbf{h} F_{rbm}(\\mathbf{x}, \\mathbf{h}) \n",
    "\\label{_auto13} \\tag{24}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa17fb72",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto14\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "                                = \\frac{1}{Z}\\sum_\\mathbf{h} e^{-E(\\mathbf{x}, \\mathbf{h})}.\n",
    "\\label{_auto14} \\tag{25}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f87dc2a",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now this is what we use to represent the wave function, calling it a neural-network quantum state (NQS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6e18e1",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto15\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "        \\Psi (\\mathbf{X}) = F_{rbm}(\\mathbf{x}) \n",
    "\\label{_auto15} \\tag{26}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36a998d",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto16\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "        = \\frac{1}{Z}\\sum_{\\boldsymbol{h}} e^{-E(\\mathbf{x}, \\mathbf{h})} \n",
    "\\label{_auto16} \\tag{27}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d340854",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto17\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "        = \\frac{1}{Z} \\sum_{\\{h_j\\}} e^{-\\sum_i^M \\frac{(x_i - a_i)^2}{2\\sigma^2} + \\sum_j^N b_j h_j + \\sum_\\\n",
    "{i,j}^{M,N} \\frac{x_i w_{ij} h_j}{\\sigma^2}} \n",
    "\\label{_auto17} \\tag{28}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5887b12a",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto18\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "        = \\frac{1}{Z} e^{-\\sum_i^M \\frac{(x_i - a_i)^2}{2\\sigma^2}} \\prod_j^N (1 + e^{b_j + \\sum_i^M \\frac{x\\\n",
    "_i w_{ij}}{\\sigma^2}}). \n",
    "\\label{_auto18} \\tag{29}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb57b8c8",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto19\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\\label{_auto19} \\tag{30}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679081dc",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Choose the cost function\n",
    "\n",
    "Now we don't necessarily have training data (unless we generate it by using some other method). However, what we do have is the variational principle which allows us to obtain the ground state wave function by minimizing the expectation value of the energy of a trial wavefunction (corresponding to the untrained NQS). Similarly to the traditional variational Monte Carlo method then, it is the local energy we wish to minimize. The gradient to use for the stochastic gradient descent procedure is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5b16e0",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto20\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\tC_i = \\frac{\\partial \\langle E_L \\rangle}{\\partial \\theta_i}\n",
    "\t= 2(\\langle E_L \\frac{1}{\\Psi}\\frac{\\partial \\Psi}{\\partial \\theta_i} \\rangle - \\langle E_L \\rangle \\langle \\frac{1}{\\Psi}\\frac{\\partial \\Psi}{\\partial \\theta_i} \\rangle ),\n",
    "\\label{_auto20} \\tag{31}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cdf0d2",
   "metadata": {
    "editable": true
   },
   "source": [
    "where the local energy is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a306c2f",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto21\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\tE_L = \\frac{1}{\\Psi} \\hat{\\mathbf{H}} \\Psi.\n",
    "\\label{_auto21} \\tag{32}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886985eb",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Neural Quantum States\n",
    "\n",
    "The wavefunction should be a probability amplitude depending on $\\boldsymbol{x}$. The RBM model is given by the joint distribution of $\\boldsymbol{x}$ and $\\boldsymbol{h}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6701746",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto22\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\tF_{rbm}(\\boldsymbol{x},\\mathbf{h}) = \\frac{1}{Z} e^{-\\frac{1}{T_0}E(\\boldsymbol{x},\\mathbf{h})}\n",
    "\\label{_auto22} \\tag{33}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b012482b",
   "metadata": {
    "editable": true
   },
   "source": [
    "To find the marginal distribution of $\\boldsymbol{x}$ we set:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724a8367",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto23\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\tF_{rbm}(\\mathbf{x}) = \\sum_\\mathbf{h} F_{rbm}(\\mathbf{x}, \\mathbf{h}) \n",
    "\\label{_auto23} \\tag{34}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42619211",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto24\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\t\t\t\t= \\frac{1}{Z}\\sum_\\mathbf{h} e^{-E(\\mathbf{x}, \\mathbf{h})}\n",
    "\\label{_auto24} \\tag{35}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d4a881",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now this is what we use to represent the wave function, calling it a neural-network quantum state (NQS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153ab1d1",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto25\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\t\\Psi (\\mathbf{X}) = F_{rbm}(\\mathbf{x}) \n",
    "\\label{_auto25} \\tag{36}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6136f7e1",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto26\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\t= \\frac{1}{Z}\\sum_{\\boldsymbol{h}} e^{-E(\\mathbf{x}, \\mathbf{h})} \n",
    "\\label{_auto26} \\tag{37}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223e3e32",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto27\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\t= \\frac{1}{Z} \\sum_{\\{h_j\\}} e^{-\\sum_i^M \\frac{(x_i - a_i)^2}{2\\sigma^2} + \\sum_j^N b_j h_j + \\sum_{i,j}^{M,N} \\frac{x_i w_{ij} h_j}{\\sigma^2}} \n",
    "\\label{_auto27} \\tag{38}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf0674b",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto28\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\t= \\frac{1}{Z} e^{-\\sum_i^M \\frac{(x_i - a_i)^2}{2\\sigma^2}} \\prod_j^N (1 + e^{b_j + \\sum_i^M \\frac{x_i w_{ij}}{\\sigma^2}}) \n",
    "\\label{_auto28} \\tag{39}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa380f9",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto29\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\\label{_auto29} \\tag{40}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25af183d",
   "metadata": {
    "editable": true
   },
   "source": [
    "The above wavefunction is the most general one because it allows for\n",
    "complex valued wavefunctions. However it fundamentally changes the\n",
    "probabilistic foundation of the RBM, because what is usually a\n",
    "probability in the RBM framework is now a an amplitude. This means\n",
    "that a lot of the theoretical framework usually used to interpret the\n",
    "model, i.e. graphical models, conditional probabilities, and Markov\n",
    "random fields, breaks down. If we assume the wavefunction to be\n",
    "postive definite, however, we can use the RBM to represent the squared\n",
    "wavefunction, and thereby a probability. This also makes it possible\n",
    "to sample from the model using Gibbs sampling, because we can obtain\n",
    "the conditional probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94802368",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto30\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\t|\\Psi (\\mathbf{X})|^2 = F_{rbm}(\\mathbf{X}) \n",
    "\\label{_auto30} \\tag{41}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da93de1d",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto31\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\t\\Rightarrow \\Psi (\\mathbf{X}) = \\sqrt{F_{rbm}(\\mathbf{X})} \n",
    "\\label{_auto31} \\tag{42}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4d20d5",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto32\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\t= \\frac{1}{\\sqrt{Z}}\\sqrt{\\sum_{\\{h_j\\}} e^{-E(\\mathbf{X}, \\mathbf{h})}} \n",
    "\\label{_auto32} \\tag{43}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426b60fa",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto33\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\t= \\frac{1}{\\sqrt{Z}} \\sqrt{\\sum_{\\{h_j\\}} e^{-\\sum_i^M \\frac{(X_i - a_i)^2}{2\\sigma^2} + \\sum_j^N b_j h_j + \\sum_{i,j}^{M,N} \\frac{X_i w_{ij} h_j}{\\sigma^2}} }\n",
    "\\label{_auto33} \\tag{44}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f4f594",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto34\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\t= \\frac{1}{\\sqrt{Z}} e^{-\\sum_i^M \\frac{(X_i - a_i)^2}{4\\sigma^2}} \\sqrt{\\sum_{\\{h_j\\}} \\prod_j^N e^{b_j h_j + \\sum_i^M \\frac{X_i w_{ij} h_j}{\\sigma^2}}} \n",
    "\\label{_auto34} \\tag{45}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c3ddb0",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto35\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\t= \\frac{1}{\\sqrt{Z}} e^{-\\sum_i^M \\frac{(X_i - a_i)^2}{4\\sigma^2}} \\sqrt{\\prod_j^N \\sum_{h_j}  e^{b_j h_j + \\sum_i^M \\frac{X_i w_{ij} h_j}{\\sigma^2}}} \n",
    "\\label{_auto35} \\tag{46}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ef998b",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto36\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\t= \\frac{1}{\\sqrt{Z}} e^{-\\sum_i^M \\frac{(X_i - a_i)^2}{4\\sigma^2}} \\prod_j^N \\sqrt{e^0 + e^{b_j + \\sum_i^M \\frac{X_i w_{ij}}{\\sigma^2}}} \n",
    "\\label{_auto36} \\tag{47}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a88d5e",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto37\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\t= \\frac{1}{\\sqrt{Z}} e^{-\\sum_i^M \\frac{(X_i - a_i)^2}{4\\sigma^2}} \\prod_j^N \\sqrt{1 + e^{b_j + \\sum_i^M \\frac{X_i w_{ij}}{\\sigma^2}}} \n",
    "\\label{_auto37} \\tag{48}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35d8d1f",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto38\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\\label{_auto38} \\tag{49}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501b18f",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Cost function\n",
    "\n",
    "This is where we deviate from what is common in machine\n",
    "learning. Rather than defining a cost function based on some dataset,\n",
    "our cost function is the energy of the quantum mechanical system. From\n",
    "the variational principle we know that minizing this energy should\n",
    "lead to the ground state wavefunction. As stated previously the local\n",
    "energy is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f6b04c",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto39\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\tE_L = \\frac{1}{\\Psi} \\hat{\\mathbf{H}} \\Psi,\n",
    "\\label{_auto39} \\tag{50}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a716a1db",
   "metadata": {
    "editable": true
   },
   "source": [
    "and the gradient is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bdc37b",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto40\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\tG_i = \\frac{\\partial \\langle E_L \\rangle}{\\partial \\alpha_i}\n",
    "\t= 2(\\langle E_L \\frac{1}{\\Psi}\\frac{\\partial \\Psi}{\\partial \\alpha_i} \\rangle - \\langle E_L \\rangle \\langle \\frac{1}{\\Psi}\\frac{\\partial \\Psi}{\\partial \\alpha_i} \\rangle ),\n",
    "\\label{_auto40} \\tag{51}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d2a9dc",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\alpha_i = a_1,...,a_M,b_1,...,b_N,w_{11},...,w_{MN}$.\n",
    "\n",
    "We use that $\\frac{1}{\\Psi}\\frac{\\partial \\Psi}{\\partial \\alpha_i} \n",
    "\t= \\frac{\\partial \\ln{\\Psi}}{\\partial \\alpha_i}$,\n",
    "and find"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e65a3b0",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto41\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\t\\ln{\\Psi({\\mathbf{X}})} = -\\ln{Z} - \\sum_m^M \\frac{(X_m - a_m)^2}{2\\sigma^2}\n",
    "\t+ \\sum_n^N \\ln({1 + e^{b_n + \\sum_i^M \\frac{X_i w_{in}}{\\sigma^2}})}.\n",
    "\\label{_auto41} \\tag{52}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d537a1",
   "metadata": {
    "editable": true
   },
   "source": [
    "This gives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640693c5",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto42\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\t\\frac{\\partial }{\\partial a_m} \\ln\\Psi\n",
    "\t= \t\\frac{1}{\\sigma^2} (X_m - a_m) \n",
    "\\label{_auto42} \\tag{53}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e06dce4",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto43\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\t\\frac{\\partial }{\\partial b_n} \\ln\\Psi\n",
    "\t=\n",
    "\t\\frac{1}{e^{-b_n-\\frac{1}{\\sigma^2}\\sum_i^M X_i w_{in}} + 1} \n",
    "\\label{_auto43} \\tag{54}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d29356",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto44\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\t\\frac{\\partial }{\\partial w_{mn}} \\ln\\Psi\n",
    "\t= \\frac{X_m}{\\sigma^2(e^{-b_n-\\frac{1}{\\sigma^2}\\sum_i^M X_i w_{in}} + 1)}.\n",
    "\\label{_auto44} \\tag{55}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99132f7",
   "metadata": {
    "editable": true
   },
   "source": [
    "If $\\Psi = \\sqrt{F_{rbm}}$ we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe119b4",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto45\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\t\\ln{\\Psi({\\mathbf{X}})} = -\\frac{1}{2}\\ln{Z} - \\sum_m^M \\frac{(X_m - a_m)^2}{4\\sigma^2}\n",
    "\t+ \\frac{1}{2}\\sum_n^N \\ln({1 + e^{b_n + \\sum_i^M \\frac{X_i w_{in}}{\\sigma^2}})},\n",
    "\\label{_auto45} \\tag{56}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e669125a",
   "metadata": {
    "editable": true
   },
   "source": [
    "which results in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460e979d",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto46\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\t\\frac{\\partial }{\\partial a_m} \\ln\\Psi\n",
    "\t= \t\\frac{1}{2\\sigma^2} (X_m - a_m) \n",
    "\\label{_auto46} \\tag{57}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43286d42",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto47\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\t\\frac{\\partial }{\\partial b_n} \\ln\\Psi\n",
    "\t=\n",
    "\t\\frac{1}{2(e^{-b_n-\\frac{1}{\\sigma^2}\\sum_i^M X_i w_{in}} + 1)} \n",
    "\\label{_auto47} \\tag{58}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a863dd",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto48\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\t\\frac{\\partial }{\\partial w_{mn}} \\ln\\Psi\n",
    "\t= \\frac{X_m}{2\\sigma^2(e^{-b_n-\\frac{1}{\\sigma^2}\\sum_i^M X_i w_{in}} + 1)}.\n",
    "\\label{_auto48} \\tag{59}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d3ee15",
   "metadata": {
    "editable": true
   },
   "source": [
    "Let us assume again that our Hamiltonian is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5df3a78",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto49\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\t\\hat{\\mathbf{H}} = \\sum_p^P (-\\frac{1}{2}\\nabla_p^2 + \\frac{1}{2}\\omega^2 r_p^2 ) + \\sum_{p<q} \\frac{1}{r_{pq}},\n",
    "\\label{_auto49} \\tag{60}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcc33ad",
   "metadata": {
    "editable": true
   },
   "source": [
    "where the first summation term represents the standard harmonic\n",
    "oscillator part and the latter the repulsive interaction between two\n",
    "electrons. Natural units ($\\hbar=c=e=m_e=1$) are used, and $P$ is the\n",
    "number of particles. This gives us the following expression for the\n",
    "local energy ($D$ being the number of dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1bf706",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto50\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\tE_L = \\frac{1}{\\Psi} \\mathbf{H} \\Psi \n",
    "\\label{_auto50} \\tag{61}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccac620",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto51\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\t= \\frac{1}{\\Psi} (\\sum_p^P (-\\frac{1}{2}\\nabla_p^2 + \\frac{1}{2}\\omega^2 r_p^2 ) + \\sum_{p<q} \\frac{1}{r_{pq}}) \\Psi \n",
    "\\label{_auto51} \\tag{62}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e17c12c",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto52\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\t= -\\frac{1}{2}\\frac{1}{\\Psi} \\sum_p^P \\nabla_p^2 \\Psi \n",
    "\t+ \\frac{1}{2}\\omega^2 \\sum_p^P  r_p^2  + \\sum_{p<q} \\frac{1}{r_{pq}} \n",
    "\\label{_auto52} \\tag{63}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a03491c",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto53\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\t= -\\frac{1}{2}\\frac{1}{\\Psi} \\sum_p^P \\sum_d^D \\frac{\\partial^2 \\Psi}{\\partial x_{pd}^2} + \\frac{1}{2}\\omega^2 \\sum_p^P  r_p^2  + \\sum_{p<q} \\frac{1}{r_{pq}} \n",
    "\\label{_auto53} \\tag{64}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfeb415",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto54\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\t= \\frac{1}{2} \\sum_p^P \\sum_d^D (-(\\frac{\\partial}{\\partial x_{pd}} \\ln\\Psi)^2 -\\frac{\\partial^2}{\\partial x_{pd}^2} \\ln\\Psi + \\omega^2 x_{pd}^2)  + \\sum_{p<q} \\frac{1}{r_{pq}}. \n",
    "\\label{_auto54} \\tag{65}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc8d019",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto55\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\\label{_auto55} \\tag{66}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bac58b",
   "metadata": {
    "editable": true
   },
   "source": [
    "Letting each visible node in the Boltzmann machine \n",
    "represent one coordinate of one particle, we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c3fb20",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto56\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\tE_L =\n",
    "\t\\frac{1}{2} \\sum_m^M (-(\\frac{\\partial}{\\partial v_m} \\ln\\Psi)^2 -\\frac{\\partial^2}{\\partial v_m^2} \\ln\\Psi + \\omega^2 v_m^2)  + \\sum_{p<q} \\frac{1}{r_{pq}},\n",
    "\\label{_auto56} \\tag{67}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248c3c88",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we have that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c01eccf",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto57\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\t\\frac{\\partial}{\\partial x_m} \\ln\\Psi\n",
    "\t= - \\frac{1}{\\sigma^2}(x_m - a_m) + \\frac{1}{\\sigma^2} \\sum_n^N \\frac{w_{mn}}{e^{-b_n - \\frac{1}{\\sigma^2}\\sum_i^M x_i w_{in}} + 1} \n",
    "\\label{_auto57} \\tag{68}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52007b90",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto58\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\t\\frac{\\partial^2}{\\partial x_m^2} \\ln\\Psi\n",
    "\t= - \\frac{1}{\\sigma^2} + \\frac{1}{\\sigma^4}\\sum_n^N \\omega_{mn}^2 \\frac{e^{b_n + \\frac{1}{\\sigma^2}\\sum_i^M x_i w_{in}}}{(e^{b_n + \\frac{1}{\\sigma^2}\\sum_i^M x_i w_{in}} + 1)^2}.\n",
    "\\label{_auto58} \\tag{69}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2a7b6f",
   "metadata": {
    "editable": true
   },
   "source": [
    "We now have all the expressions neeeded to calculate the gradient of\n",
    "the expected local energy with respect to the RBM parameters\n",
    "$\\frac{\\partial \\langle E_L \\rangle}{\\partial \\alpha_i}$.\n",
    "\n",
    "If we use $\\Psi = \\sqrt{F_{rbm}}$ we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311f8e03",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto59\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\t\\frac{\\partial}{\\partial x_m} \\ln\\Psi\n",
    "\t= - \\frac{1}{2\\sigma^2}(x_m - a_m) + \\frac{1}{2\\sigma^2} \\sum_n^N\n",
    " \t\\frac{w_{mn}}{e^{-b_n-\\frac{1}{\\sigma^2}\\sum_i^M x_i w_{in}} + 1}\n",
    "\t\n",
    "\\label{_auto59} \\tag{70}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1830f344",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto60\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "\t\\frac{\\partial^2}{\\partial x_m^2} \\ln\\Psi\n",
    "\t= - \\frac{1}{2\\sigma^2} + \\frac{1}{2\\sigma^4}\\sum_n^N \\omega_{mn}^2 \\frac{e^{b_n + \\frac{1}{\\sigma^2}\\sum_i^M x_i w_{in}}}{(e^{b_n + \\frac{1}{\\sigma^2}\\sum_i^M x_i w_{in}} + 1)^2}.\n",
    "\\label{_auto60} \\tag{71}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165f4155",
   "metadata": {
    "editable": true
   },
   "source": [
    "The difference between this equation and the previous one is that we multiply by a factor $1/2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95873e99",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Python version for the two non-interacting particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68896aeb",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 2-electron VMC code for 2dim quantum dot with importance sampling\n",
    "# Using gaussian rng for new positions and Metropolis- Hastings \n",
    "# Added restricted boltzmann machine method for dealing with the wavefunction\n",
    "# RBM code based heavily off of:\n",
    "# https://github.com/CompPhysics/ComputationalPhysics2/tree/gh-pages/doc/Programs/BoltzmannMachines/MLcpp/src/CppCode/ob\n",
    "from math import exp, sqrt\n",
    "from random import random, seed, normalvariate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "# Trial wave function for the 2-electron quantum dot in two dims\n",
    "def WaveFunction(r,a,b,w):\n",
    "    sigma=1.0\n",
    "    sig2 = sigma**2\n",
    "    Psi1 = 0.0\n",
    "    Psi2 = 1.0\n",
    "    Q = Qfac(r,b,w)\n",
    "    \n",
    "    for iq in range(NumberParticles):\n",
    "        for ix in range(Dimension):\n",
    "            Psi1 += (r[iq,ix]-a[iq,ix])**2\n",
    "            \n",
    "    for ih in range(NumberHidden):\n",
    "        Psi2 *= (1.0 + np.exp(Q[ih]))\n",
    "        \n",
    "    Psi1 = np.exp(-Psi1/(2*sig2))\n",
    "\n",
    "    return Psi1*Psi2\n",
    "\n",
    "# Local energy  for the 2-electron quantum dot in two dims, using analytical local energy\n",
    "def LocalEnergy(r,a,b,w):\n",
    "    sigma=1.0\n",
    "    sig2 = sigma**2\n",
    "    locenergy = 0.0\n",
    "    \n",
    "    Q = Qfac(r,b,w)\n",
    "\n",
    "    for iq in range(NumberParticles):\n",
    "        for ix in range(Dimension):\n",
    "            sum1 = 0.0\n",
    "            sum2 = 0.0\n",
    "            for ih in range(NumberHidden):\n",
    "                sum1 += w[iq,ix,ih]/(1+np.exp(-Q[ih]))\n",
    "                sum2 += w[iq,ix,ih]**2 * np.exp(Q[ih]) / (1.0 + np.exp(Q[ih]))**2\n",
    "    \n",
    "            dlnpsi1 = -(r[iq,ix] - a[iq,ix]) /sig2 + sum1/sig2\n",
    "            dlnpsi2 = -1/sig2 + sum2/sig2**2\n",
    "            locenergy += 0.5*(-dlnpsi1*dlnpsi1 - dlnpsi2 + r[iq,ix]**2)\n",
    "            \n",
    "    if(interaction==True):\n",
    "        for iq1 in range(NumberParticles):\n",
    "            for iq2 in range(iq1):\n",
    "                distance = 0.0\n",
    "                for ix in range(Dimension):\n",
    "                    distance += (r[iq1,ix] - r[iq2,ix])**2\n",
    "                    \n",
    "                locenergy += 1/sqrt(distance)\n",
    "                \n",
    "    return locenergy\n",
    "\n",
    "# Derivate of wave function ansatz as function of variational parameters\n",
    "def DerivativeWFansatz(r,a,b,w):\n",
    "    \n",
    "    sigma=1.0\n",
    "    sig2 = sigma**2\n",
    "    \n",
    "    Q = Qfac(r,b,w)\n",
    "    \n",
    "    WfDer = np.empty((3,),dtype=object)\n",
    "    WfDer = [np.copy(a),np.copy(b),np.copy(w)]\n",
    "    \n",
    "    WfDer[0] = (r-a)/sig2\n",
    "    WfDer[1] = 1 / (1 + np.exp(-Q))\n",
    "    \n",
    "    for ih in range(NumberHidden):\n",
    "        WfDer[2][:,:,ih] = w[:,:,ih] / (sig2*(1+np.exp(-Q[ih])))\n",
    "            \n",
    "    return  WfDer\n",
    "\n",
    "# Setting up the quantum force for the two-electron quantum dot, recall that it is a vector\n",
    "def QuantumForce(r,a,b,w):\n",
    "\n",
    "    sigma=1.0\n",
    "    sig2 = sigma**2\n",
    "    \n",
    "    qforce = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    sum1 = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    \n",
    "    Q = Qfac(r,b,w)\n",
    "    \n",
    "    for ih in range(NumberHidden):\n",
    "        sum1 += w[:,:,ih]/(1+np.exp(-Q[ih]))\n",
    "    \n",
    "    qforce = 2*(-(r-a)/sig2 + sum1/sig2)\n",
    "    \n",
    "    return qforce\n",
    "    \n",
    "def Qfac(r,b,w):\n",
    "    Q = np.zeros((NumberHidden), np.double)\n",
    "    temp = np.zeros((NumberHidden), np.double)\n",
    "    \n",
    "    for ih in range(NumberHidden):\n",
    "        temp[ih] = (r*w[:,:,ih]).sum()\n",
    "        \n",
    "    Q = b + temp\n",
    "    \n",
    "    return Q\n",
    "    \n",
    "# Computing the derivative of the energy and the energy \n",
    "def EnergyMinimization(a,b,w):\n",
    "\n",
    "    NumberMCcycles= 10000\n",
    "    # Parameters in the Fokker-Planck simulation of the quantum force\n",
    "    D = 0.5\n",
    "    TimeStep = 0.05\n",
    "    # positions\n",
    "    PositionOld = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    PositionNew = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    # Quantum force\n",
    "    QuantumForceOld = np.zeros((NumberParticles,Dimension), np.double)\n",
    "    QuantumForceNew = np.zeros((NumberParticles,Dimension), np.double)\n",
    "\n",
    "    # seed for rng generator \n",
    "    seed()\n",
    "    energy = 0.0\n",
    "    DeltaE = 0.0\n",
    "\n",
    "    EnergyDer = np.empty((3,),dtype=object)\n",
    "    DeltaPsi = np.empty((3,),dtype=object)\n",
    "    DerivativePsiE = np.empty((3,),dtype=object)\n",
    "    EnergyDer = [np.copy(a),np.copy(b),np.copy(w)]\n",
    "    DeltaPsi = [np.copy(a),np.copy(b),np.copy(w)]\n",
    "    DerivativePsiE = [np.copy(a),np.copy(b),np.copy(w)]\n",
    "    for i in range(3): EnergyDer[i].fill(0.0)\n",
    "    for i in range(3): DeltaPsi[i].fill(0.0)\n",
    "    for i in range(3): DerivativePsiE[i].fill(0.0)\n",
    "\n",
    "    \n",
    "    #Initial position\n",
    "    for i in range(NumberParticles):\n",
    "        for j in range(Dimension):\n",
    "            PositionOld[i,j] = normalvariate(0.0,1.0)*sqrt(TimeStep)\n",
    "    wfold = WaveFunction(PositionOld,a,b,w)\n",
    "    QuantumForceOld = QuantumForce(PositionOld,a,b,w)\n",
    "\n",
    "    #Loop over MC MCcycles\n",
    "    for MCcycle in range(NumberMCcycles):\n",
    "        #Trial position moving one particle at the time\n",
    "        for i in range(NumberParticles):\n",
    "            for j in range(Dimension):\n",
    "                PositionNew[i,j] = PositionOld[i,j]+normalvariate(0.0,1.0)*sqrt(TimeStep)+\\\n",
    "                                       QuantumForceOld[i,j]*TimeStep*D\n",
    "            wfnew = WaveFunction(PositionNew,a,b,w)\n",
    "            QuantumForceNew = QuantumForce(PositionNew,a,b,w)\n",
    "            \n",
    "            GreensFunction = 0.0\n",
    "            for j in range(Dimension):\n",
    "                GreensFunction += 0.5*(QuantumForceOld[i,j]+QuantumForceNew[i,j])*\\\n",
    "                                      (D*TimeStep*0.5*(QuantumForceOld[i,j]-QuantumForceNew[i,j])-\\\n",
    "                                      PositionNew[i,j]+PositionOld[i,j])\n",
    "      \n",
    "            GreensFunction = exp(GreensFunction)\n",
    "            ProbabilityRatio = GreensFunction*wfnew**2/wfold**2\n",
    "            #Metropolis-Hastings test to see whether we accept the move\n",
    "            if random() <= ProbabilityRatio:\n",
    "                for j in range(Dimension):\n",
    "                    PositionOld[i,j] = PositionNew[i,j]\n",
    "                    QuantumForceOld[i,j] = QuantumForceNew[i,j]\n",
    "                wfold = wfnew\n",
    "        #print(\"wf new:        \", wfnew)\n",
    "        #print(\"force on 1 new:\", QuantumForceNew[0,:])\n",
    "        #print(\"pos of 1 new:  \", PositionNew[0,:])\n",
    "        #print(\"force on 2 new:\", QuantumForceNew[1,:])\n",
    "        #print(\"pos of 2 new:  \", PositionNew[1,:])\n",
    "        DeltaE = LocalEnergy(PositionOld,a,b,w)\n",
    "        DerPsi = DerivativeWFansatz(PositionOld,a,b,w)\n",
    "        \n",
    "        DeltaPsi[0] += DerPsi[0]\n",
    "        DeltaPsi[1] += DerPsi[1]\n",
    "        DeltaPsi[2] += DerPsi[2]\n",
    "        \n",
    "        energy += DeltaE\n",
    "\n",
    "        DerivativePsiE[0] += DerPsi[0]*DeltaE\n",
    "        DerivativePsiE[1] += DerPsi[1]*DeltaE\n",
    "        DerivativePsiE[2] += DerPsi[2]*DeltaE\n",
    "            \n",
    "    # We calculate mean values\n",
    "    energy /= NumberMCcycles\n",
    "    DerivativePsiE[0] /= NumberMCcycles\n",
    "    DerivativePsiE[1] /= NumberMCcycles\n",
    "    DerivativePsiE[2] /= NumberMCcycles\n",
    "    DeltaPsi[0] /= NumberMCcycles\n",
    "    DeltaPsi[1] /= NumberMCcycles\n",
    "    DeltaPsi[2] /= NumberMCcycles\n",
    "    EnergyDer[0]  = 2*(DerivativePsiE[0]-DeltaPsi[0]*energy)\n",
    "    EnergyDer[1]  = 2*(DerivativePsiE[1]-DeltaPsi[1]*energy)\n",
    "    EnergyDer[2]  = 2*(DerivativePsiE[2]-DeltaPsi[2]*energy)\n",
    "    return energy, EnergyDer\n",
    "\n",
    "\n",
    "#Here starts the main program with variable declarations\n",
    "NumberParticles = 2\n",
    "Dimension = 2\n",
    "NumberHidden = 2\n",
    "\n",
    "interaction=False\n",
    "\n",
    "# guess for parameters\n",
    "a=np.random.normal(loc=0.0, scale=0.001, size=(NumberParticles,Dimension))\n",
    "b=np.random.normal(loc=0.0, scale=0.001, size=(NumberHidden))\n",
    "w=np.random.normal(loc=0.0, scale=0.001, size=(NumberParticles,Dimension,NumberHidden))\n",
    "# Set up iteration using stochastic gradient method\n",
    "Energy = 0\n",
    "EDerivative = np.empty((3,),dtype=object)\n",
    "EDerivative = [np.copy(a),np.copy(b),np.copy(w)]\n",
    "# Learning rate eta, max iterations, need to change to adaptive learning rate\n",
    "eta = 0.001\n",
    "MaxIterations = 50\n",
    "iter = 0\n",
    "np.seterr(invalid='raise')\n",
    "Energies = np.zeros(MaxIterations)\n",
    "EnergyDerivatives1 = np.zeros(MaxIterations)\n",
    "EnergyDerivatives2 = np.zeros(MaxIterations)\n",
    "\n",
    "while iter < MaxIterations:\n",
    "    Energy, EDerivative = EnergyMinimization(a,b,w)\n",
    "    agradient = EDerivative[0]\n",
    "    bgradient = EDerivative[1]\n",
    "    wgradient = EDerivative[2]\n",
    "    a -= eta*agradient\n",
    "    b -= eta*bgradient \n",
    "    w -= eta*wgradient \n",
    "    Energies[iter] = Energy\n",
    "    print(\"Energy:\",Energy)\n",
    "    #EnergyDerivatives1[iter] = EDerivative[0] \n",
    "    #EnergyDerivatives2[iter] = EDerivative[1]\n",
    "    #EnergyDerivatives3[iter] = EDerivative[2] \n",
    "\n",
    "\n",
    "    iter += 1\n",
    "\n",
    "#nice printout with Pandas\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "pd.set_option('max_columns', 6)\n",
    "data ={'Energy':Energies}#,'A Derivative':EnergyDerivatives1,'B Derivative':EnergyDerivatives2,'Weights Derivative':EnergyDerivatives3}\n",
    "\n",
    "frame = pd.DataFrame(data)\n",
    "print(frame)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
